{
  "paper_id": "apep_0180",
  "scan_date": "2026-02-06T12:58:56.552390+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "CLEAN",
  "files_scanned": 7,
  "flags": [
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        55,
        94
      ],
      "evidence": "Key empirical inputs (treatment effects, SEs, p-values, sample sizes) are hard-coded via tribble() rather than computed from replication microdata. In this project, this is plausibly intended and consistent with the manuscript and script header: both explicitly state the MVPF is constructed from published estimates (as in Hendren & Sprung-Keyser). Still, hard-coding increases transcription-risk and makes it difficult to verify against the cited tables without an automated crosswalk.: haushofer_shapiro_effects <- tibble::tribble(\n  ~outcome, ~control_mean, ~treatment_effect, ~se, ~pvalue, ~n_obs,\n  \"Total consumption\", 158, 35, 8, 0.001, 1372,\n  ...\n)\n\negger_ge_effects <- tibble::tribble(\n  ~outcome, ~recipient_effect, ~recipient_se, ~nonrecipient_effect, ~nonrecipient_se, ~n_villages,\n  \"Consumption\", 293, 62, 245, 78, 653,\n  ...\n)",
      "confidence": 0.78
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        1,
        210
      ],
      "evidence": "The pipeline does not fetch or verify any primary/replication datasets; it creates a local dataset entirely from manually-entered published summary statistics. The manuscript\u2019s Data Appendix acknowledges reliance on published treatment effects and points to Dataverse/supplementary materials, so this is not necessarily improper. However, without a scripted download + checksum + parsing of the published tables (or replication outputs), provenance is not reproducible and transcription errors are harder to rule out.: # This script compiles PUBLISHED treatment effect estimates...\n# All values below are transcribed from the original publications:\n#   - Haushofer & Shapiro (2016) QJE Tables 2-4\n#   - Haushofer & Shapiro (2018) working paper\n#   - Egger et al. (2022) Econometrica Tables 2-5\n...\nsave(\n  haushofer_shapiro_effects,\n  egger_ge_effects,\n  ...,\n  file = file.path(data_dir, \"kenya_uct_data.RData\")\n)",
      "confidence": 0.74
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "03_main_analysis.R",
      "lines": [
        70,
        140
      ],
      "evidence": "Random number generation is used to construct uncertainty intervals. This is not fabrication per se (it is simulation for CI construction), but the manuscript states \u201c95% CIs from bootstrap with 1,000 replications.\u201d The implemented procedure is a parametric/normal-approximation Monte Carlo (drawing from rnorm with ad hoc SD scaling factors), not a bootstrap resampling of units or clusters. This is more a methodology-label mismatch than fabrication, but it is a notable integrity/reproducibility point because uncertainty can be materially different under true (clustered) bootstrap vs. normal draws.: set.seed(42)\n n_boot <- 1000\n...\nspillover_draws <- rnorm(n_boot,\n                         mean = wtp_spillover_per_recipient,\n                         sd = egger_ge_effects$nonrecipient_se[1] * 0.5)\n...\nvat_draws <- rnorm(n_boot,\n                   mean = pv_vat,\n                   sd = consumption_se * kenya_fiscal$vat_rate * 0.5 * 3)\n...\nincome_tax_draws <- rnorm(n_boot,\n                          mean = pv_income_tax,\n                          sd = earnings_se * kenya_fiscal$income_tax_formal * (1 - kenya_fiscal$informal_share) * 5)",
      "confidence": 0.83
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "03_main_analysis.R",
      "lines": [
        61,
        140
      ],
      "evidence": "The paper (Table \u201cMain MVPF Estimates\u201d notes) says \u201c95% CIs from bootstrap with 1,000 replications,\u201d but the code explicitly uses \u201cnormal approximation from SEs\u201d and draws from rnorm() with hand-constructed standard deviations for components. That is a parametric simulation, not a bootstrap. If the paper intends a bootstrap over microdata, it is not implemented; if it intends parametric simulation, the manuscript wording should be corrected. In addition, the SD formulas include heuristic multipliers (e.g., \u201c* 3\u201d for VAT PV, \u201c* 5\u201d for income tax PV) that are not derived from a documented delta method, which can under/overstate uncertainty.: # Bootstrap confidence intervals\n...\n# Bootstrap parameters (using normal approximation from SEs)\nset.seed(42)\nn_boot <- 1000\n...\nspillover_draws <- rnorm(...)\nvat_draws <- rnorm(...)\nincome_tax_draws <- rnorm(...)",
      "confidence": 0.86
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "02_clean_data.R",
      "lines": [
        55,
        90
      ],
      "evidence": "Spillover scaling includes an unused and weakly-justified calculation: a hard-coded \u201c150 HH/village\u201d and \u201c0.33 non-recipient rate\u201d is computed (n_spillover_hh) but never used in subsequent WTP calculations. The actual spillover scaling uses spillover_ratio derived mechanically from saturation (0.5). This is not directly result-biasing because it is unused, but it signals potential confusion about spillover aggregation and could hide earlier specification changes if not cleaned up.: n_spillover_hh <- study_design$egger_high_sat_villages * 150 * 0.33\n...\nspillover_ratio <- (1 - study_design$high_saturation_rate) / study_design$high_saturation_rate",
      "confidence": 0.66
    },
    {
      "category": "STATISTICAL_IMPOSSIBILITY",
      "severity": "MEDIUM",
      "file": "paper.tex",
      "lines": [
        280,
        360
      ],
      "evidence": "The reported CI for the baseline MVPF is extremely tight (width 0.02) given that key inputs (GE consumption/earnings effects, persistence, VAT coverage, and informal share) are uncertain. The code\u2019s uncertainty procedure fixes WTP_direct and uses a normal-draw simulation with ad hoc component SDs, which can mechanically produce very narrow intervals. This is not mathematically impossible, but it is potentially implausible as a \u201cbootstrap\u201d CI and may materially understate uncertainty relative to a cluster bootstrap or a fuller sensitivity/uncertainty model.: I estimate an MVPF of 0.87 (95\\% CI: 0.86--0.88) ...\n...\n\\item Notes: WTP = willingness to pay per recipient. ... 95\\% CIs from bootstrap with 1,000 replications.",
      "confidence": 0.72
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "CLEAN",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 0,
      "MEDIUM": 2,
      "LOW": 4
    },
    "one_liner": "Minor issues only",
    "executive_summary": "Minor code quality issues detected, but no evidence of data fabrication or manipulation.",
    "top_issues": [],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0180_scan.json"
  },
  "error": null
}