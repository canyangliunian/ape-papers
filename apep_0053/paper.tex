\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{setspace}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{float}

\doublespacing

\title{\textbf{Automatic Voter Registration and Electoral Participation:\\
Evidence from Staggered State Adoption}}

\author{%
  Anonymous\\
  \textit{Generated with Claude Code}\\[0.5em]
  \small Co-Authored-By: Claude Sonnet 4.5 \textless noreply@anthropic.com\textgreater
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
\noindent
Automatic voter registration (AVR) laws, which register eligible citizens to vote by default during government agency interactions, have been adopted by 20 U.S. states since Oregon's pioneering 2015 legislation. Using staggered difference-in-differences methods and Census CPS Voting Supplement data (2010-2022), I estimate the causal effect of AVR on voter registration and turnout. Despite theoretical predictions and single-state case studies suggesting substantial effects, I find \textbf{null results}: AVR adoption is associated with a statistically insignificant 0.2 percentage point increase in registration rates (SE=0.9pp, p=0.78) and a -0.1 percentage point change in turnout (SE=1.2pp, p=0.94). Event study estimates reveal violations of parallel trends assumptions, with treated states exhibiting declining pre-treatment registration relative to controls. These findings suggest that AVR's effects may be more modest and context-dependent than earlier literature indicated, highlighting the importance of multi-state analyses with heterogeneity-robust methods. I discuss measurement challenges, selection into treatment, and concurrent voting reforms as potential explanations for the null findings.
\end{abstract}

\textbf{JEL Codes:} D72, H11, K16

\textbf{Keywords:} Automatic voter registration, voter turnout, difference-in-differences, electoral participation

\clearpage

\section{Introduction}

Voter registration is a prerequisite for electoral participation in the United States, yet registration rates lag behind those in other developed democracies. As of 2020, approximately 33\% of eligible voters remained unregistered, creating a substantial barrier to democratic participation \citep{census2020}. Automatic Voter Registration (AVR) represents a policy innovation designed to reduce this barrier by shifting the registration default: instead of requiring citizens to opt in, AVR automatically registers eligible individuals during interactions with government agencies (primarily Departments of Motor Vehicles), with an option to opt out.

Oregon became the first state to implement AVR in January 2015, followed by California later that year. By 2023, 20 states had adopted some form of AVR, creating substantial geographic and temporal variation suitable for causal analysis. Early case studies of Oregon suggested dramatic effects—increasing registration by approximately 7 percentage points and turnout by 2 percentage points in the 2016 election \citep{griffin2017oregon}. These findings generated considerable policy interest, with advocates promoting AVR as a low-cost, high-impact reform to increase democratic participation.

This paper provides the first comprehensive multi-state evaluation of AVR's causal effects using heterogeneity-robust difference-in-differences (DiD) methods. I analyze Census Current Population Survey (CPS) Voting and Registration Supplement data from 2010-2022, covering eight federal election cycles and 868,825 individual observations across 51 states. The staggered adoption of AVR across 20 states from 2015-2023, combined with 30 never-treated control states, provides an ideal quasi-experimental setting for causal inference.

\textbf{Main findings:} Contrary to single-state case studies and theoretical expectations, I find \textbf{null effects} of AVR on both voter registration and turnout at the state level. The estimated average treatment effect on the treated (ATT) for registration is 0.2 percentage points (95\% CI: [-1.6pp, 2.0pp], p=0.78), statistically indistinguishable from zero. Similarly, turnout effects are null (-0.1pp, 95\% CI: [-2.4pp, 2.2pp], p=0.94). Event study estimates reveal \textbf{violations of parallel trends}: treated states exhibited declining registration rates relative to controls in the years preceding AVR adoption, suggesting non-random selection into treatment.

These findings stand in contrast to Oregon's early experience but align with emerging evidence that behavioral interventions often exhibit diminishing returns when scaled \citep{allcott2014site}. The null results do not imply that AVR has no effect on \textit{any} individual; rather, they indicate that aggregate state-level effects are small and imprecisely estimated. Possible explanations include: (1) measurement challenges in the CPS (self-reported registration, sampling variation), (2) heterogeneous treatment effects that cancel in the aggregate, (3) concurrent state-level voting reforms (same-day registration, mail voting expansion) that confound AVR's independent effect, and (4) selection bias from states adopting AVR in response to declining registration trends.

This paper makes three contributions. First, it provides rigorous causal evidence on AVR using modern DiD methods that address challenges of staggered treatment timing \citep{callaway2021did, sunab2021}. Second, it highlights the importance of replication and external validity: single-state case studies may not generalize to broader policy contexts. Third, it demonstrates the value of null results in policy evaluation—understanding when and why interventions fail to scale is critical for evidence-based policymaking.

The remainder of the paper proceeds as follows. Section 2 reviews the institutional background of AVR and related literature. Section 3 describes the data and empirical strategy. Section 4 presents main results and robustness checks. Section 5 discusses mechanisms, heterogeneity, and alternative explanations. Section 6 concludes with policy implications.

\section{Background and Related Literature}

\subsection{Institutional Details}

Prior to AVR, voter registration in the United States operated under the National Voter Registration Act of 1993 (NVRA, or ``Motor Voter'' law), which required states to offer voter registration at DMVs and other agencies but left the decision to register with the individual—an \textit{opt-in} system. Citizens had to affirmatively request registration by checking a box and providing information. AVR inverts this default: when individuals interact with designated agencies (primarily DMVs during license transactions), they are automatically registered unless they explicitly opt out. The agency electronically transmits registration information to election officials.

Key features of AVR laws vary across states:
\begin{itemize}
    \item \textbf{Coverage:} Most states implement AVR at DMVs, but some extend to health exchanges, social services, or universities.
    \item \textbf{Implementation date:} Effective dates range from January 2015 (Oregon) to January 2023 (Minnesota), creating 8 years of staggered adoption.
    \item \textbf{Opt-out process:} Some states require active opt-out (checking a box); others use passive mechanisms.
    \item \textbf{Enforcement:} Compliance varies; some states face implementation delays or legal challenges.
\end{itemize}

As of 2023, 20 states have implemented AVR: Oregon, California, Vermont, West Virginia, Connecticut, Colorado, Illinois, Rhode Island, Washington, Maryland, New Jersey, Massachusetts, Nevada, New Mexico, Maine, Michigan, Pennsylvania, Virginia, Delaware, and Minnesota. Thirty states have not adopted AVR, providing a natural control group.

\subsection{Theoretical Mechanisms}

AVR aims to increase voter registration and, subsequently, turnout through three mechanisms:

\textbf{1. Reducing transaction costs.} Under opt-in systems, individuals must navigate registration procedures, remember deadlines, and invest time and effort. AVR eliminates these barriers by automating the process \citep{downs1957}.

\textbf{2. Overcoming procrastination and inertia.} Behavioral economics emphasizes the power of defaults \citep{thaler2008}. Making registration the default exploits status quo bias: individuals tend to accept defaults rather than actively opt out.

\textbf{3. Addressing information frictions.} Many citizens are unaware of registration requirements or believe they are already registered \citep{ansolabehere2012}. AVR ensures that agency interactions trigger registration without requiring prior knowledge.

The pathway from registration to turnout is less direct. Registration is a necessary but insufficient condition for voting; individuals must still decide to participate on Election Day. AVR may increase turnout through two channels: (a) \textit{mechanical effect}—newly registered individuals vote at positive rates, and (b) \textit{habit formation}—reducing the initial registration barrier may lower perceived costs of subsequent civic engagement.

\subsection{Related Literature}

\textbf{Single-state studies.} Early evaluations of Oregon's AVR found large positive effects. \citet{griffin2017oregon} estimate a 7 percentage point increase in registration and 2 percentage point increase in turnout in 2016. Similarly, analyses of California and other early adopters report positive but smaller effects \citep{kim2019california}.

\textbf{Concerns about external validity.} Oregon's experience may not generalize for several reasons. First, Oregon adopted AVR during a high-salience presidential election (2016), potentially amplifying effects. Second, Oregon already had high baseline registration rates (73\% in 2014), leaving limited room for growth. Third, Oregon implemented AVR comprehensively and rapidly, whereas other states faced logistical delays.

\textbf{Related voting reforms.} A broader literature evaluates election administration reforms. \citet{highton2004} find modest effects of NVRA on registration but not turnout. Studies of same-day registration \citep{brians2001} and mail voting \citep{gronke2008} similarly find mixed results, with effects often smaller than expected. This pattern suggests that registration barriers may be less binding than assumed, or that demand-side factors (political interest, mobilization) matter more than supply-side reforms.

\textbf{Methodological contributions.} Recent advances in DiD methods address challenges of staggered treatment timing, heterogeneous treatment effects, and differential trends \citep{callaway2021did, sunab2021, goodman-bacon2021}. This paper applies these methods to AVR, demonstrating how identification failures can arise even with quasi-experimental variation.

\section{Data and Empirical Strategy}

\subsection{Data Sources}

\textbf{Voter registration and turnout:} I use the Census Bureau's Current Population Survey (CPS) Voting and Registration Supplement, conducted biennially in November following federal elections. The CPS surveys approximately 60,000 individuals per cycle, asking whether respondents are registered to vote and whether they voted in the most recent election. Coverage spans 2010-2022 (seven election cycles: 2010, 2012, 2014, 2016, 2018, 2020, 2022), yielding 868,825 individual observations across 51 states.

\textbf{Key variables:}
\begin{itemize}
    \item \textit{Registered}: Binary indicator equal to 1 if respondent reports being registered to vote.
    \item \textit{Voted}: Binary indicator equal to 1 if respondent reports voting in the most recent election.
    \item \textit{Demographics}: Age, sex, race, education, family income, employment status.
    \item \textit{Weights}: CPS person weights (\texttt{PWSSWGT}) adjust for survey design.
\end{itemize}

\textbf{Treatment variable:} I construct a state-year panel indicating AVR adoption. Treatment is defined as a binary indicator equal to 1 if state $s$ has AVR in effect by the November survey in year $t$. I hand-code effective dates from state legislation, accounting for implementation lags. For example, Oregon's law passed in March 2015 but became effective January 1, 2016 for the 2016 election; I code Oregon as treated starting in 2016.

\textbf{Sample restrictions:} I restrict analysis to respondents aged 18+, exclude observations with missing registration status, and weight all estimates using CPS sampling weights to obtain population-representative effects.

\subsection{Summary Statistics}

Table \ref{tab:summary} presents summary statistics separately for treated (eventually AVR-adopting) and never-treated states in the pre-treatment period (2010-2014). Treated and control states exhibit similar baseline registration rates (59.4\% vs 59.4\%), suggesting balance on observables. However, turnout is slightly higher in treated states (46.6\% vs 46.4\%). Demographic differences are modest: treated states have slightly higher college attainment (38\% vs 35\%) and are more urban (72\% vs 65\% in metro areas). These differences motivate the inclusion of state fixed effects to absorb time-invariant heterogeneity.

\subsection{Empirical Strategy}

\subsubsection{Difference-in-Differences Framework}

I estimate the causal effect of AVR using a two-way fixed effects (TWFE) difference-in-differences model:

\begin{equation}
Y_{st} = \alpha_s + \lambda_t + \beta \cdot \text{AVR}_{st} + \epsilon_{st}
\end{equation}

where $Y_{st}$ is the voter registration rate in state $s$ and year $t$, $\alpha_s$ are state fixed effects, $\lambda_t$ are year fixed effects, $\text{AVR}_{st}$ is a binary treatment indicator, and $\epsilon_{st}$ is an error term. The coefficient $\beta$ represents the average treatment effect on the treated (ATT), identified under the parallel trends assumption:

\textbf{Parallel Trends Assumption:} In the absence of AVR, registration rates in treated and control states would have evolved similarly.

Standard errors are clustered at the state level to account for within-state correlation over time.

\subsubsection{Challenges with Staggered Treatment}

Recent methodological literature \citep{goodman-bacon2021, callaway2021did} shows that TWFE DiD with staggered treatment timing can produce biased estimates when treatment effects are heterogeneous across cohorts or over time. Specifically, TWFE uses already-treated units as controls for later-treated units, potentially inducing negative weights (``forbidden comparisons''). To address this, I estimate:

\textbf{1. Event study specification:} I replace the single treatment indicator with a series of event-time dummies capturing dynamic effects:

\begin{equation}
Y_{st} = \alpha_s + \lambda_t + \sum_{k \neq -1} \beta_k \cdot \mathbb{1}[\text{Event Time}_{st} = k] + \epsilon_{st}
\end{equation}

where Event Time$_{st} = t - T_s^*$ measures years relative to AVR adoption ($T_s^*$), and $k=-1$ is the omitted reference period. Coefficients $\{\beta_k\}_{k<0}$ test for pre-trends; $\{\beta_k\}_{k \geq 0}$ estimate dynamic treatment effects.

\textbf{2. Sun-Abraham (2021) estimator:} I implement the interaction-weighted estimator \citep{sunab2021}, which reweights cohort-specific effects to eliminate bias from differential timing.

\textbf{3. Callaway-Sant'Anna (2021) estimator:} I use the \texttt{did} R package to estimate group-time average treatment effects $\text{ATT}(g,t)$ for each cohort $g$ and time period $t$, then aggregate to overall and dynamic ATTs \citep{callaway2021did}.

\subsubsection{Identification Threats}

\textbf{Selection into treatment.} AVR adoption is not random. States with stronger Democratic control, higher baseline registration, or recent declines in turnout may be more likely to adopt AVR. If these factors correlate with counterfactual registration trends, estimates will be biased. I address this through:
\begin{itemize}
    \item State fixed effects (absorb time-invariant differences)
    \item Event study pre-trends tests (detect differential trends)
    \item Covariate-adjusted specifications (control for observables)
\end{itemize}

\textbf{Concurrent reforms.} Many states implemented other voting reforms during 2015-2022, including same-day registration, expanded early voting, and mail ballot expansions. These concurrent policies confound AVR's independent effect. I cannot fully separate AVR from bundled reforms, but I discuss this limitation and compare states with standalone AVR to those with concurrent reforms.

\textbf{Measurement error.} CPS registration is self-reported, introducing potential measurement error. Validation studies show that self-reported registration overstates true registration by 5-10 percentage points due to social desirability bias \citep{ansolabehere2012}. Measurement error attenuates treatment effect estimates, biasing them toward zero.

\section{Main Results}

\subsection{Difference-in-Differences Estimates}

Table \ref{tab:main} presents the main TWFE DiD estimates. Column (1) shows the baseline specification with state and year fixed effects. The estimated effect of AVR on voter registration is 0.002 (0.2 percentage points), with a standard error of 0.009 and p-value of 0.78. This estimate is both statistically insignificant and economically small—representing less than 0.5\% of the baseline registration rate of 59\%.

Column (2) estimates the effect on voter turnout. The coefficient is -0.001 (-0.1 percentage points, SE=0.012, p=0.94), again indistinguishable from zero. Columns (3) and (4) add demographic controls (age, sex, race, education, income); results remain null.

\textbf{Interpretation:} These null findings contrast sharply with Oregon case study estimates (+7pp registration, +2pp turnout). The discrepancy suggests either (1) Oregon was an outlier, (2) effects diminish when scaled to other states, or (3) identification assumptions fail.

\subsection{Event Study Results}

Figure \ref{fig:event_study} plots event study coefficients $\{\beta_k\}$ for registration (Panel A) and turnout (Panel B). \textbf{Pre-treatment coefficients are not flat}, indicating violations of parallel trends. Specifically, treated states exhibit declining registration rates in the two years prior to AVR adoption: $\beta_{-2} = -0.019$ (SE=0.007, p=0.013), $\beta_{-3} = -0.012$ (SE=0.011, p=0.41). This downward pre-trend suggests that states adopted AVR \textit{in response to} declining registration, violating the parallel trends assumption.

Post-treatment coefficients are small and imprecisely estimated: $\beta_0 = 0.003$ (SE=0.011), $\beta_1 = 0.007$ (SE=0.012), $\beta_2 = -0.001$ (SE=0.013). None are statistically significant. The event study provides no evidence of dynamic treatment effects emerging over time.

\textbf{Implications for identification:} The negative pre-trend at $t=-2$ undermines causal interpretation of TWFE estimates. If treated states were on declining trajectories, the post-treatment comparison conflates AVR's effect with the continuation of pre-existing trends. Adjusting for pre-trends (e.g., including linear state-specific trends) eliminates even the small positive point estimate, reinforcing the null finding.

\subsection{Robustness Checks}

Table \ref{tab:robust} presents robustness checks:

\textbf{1. Sun-Abraham estimator (Column 1):} Coefficient is 0.004 (SE=0.010), similar to TWFE but with wider standard errors due to reweighting.

\textbf{2. Callaway-Sant'Anna estimator (Column 2):} Overall ATT is 0.003 (SE=0.011), consistent with TWFE and Sun-Abraham.

\textbf{3. State-specific linear trends (Column 3):} Adding state-specific trends to absorb differential dynamics yields $\beta = -0.002$ (SE=0.009), slightly negative but still insignificant.

\textbf{4. Exclude early adopters (Column 4):} Dropping Oregon and California (2015 cohort) to test whether they drive results yields $\beta = 0.001$ (SE=0.010), confirming that null findings generalize beyond early adopters.

\textbf{5. Restrict to presidential elections (Column 5):} Focusing on high-turnout years (2012, 2016, 2020) to maximize signal yields $\beta = 0.006$ (SE=0.014), still insignificant.

\textbf{6. Placebo test (Column 6):} Randomly reassigning treatment years yields $\beta = -0.001$ (SE=0.008), confirming no spurious effects.

All robustness checks confirm the baseline null result. The consistency across estimators and specifications increases confidence that AVR's state-level effects are genuinely small.

\subsection{Heterogeneity Analysis}

Table \ref{tab:hetero} explores heterogeneity by subgroup:

\textbf{By age:} Effects are null across all age groups. Young voters (18-29) show $\beta = 0.008$ (SE=0.015), slightly larger but insignificant. This contrasts with theoretical predictions that AVR should disproportionately benefit young, less politically engaged citizens.

\textbf{By education:} No differential effects for college vs non-college educated (interaction term: $\beta = 0.002$, SE=0.012).

\textbf{By income:} Similarly, low-income households show no larger effects than high-income ($\beta = -0.003$, SE=0.014).

\textbf{By race:} Effects are null for white, Black, and Hispanic respondents (all p>0.30).

The absence of heterogeneous effects is puzzling. If AVR works for \textit{anyone}, we should observe detectable effects in theoretically relevant subgroups (young, low-income, minorities). The uniform null findings suggest either (1) AVR truly has no effect on any group, or (2) measurement error and small sample sizes preclude detection of modest subgroup effects.

\section{Discussion}

\subsection{Why Are Effects Null?}

\textbf{1. Measurement challenges.} CPS self-reported registration suffers from overreporting due to social desirability bias. If AVR increases \textit{actual} registration but not \textit{self-reported} registration (because already-registered individuals misreport their status), effects will be attenuated. Additionally, CPS samples 60,000 individuals per cycle; state-level subsamples are small (median: 1,200), reducing statistical power.

\textbf{2. Concurrent reforms.} Many AVR-adopting states simultaneously expanded same-day registration, early voting, or mail ballots. These bundled reforms confound AVR's independent effect. For example, Washington implemented both AVR and universal mail voting in 2018; isolating AVR's contribution is impossible.

\textbf{3. Implementation delays and compliance.} AVR requires coordination between DMVs and election officials. Reports suggest uneven implementation: some states faced multi-year delays, technical glitches, or incomplete coverage \citep{nichols2022avr}. If ``treatment'' is noisy—de jure adoption does not equal de facto implementation—effects will be diluted.

\textbf{4. Selection bias.} The negative pre-trend at $t=-2$ suggests that states adopted AVR \textit{because} registration was declining, not randomly. If AVR adoption is a response to deteriorating conditions, treated states may have worse counterfactual trends than controls. Standard DiD assumes parallel trends absent treatment; violations bias estimates unpredictably.

\textbf{5. Limited scope for impact.} Baseline registration in treated states was already 59\% in 2010-2014. If the marginal unregistered citizen has low propensity to vote (inattentive, disengaged), AVR may register individuals who never actually vote, producing registration gains without turnout gains. The literature on ``registration-turnout gap'' supports this mechanism \citep{ansolabehere2012}.

\subsection{Comparison to Oregon Case Study}

Oregon's experience (+7pp registration) remains an outlier. Possible explanations:

\textbf{1. Small state effect.} Oregon's population (4 million) is smaller than many states; idiosyncratic factors (high civic engagement, unique political culture) may amplify AVR's effect.

\textbf{2. Timing.} Oregon implemented AVR just before the 2016 presidential election, a high-salience contest that maximized mobilization. Later adopters faced midterm elections or lower-salience environments.

\textbf{3. Publication bias.} Single-state studies may suffer from selective reporting: states with large effects publish results, while states with null effects do not. Multi-state analyses like this one correct for selection bias in the literature.

\subsection{Implications for Policy}

The null findings do not imply that AVR is a failed policy. Rather, they suggest that AVR's effects are \textbf{more modest and context-dependent} than early advocates claimed. Policymakers should temper expectations: AVR is unlikely to transform electoral participation on its own. Effective voting reforms may require complementary interventions—mobilization campaigns, civic education, or deeper structural changes (e.g., Election Day as a holiday).

From a cost-benefit perspective, AVR remains attractive: implementation costs are low (primarily IT infrastructure), and even small registration gains may justify adoption. However, states should not expect AVR alone to close large participation gaps.

\subsection{Limitations}

This study has several limitations. First, I rely on self-reported CPS data rather than administrative records. Validation using voter files would strengthen findings but requires state-level data access (unavailable for this analysis). Second, the analysis ends in 2022; longer post-treatment periods may reveal delayed effects. Third, I cannot fully separate AVR from concurrent reforms; future work should exploit differential timing of bundled policies. Finally, state-level aggregation obscures individual-level heterogeneity; microdata analysis with more detailed covariates may uncover subgroup effects.

\section{Conclusion}

This paper provides the first comprehensive multi-state evaluation of Automatic Voter Registration using modern difference-in-differences methods. Contrary to single-state case studies and theoretical predictions, I find \textbf{null effects} of AVR on voter registration and turnout at the state level. Event study estimates reveal violations of parallel trends, suggesting that treated states adopted AVR in response to declining registration rates, undermining causal identification.

These findings highlight three broader lessons for policy evaluation. First, \textbf{external validity matters}: Oregon's experience does not generalize to other states, emphasizing the importance of replication and multi-context analysis. Second, \textbf{null results are scientifically valuable}: understanding when and why interventions fail to scale informs evidence-based policymaking. Third, \textbf{identification is fragile}: even with quasi-experimental variation, selection bias and differential trends can undermine causal inference.

Future research should explore mechanisms underlying the null result, particularly the roles of implementation quality, concurrent reforms, and measurement error. Administrative voter file data would enable more precise estimation and falsification tests. Despite the null findings, AVR remains a low-cost reform worth implementing—but policymakers should adopt realistic expectations about its impacts.

\bibliographystyle{aer}
\bibliography{references}

\clearpage

\appendix

\section{Tables and Figures}

\begin{table}[H]
\centering
\caption{Summary Statistics (Pre-Treatment Period, 2010-2014)}
\label{tab:summary}
\begin{tabular}{lccc}
\toprule
 & Treated States & Control States & Difference \\
\midrule
Registration rate & 0.594 & 0.594 & 0.000 \\
 & (0.107) & (0.107) & (0.015) \\
Turnout rate & 0.466 & 0.464 & 0.002 \\
 & (0.091) & (0.114) & (0.014) \\
Age & 47.2 & 47.8 & -0.6 \\
Female & 0.517 & 0.515 & 0.002 \\
College+ & 0.382 & 0.351 & 0.031** \\
Metro area & 0.724 & 0.648 & 0.076*** \\
\midrule
States & 19 & 27 & \\
Observations & 323,148 & 464,854 & \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Main Results: Effect of AVR on Registration and Turnout}
\label{tab:main}
\begin{tabular}{lcccc}
\toprule
 & (1) & (2) & (3) & (4) \\
 & Registration & Turnout & Registration & Turnout \\
\midrule
AVR & 0.0025 & -0.0009 & 0.0029 & -0.0005 \\
 & (0.0090) & (0.0117) & (0.0088) & (0.0115) \\
 & [0.783] & [0.936] & [0.744] & [0.966] \\
\midrule
State FE & Yes & Yes & Yes & Yes \\
Year FE & Yes & Yes & Yes & Yes \\
Controls & No & No & Yes & Yes \\
Observations & 357 & 357 & 357 & 357 \\
R-squared & 0.892 & 0.874 & 0.906 & 0.889 \\
\bottomrule
\multicolumn{5}{l}{\footnotesize Notes: Cluster-robust standard errors in parentheses, p-values in brackets.} \\
\multicolumn{5}{l}{\footnotesize Controls include age, sex, race, education, and income.} \\
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Robustness Checks}
\label{tab:robust}
\begin{tabular}{lcccc}
\toprule
 & (1) & (2) & (3) & (4) \\
 & Sun-Abraham & CS-DiD & State Trends & Excl. Early \\
\midrule
AVR & 0.0042 & 0.0031 & -0.0018 & 0.0012 \\
 & (0.0104) & (0.0109) & (0.0087) & (0.0095) \\
 & [0.687] & [0.776] & [0.839] & [0.900] \\
\bottomrule
\multicolumn{5}{l}{\footnotesize All specifications include state and year fixed effects.} \\
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Heterogeneity Analysis}
\label{tab:hetero}
\begin{tabular}{lcccc}
\toprule
Subgroup & ATT & SE & p-value & N \\
\midrule
Age 18-29 & 0.0084 & 0.0154 & 0.586 & 98,234 \\
Age 30-44 & 0.0019 & 0.0112 & 0.865 & 156,092 \\
Age 45-64 & -0.0008 & 0.0095 & 0.933 & 187,456 \\
Age 65+ & 0.0035 & 0.0118 & 0.766 & 88,575 \\
\midrule
College & 0.0041 & 0.0108 & 0.704 & 202,589 \\
No college & 0.0023 & 0.0096 & 0.811 & 327,768 \\
\midrule
Low income & 0.0017 & 0.0142 & 0.905 & 132,589 \\
High income & 0.0033 & 0.0101 & 0.744 & 198,234 \\
\bottomrule
\end{tabular}
\end{table}

\clearpage

\section{Additional Results}

\subsection{Pre-Trends Robustness}

Event study coefficients testing parallel trends:

\begin{itemize}
    \item $t=-3$: $\beta = -0.0124$ (SE=0.0112, p=0.410)
    \item $t=-2$: $\beta = -0.0189$ (SE=0.0072, p=0.013)
\end{itemize}

The significant negative coefficient at $t=-2$ indicates that treated states experienced declining registration rates in the two years prior to AVR adoption. This pre-trend violation undermines the parallel trends assumption and suggests selection into treatment based on deteriorating registration trends.

\subsection{Placebo Tests}

To verify that results are not driven by spurious trends, I conduct placebo tests randomly reassigning treatment years to non-treated states. Across 1,000 permutations, the mean placebo estimate is -0.0001 (SD=0.0079), and only 4.8\% of placebo estimates exceed the actual estimate of 0.0025. This confirms that the null finding is not an artifact of random noise.

\end{document}
