{
  "paper_id": "apep_0053",
  "scan_date": "2026-02-06T12:37:02.619455+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 8,
  "flags": [
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "01_fetch_cps_voting.R",
      "lines": [
        176,
        177,
        178,
        179,
        180
      ],
      "evidence": "The manuscript states AVR treatment is coded as in effect by the November CPS survey and explicitly gives Oregon as treated starting in 2016 due to implementation timing. The code instead assigns Oregon cohort=2015 with avr_effective=2015-01-01 (and similarly California cohort=2015), which contradicts the paper\u2019s stated treatment timing/implementation-lag handling. This can materially change event-time alignment and ATT estimates.: avr_states <- tribble(\n  ~state, ~state_fips, ~state_abbr, ~avr_effective, ~cohort,\n  \"Oregon\", \"41\", \"OR\", \"2015-01-01\", 2015,\n  \"California\", \"06\", \"CA\", \"2015-10-10\", 2015,\n  ...\n)\n...\nmutate(\n  avr_effective = as.Date(avr_effective),\n  year_effective = year(avr_effective),\n  ever_treated = TRUE\n)",
      "confidence": 0.9
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "02_clean_data.R",
      "lines": [
        90,
        91,
        92,
        93,
        94,
        95
      ],
      "evidence": "The manuscript defines treatment as AVR in effect by the November survey and claims hand-coding of effective dates with implementation lags. The analysis code uses a coarse rule treated = 1{year >= cohort} and does not use avr_effective month/day or the election-month alignment. If a state\u2019s law becomes effective mid-year or after November, this coding disagrees with the paper\u2019s definition.: treated = case_when(\n  !ever_treated ~ 0,\n  is.na(cohort) ~ 0,\n  year >= cohort ~ 1,  # Treated if year >= cohort year\n  TRUE ~ 0\n)",
      "confidence": 0.8
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "HIGH",
      "file": "01_fetch_cps_voting.R",
      "lines": [
        128,
        129
      ],
      "evidence": "The fetch script writes the raw CPS data and AVR treatment database to output/paper_66/data/..., but downstream scripts (02_clean_data.R, 05_figures.R) read from data/... (different directory). Without an explicit copy/move step, the provenance chain is broken and it is unclear what data files were actually used to produce results.: saveRDS(cps_voting, \"output/paper_66/data/cps_voting_raw.rds\")\n...\nwrite_csv(full_avr_db, \"output/paper_66/data/avr_treatment_database.csv\")",
      "confidence": 0.9
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "HIGH",
      "file": "02_clean_data.R",
      "lines": [
        13,
        14
      ],
      "evidence": "These inputs are not created anywhere in the provided code at the same paths (the fetch script saves to output/paper_66/data/...). Unless the user manually moved files, the analysis can silently run on stale/other data. This undermines traceability and replicability.: cps_raw <- readRDS(\"data/cps_voting_raw.rds\")\n...\navr_db <- read_csv(\"data/avr_treatment_database.csv\", show_col_types = FALSE)",
      "confidence": 0.9
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "paper.tex",
      "lines": [
        224,
        225,
        226,
        227
      ],
      "evidence": "Key headline results (point estimates, SEs, p-values) are hard-coded in the manuscript text. No LaTeX table-generation script or automated export (e.g., modelsummary/texreg/esttab) is provided to verify that these values are programmatically produced from model objects. This creates risk of transcription errors or post-hoc editing inconsistent with the code outputs.: I find \\textbf{null results}: AVR adoption is associated with a statistically insignificant 0.2 percentage point increase in registration rates (SE=0.9pp, p=0.78) and a -0.1 percentage point change in turnout (SE=1.2pp, p=0.94).",
      "confidence": 0.85
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "paper.tex",
      "lines": [
        323,
        324,
        325,
        326,
        327,
        328,
        329
      ],
      "evidence": "Regression table entries (coefficients/SEs/p-values) are embedded as literals in LaTeX rather than being generated from the R model objects in 03_main_analysis*.R. Without a reproducible table pipeline, it is not possible to audit whether the manuscript tables match the code\u2019s computed results.: \\begin{table}[H]\n\\centering\n\\caption{Main Results: Effect of AVR on Registration and Turnout}\n...\nAVR & 0.0025 & -0.0009 & 0.0029 & -0.0005 \\\\\n & (0.0090) & (0.0117) & (0.0088) & (0.0115) \\\\\n & [0.783] & [0.936] & [0.744] & [0.966] \\\\\n...",
      "confidence": 0.9
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "03_main_analysis.R",
      "lines": [
        190,
        191
      ],
      "evidence": "Multiple scripts write to the same results file name (data/did_results.rds). In particular, 03_main_analysis.R saves a list that does NOT include df_state_year/es_reg/es_vote, while 03_main_analysis_fast.R saves a list that DOES include those elements. Which script was run last determines what 05_figures.R will plot. This creates a high risk of inadvertently mixing outputs from different estimation approaches without noticing.: saveRDS(results, \"data/did_results.rds\")",
      "confidence": 0.85
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "03_main_analysis_fast.R",
      "lines": [
        170,
        171
      ],
      "evidence": "Same overwrite concern as above: the 'fast' aggregate-panel analysis overwrites the main individual-level CS-DiD results file. This can lead to figures/tables reflecting a different specification than the manuscript describes (e.g., CS-DiD vs TWFE on aggregates), depending on execution order.: saveRDS(results, \"data/did_results.rds\")",
      "confidence": 0.85
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "02_clean_data.R",
      "lines": [
        64,
        65
      ],
      "evidence": "CPS income variable HEFAMINC typically includes special codes for nonresponse/NIU in some years. The code does not recode such values to NA before constructing quartiles. If special codes are included, quartiles can be distorted and could affect covariate-adjusted estimates. This is likely a fixable cleaning omission but should be documented/handled explicitly.: income_quartile = ntile(HEFAMINC, 4),",
      "confidence": 0.6
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "01_fetch_data_v2.R",
      "lines": [
        121,
        122,
        123,
        124,
        125,
        126,
        127,
        128,
        129
      ],
      "evidence": "This script creates a placeholder panel (explicitly labeled placeholder) for a different project topic (pay transparency). It is not used in the AVR analysis, but its presence in the repository can create confusion about whether any outcomes were simulated. Because it is NA-filled and clearly marked as placeholder, this is low severity but should be separated from the AVR project to avoid ambiguity.: # Create Simulated Gender Wage Gap Data (PLACEHOLDER)\n...\ncat(\"   Creating placeholder structure for now.\\n\")\n...\nmutate(\n  log_wage_male = NA_real_,\n  log_wage_female = NA_real_,\n  wage_gap = NA_real_,\n  n_male = NA_integer_,\n  n_female = NA_integer_\n)",
      "confidence": 0.75
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis_fast.R",
      "verdict": "CLEAN"
    },
    {
      "file": "01_fetch_cps_voting.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "01_fetch_data_v2.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "paper.tex",
      "verdict": "SUSPICIOUS"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 5,
      "MEDIUM": 3,
      "LOW": 2
    },
    "one_liner": "method mismatch; unclear provenance",
    "executive_summary": "The AVR treatment coding in `01_fetch_cps_voting.R` does not match the manuscript\u2019s stated methodology, including the timing of Oregon\u2019s treated status relative to the November CPS survey, creating a direct mismatch between the described design and the implemented assignment. The data pipeline is not reproducible because the fetch script saves raw CPS and AVR inputs under `output/paper_66/data/...` while downstream scripts read from `data/...`, implying missing/relocated inputs and allowing the analysis to run on unstated files. Key headline results and regression table values are hard-coded directly into `paper.tex` rather than being generated from the R model outputs, preventing verification that the reported estimates correspond to the executed code.",
    "top_issues": [
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript states AVR treatment is coded as in effect...",
        "file": "01_fetch_cps_voting.R",
        "lines": [
          176,
          177
        ],
        "github_url": "/apep_0053/code/01_fetch_cps_voting.R#L176-L180"
      },
      {
        "category": "DATA_PROVENANCE_MISSING",
        "severity": "HIGH",
        "short": "The fetch script writes the raw CPS data and AVR treatmen...",
        "file": "01_fetch_cps_voting.R",
        "lines": [
          128,
          129
        ],
        "github_url": "/apep_0053/code/01_fetch_cps_voting.R#L128-L129"
      },
      {
        "category": "DATA_PROVENANCE_MISSING",
        "severity": "HIGH",
        "short": "These inputs are not created anywhere in the provided cod...",
        "file": "02_clean_data.R",
        "lines": [
          13,
          14
        ],
        "github_url": "/apep_0053/code/02_clean_data.R#L13-L14"
      }
    ],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0053_scan.json"
  },
  "error": null
}