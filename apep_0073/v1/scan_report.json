{
  "paper_id": "apep_0073",
  "scan_date": "2026-02-06T12:41:07.319862+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SEVERE",
  "files_scanned": 11,
  "flags": [
    {
      "category": "DATA_FABRICATION",
      "severity": "CRITICAL",
      "file": "analysis_simulated.py",
      "lines": [
        12,
        14,
        98,
        115,
        126
      ],
      "evidence": "This script explicitly generates simulated state-year employment outcomes (including an assumed true treatment effect and random shocks) and then saves them to ../data/panel.csv, which downstream figure code (05_figures.py) reads. If this script is run in the project pipeline (even unintentionally), it can replace real empirical results with simulated results while still producing plausible-looking tables/figures. The manuscript presents the findings as using real ACS/CPS/BLS/USDA data rather than simulated outcomes.: np.random.seed(42)\n...\n# State-specific shock (random)\nshock = np.random.normal(0, 0.008)\n...\nTRUE_TREATMENT_EFFECT = 0.005  # 0.5 pp - small positive effect\n...\npanel = pd.DataFrame(data)\n...\npanel.to_csv('../data/panel.csv', index=False)",
      "confidence": 0.98
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "05_figures.py",
      "lines": [
        60,
        78,
        83
      ],
      "evidence": "Event-study standard errors are not computed from the model/estimator; they are fabricated via a constant scaling factor (1.2) applied to the overall SE. This can materially change confidence intervals and statistical significance visualized in the paper, and conflicts with the manuscript's claim of clustered/wild bootstrap inference for dynamic effects.: # Standard errors (estimated from overall SE, scaled)\n# In a proper analysis, these would come from bootstrapping\nes_df['se'] = results['se'] * 1.2  # Approximate event-time SE",
      "confidence": 0.95
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "HIGH",
      "file": "01_fetch_data.R",
      "lines": [
        12,
        19,
        20,
        21,
        22,
        23,
        24,
        25,
        26,
        27,
        28,
        29,
        30,
        31,
        32,
        33,
        34,
        35,
        36,
        37,
        38,
        39,
        40,
        41,
        42,
        43,
        44,
        45,
        46,
        47,
        48,
        49,
        50,
        51,
        52,
        53,
        54,
        55,
        56,
        57,
        58,
        59,
        60,
        61,
        62,
        63,
        64,
        65,
        66,
        67,
        68,
        69,
        70,
        71,
        72,
        73,
        74,
        75,
        76,
        77,
        78
      ],
      "evidence": "USDA FNS waiver status (the key treatment variable) is not fetched from USDA or any archived records; it is manually entered as a large hard-coded table. The manuscript states waiver status comes from USDA FNS waiver records, but the code provides no reproducible provenance (download, parsing, citation of specific files, or checksum). This is a key identification variable; manual coding without traceable source is a major integrity risk.: waiver_data <- tribble(\n  ~state_fips, ~state_name, ~fy2010, ~fy2011, ~fy2012, ~fy2013, ~fy2014, ~fy2015, ~fy2016, ~fy2017, ~fy2018, ~fy2019,\n  \"01\", \"Alabama\",       1, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n  ...\n  \"56\", \"Wyoming\",       1, 1, 1, 1, 1, 0, 0, 0, 0, 0\n)",
      "confidence": 0.93
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "01_fetch_data.R",
      "lines": [
        118,
        119,
        120,
        121
      ],
      "evidence": "The manuscript defines the never-treated control group as 6 states: Minnesota, Montana, North Dakota, South Dakota, Utah, Vermont. The code hard-codes a different never-treated set (8 states) including New Hampshire (\"33\") and Virginia (\"51\"). This directly changes the control group composition and can change the ATT/DiD estimates and event-study patterns relative to what the paper claims.: never_treated <- waiver_data %>%\n  filter(state_fips %in% c(\"27\", \"30\", \"33\", \"38\", \"46\", \"49\", \"50\", \"51\")) %>%\n  pull(state_fips)",
      "confidence": 0.96
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "02_clean_data.R",
      "lines": [
        96,
        97,
        98,
        99,
        100,
        101,
        102,
        103
      ],
      "evidence": "The manuscript describes a design comparing the 2015 cohort (18 early-treated states) to never-treated controls (6 states), excluding later-treated and fluctuating states. The R analysis does not implement these sample restrictions: it keeps all states present in the ACS API pull and assigns treatment whenever first_treat > 0. This yields a multi-cohort staggered-treatment setup, not the single-cohort-vs-never-treated design emphasized in the manuscript as avoiding staggered-TWFE pitfalls.: first_treat = ifelse(is.na(first_treat), 0, first_treat),\n# Treatment indicator (post-reinstatement in treated state)\ntreated = ifelse(first_treat > 0 & year >= first_treat, 1, 0)",
      "confidence": 0.9
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "analysis_fast.py",
      "lines": [
        160,
        167,
        185,
        193,
        263,
        307
      ],
      "evidence": "This script constructs an 'emp_rate' outcome using a fixed assumed labor-force participation rate (0.78) rather than measuring employment-to-population from ACS/CPS microdata, then writes ../data/results.json. Multiple scripts (analysis.py, analysis_fast.py, analysis_simulated.py) write to the same output path (../data/results.json and/or ../data/panel.csv), creating a high risk that whichever script ran last determines the reported results/figures. This can enable cherry-picking/overwriting without obvious trace.: LFP_ABAWD = 0.78\nunemp_df['emp_rate'] = (1 - unemp_df['unemp_rate']/100) * LFP_ABAWD\n...\nwith open('../data/results.json', 'w') as f:\n    json.dump(results, f, indent=2)",
      "confidence": 0.9
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "analysis_fast.py",
      "lines": [
        158,
        159,
        160,
        161
      ],
      "evidence": "The script labels the constructed variable as an employment rate but it is not sourced from ACS/CPS/LAUS employment/population; it is an approximation using an assumed constant LFP. If used for the manuscript's employment rate outcome, it would contradict the paper's stated data construction from survey microdata/LAUS. At minimum, provenance and labeling are inconsistent and can mislead.: # Compute employment-to-population ratio (approximate)\n# Employment rate \u2248 (1 - unemployment_rate/100) * labor_force_participation\n# For ABAWD population (18-49), use approximate LFP of 78%\nLFP_ABAWD = 0.78",
      "confidence": 0.88
    },
    {
      "category": "STATISTICAL_IMPOSSIBILITY",
      "severity": "LOW",
      "file": "analysis_simulated.py",
      "lines": [
        318
      ],
      "evidence": "The p-value calculation is not a valid p-value function of the t-statistic; it effectively returns step values based on whether |t| exceeds 1.96. This is statistically incorrect (though the comment says approximate). If used in reporting, it would generate invalid inference.: 'p_value': round(2 * (1 - 0.5 * (1 + np.sign(abs(t_stat) - 1.96))), 3),  # Approximate",
      "confidence": 0.86
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "01_fetch_data.py",
      "verdict": "CLEAN"
    },
    {
      "file": "analysis.py",
      "verdict": "CLEAN"
    },
    {
      "file": "analysis_fast.py",
      "verdict": "CLEAN"
    },
    {
      "file": "analysis_simulated.py",
      "verdict": "SEVERE"
    },
    {
      "file": "05_figures.py",
      "verdict": "SUSPICIOUS"
    }
  ],
  "summary": {
    "verdict": "SEVERE",
    "counts": {
      "CRITICAL": 1,
      "HIGH": 4,
      "MEDIUM": 2,
      "LOW": 1
    },
    "one_liner": "fabricated data; hard-coded results; unclear provenance",
    "executive_summary": "The code writes simulated, fabricated state\u2013year employment outcomes (including an assumed \u201ctrue\u201d treatment effect plus random shocks) to `../data/panel.csv` in `analysis_simulated.py`, meaning downstream figures and estimates can be generated from synthetic data rather than the reported source data. In `05_figures.py`, event-study standard errors are not computed from the estimator but are instead manufactured by multiplying the overall SE by a constant factor (1.2), which can materially alter confidence intervals. The key treatment variable (USDA FNS waiver status) is not programmatically sourced or archived but manually hard-coded in `01_fetch_data.R`, and multiple scripts hard-code sample definitions that contradict the manuscript (different \u201cnever-treated\u201d states and failure to implement the stated 2015-cohort-vs-never-treated design in `02_clean_data.R`).",
    "top_issues": [
      {
        "category": "DATA_FABRICATION",
        "severity": "CRITICAL",
        "short": "This script explicitly generates simulated state-year emp...",
        "file": "analysis_simulated.py",
        "lines": [
          12,
          14
        ],
        "github_url": "/apep_0073/code/analysis_simulated.py#L12-L126"
      },
      {
        "category": "HARD_CODED_RESULTS",
        "severity": "HIGH",
        "short": "Event-study standard errors are not computed from the mod...",
        "file": "05_figures.py",
        "lines": [
          60,
          78
        ],
        "github_url": "/apep_0073/code/05_figures.py#L60-L83"
      },
      {
        "category": "DATA_PROVENANCE_MISSING",
        "severity": "HIGH",
        "short": "USDA FNS waiver status (the key treatment variable) is no...",
        "file": "01_fetch_data.R",
        "lines": [
          12,
          19
        ],
        "github_url": "/apep_0073/code/01_fetch_data.R#L12-L78"
      }
    ],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0073_scan.json"
  },
  "error": null
}