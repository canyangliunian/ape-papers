{
  "paper_id": "apep_0185",
  "scan_date": "2026-02-06T12:59:57.849512+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 8,
  "flags": [
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "HIGH",
      "file": "01_fetch_data.R",
      "lines": [
        86,
        130,
        150,
        170
      ],
      "evidence": "The manuscript states state minimum wages are compiled by cross-referencing DOL, NCSL, and the Vaghul\u2013Zipperer database (2010\u20132023). However, the code does not fetch or parse any of these sources; instead it manually hard-codes a partial curated list of changes for selected states. For all other states/quarters it silently imputes the federal floor (via later logic in 02_clean_data.R). This is a key input to the main constructed dataset, so provenance/replicability is incomplete without (i) the full policy file, (ii) citations/links per entry, or (iii) a reproducible fetch-and-parse script.: # Build minimum wage dataset from known policy changes\n# Sources: DOL, NCSL, Vaghul-Zipperer (2016)\n...\nstate_mw_changes <- tribble(\n  ~state_fips, ~state_abbr, ~date, ~min_wage,\n  # California\n  \"06\", \"CA\", \"2014-07-01\", 9.00,\n  ...\n  # Virginia\n  \"51\", \"VA\", \"2021-05-01\", 9.50,\n  \"51\", \"VA\", \"2022-01-01\", 11.00,\n  \"51\", \"VA\", \"2023-01-01\", 12.00\n)",
      "confidence": 0.9
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "03_main_analysis.R",
      "lines": [
        24,
        52,
        74,
        112,
        175
      ],
      "evidence": "The manuscript explicitly says: \u201cWe deliberately do not estimate causal effects \u2026 beyond the scope of this paper,\u201d and frames the contribution as descriptive/data construction. In contrast, the codebase contains a full causal-style analysis pipeline (shift-share DiD, horse race, event study, placebo industries, multiple outcomes) and produces tables titled \u201cMain Results: Effect of Network Exposure on Employment\u201d (06_tables.R). If these regressions are not in the paper, they should either be removed from the replication package, clearly marked as an optional extension, or the manuscript should be updated to align with the implemented analysis.: cat(\"=== Main Analysis: Network Shift-Share DiD ===\\n\\n\")\n...\n# Tier 2: Shift-Share with State \u00d7 Time FEs (Main Specification)\n\ntier2 <- feols(\n  log_emp ~ social_exposure | county_fips + state_fips^yearq,\n  data = panel,\n  cluster = ~state_fips\n)\n...\n# Event study: interact baseline exposure with year dummies\nevent_study <- feols(\n  log_emp ~ i(year_f, mean_social_exposure, ref = 2012) | county_fips + state_fips^yearq,\n  data = panel,\n  cluster = ~state_fips\n)",
      "confidence": 0.85
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        215,
        248,
        258
      ],
      "evidence": "The manuscript\u2019s exposure panel is 2010\u20132023 (56 quarters) and the descriptive statistics/tables are written for that full span. The code fetches QWI only for 2012\u20132022 (and even labels this as a demonstration subset). If downstream results/tables use QWI outcomes, they will not correspond to the paper\u2019s stated 2010\u20132023 coverage unless QWI is not actually used for paper outputs. This needs explicit documentation of what years are used for which outputs.: # Fetch QWI for sample of years (2012-2022) and quarters\n# Note: Full data would take long; fetching subset for demonstration\nyears_to_fetch <- 2012:2022\nquarters <- 1:4",
      "confidence": 0.8
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "MEDIUM",
      "file": "05_figures.R",
      "lines": [
        22,
        33,
        36
      ],
      "evidence": "The manuscript acknowledges filtering out observations with network exposure below $7.00, but the code applies this filter only in the figure-generation script (05_figures.R), not in the construction script (02_clean_data.R) that saves analysis_panel.rds. This creates a risk that (i) tables/regressions (using analysis_panel.rds) and (ii) figures/descriptive stats (using the filtered in-memory panel) are on different samples. If the filter is part of the official sample definition, it should be applied during data construction and saved to analysis_panel.rds (or explicitly versioned as \u201cfiltered_panel\u201d).: # Filter out counties with anomalous exposure values (data issues)\n# Remove clear outliers but retain observations near $7.25 threshold\n# Values slightly below $7.25 may reflect timing of quarterly averaging or territorial data\nvalid_exposure_threshold <- log(7.00)  # Remove clear outliers below $7.00\nn_before <- nrow(panel)\npanel <- panel %>%\n  filter(social_exposure >= valid_exposure_threshold)",
      "confidence": 0.85
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        55
      ],
      "evidence": "A global RNG seed is set for all scripts. This is generally fine for reproducibility, but it also affects any later use of sample()/randomization inference/permutations. Given the project includes randomization inference (04_robustness.R), the seed should ideally be set locally within the relevant routines (and recorded in outputs) to avoid accidental dependence on execution order across scripts.: # Set seed for reproducibility\nset.seed(42)",
      "confidence": 0.7
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "MEDIUM",
      "file": "04_robustness.R",
      "lines": [
        44,
        86,
        94
      ],
      "evidence": "This is not fabrication of the main dataset, but it is random data generation used for inference. The concern is methodological labeling: comments say \u201cPermute MW Timing,\u201d but the implemented permutation shuffles county exposure within each time period (and the more structural timing-permutation function is not actually used). This changes the null being tested (breaks cross-sectional exposure structure rather than policy timing), so the reported \u201crandomization inference p-value\u201d may not correspond to what is described.: compute_permuted_exposure <- function(panel, state_mw_panel, seed) {\n  set.seed(seed)\n  ...\n  panel_perm <- panel %>%\n    group_by(yearq) %>%\n    mutate(\n      social_exposure_perm = sample(social_exposure)\n    ) %>%\n    ungroup()\n  return(panel_perm)\n}\n...\n# Simple permutation: shuffle exposure across counties within time\npanel_perm <- panel %>%\n  group_by(yearq) %>%\n  mutate(social_exposure_perm = sample(social_exposure)) %>%\n  ungroup()",
      "confidence": 0.8
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "03_main_analysis.R",
      "lines": [
        146,
        175,
        220
      ],
      "evidence": "Multiple outcomes (employment, earnings, hiring) and multiple specifications (tiers, orthogonalization, event study) are estimated, but 06_tables.R only tabulates a subset (Tier 1\u20133 for employment; optional industry results; robustness). Without a pre-specified reporting plan in the manuscript (or explicit \u201cappendix/all-results\u201d tables), there is room for specification/outcome cherry-picking. At minimum, the replication package should include an \u201call outcomes/all specs\u201d table or a log of all estimated models to make reporting comprehensive.: # Different Outcome Variables\n...\nemp_model <- feols(...)\nearn_model <- feols(...)\nhire_model <- feols(...)\n...\nresults <- list(\n  ...\n  emp_model = emp_model,\n  earn_model = earn_model,\n  hire_model = hire_model\n)\nsaveRDS(results, \"../data/main_results.rds\")",
      "confidence": 0.75
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "02_clean_data.R",
      "lines": [
        36,
        200,
        248
      ],
      "evidence": "The code allows the analysis panel to be built without outcome data (QWI), described as \u201cfor code testing.\u201d This is fine for development, but for an academic replication package it creates ambiguity about which data are required and whether published results could be generated from an incomplete/placeholder pipeline. The repository should enforce required inputs for any results-bearing scripts (stop with an informative error if raw_qwi.rds is missing), and separate any \u2018toy\u2019 mode into a clearly labeled demo script.: if (file.exists(\"../data/raw_qwi.rds\")) {\n  qwi_raw <- readRDS(\"../data/raw_qwi.rds\")\n} else {\n  cat(\"  WARNING: QWI data not found. Will create simpler panel.\\n\")\n  qwi_raw <- NULL\n}\n...\n} else {\n  # Create basic panel without QWI (for code testing)\n  panel <- exposure_panel %>%\n    ...\n}",
      "confidence": 0.8
    }
  ],
  "file_verdicts": [
    {
      "file": "01b_fetch_qcew.R",
      "verdict": "CLEAN"
    },
    {
      "file": "01_fetch_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 2,
      "MEDIUM": 5,
      "LOW": 1
    },
    "one_liner": "unclear provenance; method mismatch",
    "executive_summary": "The code does not implement the manuscript\u2019s stated data construction workflow: `01_fetch_data.R` fails to fetch or parse the Department of Labor, NCSL, or Vaghul\u2013Zipperer minimum-wage sources described for 2010\u20132023, leaving the provenance of the \u201ccompiled\u201d wage series undocumented and unreproducible. In addition, `03_main_analysis.R` appears to run causal-effect estimation despite the paper explicitly positioning the work as descriptive and stating it does not estimate causal effects, creating a direct mismatch between the claimed methodology and the executed analysis.",
    "top_issues": [
      {
        "category": "DATA_PROVENANCE_MISSING",
        "severity": "HIGH",
        "short": "The manuscript states state minimum wages are compiled by...",
        "file": "01_fetch_data.R",
        "lines": [
          86,
          130
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0185/code/01_fetch_data.R#L86-L170"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript explicitly says",
        "file": "03_main_analysis.R",
        "lines": [
          24,
          52
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0185/code/03_main_analysis.R#L24-L175"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0185_scan.json"
  },
  "error": null
}