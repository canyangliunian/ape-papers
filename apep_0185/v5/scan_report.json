{
  "paper_id": "apep_0189",
  "scan_date": "2026-02-06T13:00:53.558680+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 8,
  "flags": [
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "paper.tex",
      "lines": [
        30,
        41
      ],
      "evidence": "Key headline estimates (2SLS coefficients/SEs/p-values/first-stage F) are hard-coded into the manuscript text rather than being automatically generated from the code outputs (e.g., via \\input of code-produced tables or a dynamic document system). This creates a high risk of transcription errors or post-hoc edits that are not traceable to computations.: Using our population-weighted measure and instrumenting with out-of-state network exposure, we find a highly significant positive effect on employment (2SLS: $\\beta = 0.827$, SE = 0.234, $p < 0.001$) with a very strong first stage ($F = 551$). In contrast, the probability-weighted specification---used in our prior work---shows no significant effect (2SLS: $\\beta = 0.27$, $p = 0.12$, $F = 290$).",
      "confidence": 0.85
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "paper.tex",
      "lines": [
        274,
        360
      ],
      "evidence": "Main result tables in the LaTeX manuscript contain literal numeric estimates (coefficients, SEs, p-values, first-stage F, N). The provided R code (06_tables.R) does generate LaTeX tables under ../tables/, but paper.tex does not appear to \\input those generated tables; instead it embeds numbers directly. This weakens computational reproducibility and auditability of reported results.: \\begin{table}[H]\n\\centering\n\\caption{Main Results: Population-Weighted Network MW Exposure}\n...\nPop-Weighted Network MW & 0.312*** & 0.638*** & 0.827*** \\\\\n & (0.095) & (0.142) & (0.234) \\\\\n & $[0.002]$ & $[<0.001]$ & $[<0.001]$ \\\\\n...\nFirst-stage F & --- & --- & 551.3 \\\\\nObservations & 134,317 & 134,317 & 134,317 \\\\\n\\bottomrule\n\\end{tabular}",
      "confidence": 0.9
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        120,
        340
      ],
      "evidence": "State minimum wage histories are manually curated/hard-coded as a tribble (not fetched programmatically from DOL/NCSL/Vaghul-Zipperer). The manuscript states these are compiled from those sources, but the repository as shown does not include a machine-verifiable ingest step or a raw-source snapshot; this increases the risk of omissions or silent edits affecting results. This is not necessarily misconduct, but it is a provenance/audit trail weakness for a key explanatory variable.: state_mw_changes <- tribble(\n  ~state_fips, ~state_abbr, ~date, ~min_wage,\n  # California\n  \"06\", \"CA\", \"2014-07-01\", 9.00,\n  ...\n  # Virginia\n  \"51\", \"VA\", \"2021-05-01\", 9.50,\n  \"51\", \"VA\", \"2022-01-01\", 11.00,\n  \"51\", \"VA\", \"2023-01-01\", 12.00\n)",
      "confidence": 0.8
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        430,
        515
      ],
      "evidence": "If the election data download fails, the script writes an empty placeholder dataset. If later scripts use these controls without explicit checks, results could silently depend on whether an external download succeeded. (In the provided analysis scripts, election data is not used, so impact may be limited.): }, error = function(e) {\n  cat(\"  WARNING: Could not download election data from Harvard Dataverse.\\n\")\n  ...\n  election_wide <- tibble(\n    county_fips = character(),\n    state_fips = character(),\n    year = integer(),\n    REP = numeric(),\n    DEM = numeric(),\n    OTHER = numeric(),\n    total_votes = numeric(),\n    rep_share = numeric(),\n    dem_share = numeric()\n  )\n  saveRDS(election_wide, \"../data/raw_presidential_county.rds\")\n}",
      "confidence": 0.65
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        250,
        290
      ],
      "evidence": "The manuscript states: \"We use total employment across all industries to maximize coverage.\" The fetch code requests multiple industries (00 plus specific sectors) and restricts ownercode to A05 (private sector). Unless the downstream cleaning explicitly selects industry==\"00\" (not shown in the provided scripts), the constructed panel may mix industries or not match the stated 'total across all industries' outcome definition. This is a methodological inconsistency that can materially change estimates.: url <- paste0(\n    qwi_base,\n    \"?get=Emp,EarnS,HirA,Sep,FrmJbC,FrmJbD\",\n    \"&for=county:*\",\n    \"&in=state:\", state_fips,\n    \"&year=\", year,\n    \"&quarter=\", quarter,\n    \"&industry=00,44-45,72,52,54\",  # All, Retail, Accommodation/Food, Finance, Professional\n    \"&ownercode=A05\",                # Private sector\n    \"&agegrp=A00\"                    # All ages\n  )",
      "confidence": 0.75
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "02_clean_data.R",
      "lines": [
        120,
        170
      ],
      "evidence": "The QWI pull in 01_fetch_data.R produces columns named Emp and EarnS (uppercase), but 02_clean_data.R computes population weights using mean(emp) (lowercase). Unless qwi_raw is renamed elsewhere (not shown), this will error or produce incorrect population proxies. Because population-weighted exposure is the paper's key innovation, a silent mismatch here would be a serious integrity risk (results might be produced from a differently structured dataset than the one the fetch script creates, or from manual post-processing outside the repo).: county_pop <- qwi_raw %>%\n    group_by(county_fips) %>%\n    summarise(pop_proxy = mean(emp, na.rm = TRUE), .groups = \"drop\")",
      "confidence": 0.85
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "02_clean_data.R",
      "lines": [
        430,
        485
      ],
      "evidence": "Outcome construction references emp and earn, but the fetched QWI variables are Emp and EarnS. If not renamed, log_emp/log_earn will be computed from non-existent columns (error) or from unrelated columns if present, causing a mismatch between the claimed QWI outcomes and what is actually analyzed. This also undermines the credibility of the reported sample size and descriptive stats in the manuscript.: panel <- panel %>%\n    mutate(\n      log_emp = if (\"log_emp\" %in% names(.)) log_emp else log(pmax(emp, 1)),\n      log_earn = if (\"log_earn\" %in% names(.)) log_earn else log(pmax(earn, 1)),\n      yearq = year + (quarter - 1) / 4\n    )",
      "confidence": 0.85
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        55,
        70
      ],
      "evidence": "A global random seed is set for the project. This is not fabrication by itself, but it affects any random procedures (e.g., permutation inference, sampling). Given the paper reports permutation inference, setting a seed is acceptable; just ensure no other parts of the pipeline use random generation to create analytic data.: # Set seed for reproducibility\nset.seed(42)",
      "confidence": 0.55
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "03_main_analysis.R",
      "lines": [
        20,
        40
      ],
      "evidence": "The regression sample is restricted to observations with non-missing values for BOTH population-weighted and probability-weighted exposures (and both IVs). This can change the estimating sample relative to the 'main' specification alone, and it may differentially drop observations based on suppression/missingness patterns. If the manuscript reports a single N for both specifications, this joint-complete-case restriction should be disclosed and justified (or alternatively run each specification on its own maximal available sample).: # Filter to complete cases for analysis\npanel_reg <- panel %>%\n  filter(!is.na(network_mw_pop) & !is.na(network_mw_pop_out_state) &\n         !is.na(network_mw_prob) & !is.na(network_mw_prob_out_state) &\n         !is.na(log_emp) & !is.na(state_fips))",
      "confidence": 0.7
    }
  ],
  "file_verdicts": [
    {
      "file": "02b_add_pop_weights.R",
      "verdict": "CLEAN"
    },
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "paper.tex",
      "verdict": "SUSPICIOUS"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 4,
      "MEDIUM": 3,
      "LOW": 2
    },
    "one_liner": "hard-coded results; method mismatch",
    "executive_summary": "Key headline estimates and the main results tables are hard-coded as literal numbers in `paper.tex` instead of being automatically populated from the analysis outputs (even though `06_tables.R` generates LaTeX tables), so the manuscript can silently diverge from the computed results. The data-cleaning script `02_clean_data.R` also appears inconsistent with the fetched QWI data: it references lowercase `emp`/`earn` for weights and outcomes while `01_fetch_data.R` produces `Emp`/`EarnS`, implying weights and log outcomes may be computed from missing or wrong columns and the reported estimates may not reflect the intended variables.",
    "top_issues": [
      {
        "category": "HARD_CODED_RESULTS",
        "severity": "HIGH",
        "short": "Key headline estimates (2SLS coefficients/SEs/p-values/fi...",
        "file": "paper.tex",
        "lines": [
          30,
          41
        ],
        "github_url": "/apep_0189/code/paper.tex#L30-L41"
      },
      {
        "category": "HARD_CODED_RESULTS",
        "severity": "HIGH",
        "short": "Main result tables in the LaTeX manuscript contain litera...",
        "file": "paper.tex",
        "lines": [
          274,
          360
        ],
        "github_url": "/apep_0189/code/paper.tex#L274-L360"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The QWI pull in 01_fetch_data.R produces columns named Em...",
        "file": "02_clean_data.R",
        "lines": [
          120,
          170
        ],
        "github_url": "/apep_0189/code/02_clean_data.R#L120-L170"
      }
    ],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0189_scan.json"
  },
  "error": null
}