{
  "paper_id": "apep_0188",
  "scan_date": "2026-02-06T13:00:46.542442+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 11,
  "flags": [
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        46
      ],
      "evidence": "A global RNG seed is set when loading packages. This is not itself fabrication, but it can unintentionally make any later random operations (e.g., permutation inference) deterministic in a way that may not be clearly communicated if results are regenerated. In this codebase, randomization is used for permutation inference/robustness (acceptable), so severity is low.: set.seed(42)",
      "confidence": 0.78
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "04_robustness.R",
      "lines": [
        78,
        92
      ],
      "evidence": "Random shuffling of exposure values is used to implement an exposure permutation inference test. This is an acceptable simulation-based robustness method and is explicitly labeled as such in comments and the manuscript.: panel_perm <- panel %>%\n    group_by(yearq) %>%\n    mutate(social_exposure_perm = sample(social_exposure)) %>%\n    ungroup()",
      "confidence": 0.9
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "HIGH",
      "file": "01_fetch_data.R",
      "lines": [
        122,
        140
      ],
      "evidence": "Key policy input (state minimum wage histories) is manually encoded directly in code via a partial-looking change list. Many states that changed minimum wages during 2012\u20132022 are not obviously included (e.g., DC is present in all_states but not in the change list; other indexing states may be missing). Downstream, missing states default to the federal floor via get_min_wage(). This can systematically mis-measure minimum wages and hence network exposure/IV values, potentially materially affecting all headline results. The manuscript claims a comprehensive compilation from DOL/NCSL/Vaghul-Zipperer; the code as shown does not demonstrate a comprehensive import or a complete state history file, and instead hardcodes a subset of states/changes.: state_mw_changes <- tribble(\n  ~state_fips, ~state_abbr, ~date, ~min_wage,\n  # California\n  \"06\", \"CA\", \"2014-07-01\", 9.00,\n  ...\n  # Virginia\n  \"51\", \"VA\", \"2021-05-01\", 9.50,\n  \"51\", \"VA\", \"2022-01-01\", 11.00,\n  \"51\", \"VA\", \"2023-01-01\", 12.00\n)",
      "confidence": 0.86
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        470,
        520
      ],
      "evidence": "If the election download fails, the script silently writes an empty placeholder dataset to disk. This is not fabrication, but it can create a non-obvious provenance failure mode where subsequent scripts find a file but it contains no data. 03c_political_outcomes.R does stop if the table is empty, which mitigates the risk for that extension, but the placeholder approach is still brittle and can confuse replication/auditing.: }, error = function(e) {\n  ...\n  election_wide <- tibble(\n    county_fips = character(),\n    state_fips = character(),\n    year = integer(),\n    REP = numeric(),\n    DEM = numeric(),\n    OTHER = numeric(),\n    total_votes = numeric(),\n    rep_share = numeric(),\n    dem_share = numeric()\n  )\n  saveRDS(election_wide, \"../data/raw_presidential_county.rds\")\n  cat(\"  Created empty election data placeholder\\n\")\n})",
      "confidence": 0.83
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "LOW",
      "file": "06_tables.R",
      "lines": [
        305,
        323
      ],
      "evidence": "This embeds a computed identity (reduced form / first stage) directly into a LaTeX footnote string. This is computed from model objects (rf_coef, fs_coef), not hard-coded, so it is acceptable; flagged only because the paper itself reports specific statistics (e.g., F=290.5) and this is one of the few places where numbers are explicitly formatted for output.: add_footnote(\n  paste0(\n    \"Notes: Dependent variable is log employment. \",\n    \"This is the direct effect of the instrument on the outcome. \",\n    \"Reduced form coefficient divided by first stage equals 2SLS: \",\n    sprintf(\"%.4f / %.4f = %.4f\", rf_coef, fs_coef, rf_coef/fs_coef), \".\"\n  ),\n  notation = \"none\"\n)",
      "confidence": 0.62
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "01b_fetch_qcew.R",
      "lines": [
        7,
        115
      ],
      "evidence": "The manuscript\u2019s main analysis is framed as using QWI true quarterly county outcomes (2012\u20132022) with county and state\u00d7time fixed effects. This separate script constructs pseudo-quarterly employment by repeating annual QCEW values across quarters, which would invalidate quarter-level dynamics and inflate the apparent N by 4\u00d7 if used without appropriate clustering. The script does document this limitation clearly in comments, so this is not deceptive, but auditors should verify whether any main tables/figures accidentally use this interpolated QCEW panel instead of QWI.: # This script downloads ANNUAL QCEW averages from BLS and creates quarterly\n# observations by replicating each annual value across 4 quarters.\n...\nqcew_quarterly <- qcew_data %>%\n  crossing(quarter = 1:4) %>%\n  mutate(\n    yearq = year + (quarter - 1) / 4,\n    log_emp = log(pmax(emp, 1)),\n    is_annual_interpolation = TRUE\n  )",
      "confidence": 0.8
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "02_clean_data.R",
      "lines": [
        468,
        476
      ],
      "evidence": "Residualizing network exposure on geographic exposure is reasonable for an 'orthogonalized exposure' robustness check, but it is done inline on the full sample without explicit documentation of how missingness is handled beyond na.exclude. This is low risk, but it can subtly change the effective sample (and residual properties) if NA patterns differ across variables.: social_exposure_resid = resid(lm(network_mw_full ~ geo_exposure, data = ., na.action = na.exclude))",
      "confidence": 0.72
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "04_robustness.R",
      "lines": [
        44,
        60
      ],
      "evidence": "The permutation test uses the 2SLS coefficient (main_results$iv_2sls) as the 'actual' benchmark but then estimates permuted models using OLS (feols without IV). That is an apples-to-oranges comparison: the permutation distribution corresponds to an OLS estimand, while the benchmark is the IV estimand. This can misstate the permutation p-value and may inadvertently make the result look more/less extreme. The manuscript describes an exposure permutation inference for OLS; if the intent is to test the IV estimate, the permutation procedure needs to re-run the IV specification each time.: # Get actual coefficient from main specification (2SLS estimate)\nactual_coef <- coef(main_results$iv_2sls)[1]\n...\n# Re-estimate with permuted exposure\nperm_fit <- tryCatch({\n  feols(\n    log_emp ~ social_exposure_perm | county_fips + state_fips^yearq,\n    data = panel_perm,\n    cluster = ~state_fips\n  )\n}, error = function(e) NULL)",
      "confidence": 0.87
    }
  ],
  "file_verdicts": [
    {
      "file": "01b_fetch_qcew.R",
      "verdict": "CLEAN"
    },
    {
      "file": "01_fetch_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02b_construct_iv.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03c_political_outcomes.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03b_iv_validation.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 1,
      "MEDIUM": 3,
      "LOW": 4
    },
    "one_liner": "unclear provenance",
    "executive_summary": "In `01_fetch_data.R`, the state minimum wage history used as a key policy input is manually hard-coded in the script as a seemingly partial change list rather than being sourced from a documented external dataset. The list appears incomplete for the 2012\u20132022 period (states with minimum-wage changes are not clearly represented), making the policy variable non-reproducible and raising a high risk of omitted or incorrect wage updates that could materially affect results.",
    "top_issues": [
      {
        "category": "DATA_PROVENANCE_MISSING",
        "severity": "HIGH",
        "short": "Key policy input (state minimum wage histories) is manual...",
        "file": "01_fetch_data.R",
        "lines": [
          122,
          140
        ],
        "github_url": "/apep_0188/code/01_fetch_data.R#L122-L140"
      }
    ],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0188_scan.json"
  },
  "error": null
}