{
  "paper_id": "apep_0190",
  "scan_date": "2026-02-06T13:01:16.751719+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "CLEAN",
  "files_scanned": 8,
  "flags": [
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        140,
        360
      ],
      "evidence": "State minimum wage histories are hard-coded as a curated list of policy change dates/values rather than being programmatically fetched from an external authoritative dataset. In isolation this can be an integrity risk (silent transcription errors, omissions), but the manuscript and header comments explicitly state the minimum wage series is manually curated and verified against primary sources (DOL/NCSL/Vaghul-Zipperer). Severity is therefore LOW, but replication would be stronger with (i) a separate, versioned input CSV + citations, and/or (ii) a script that downloads/validates against the cited sources.: state_mw_changes <- tribble(\n  ~state_fips, ~state_abbr, ~date, ~min_wage,\n  # California\n  \"06\", \"CA\", \"2014-07-01\", 9.00,\n  ...\n  # Virginia\n  \"51\", \"VA\", \"2021-05-01\", 9.50,\n  \"51\", \"VA\", \"2022-01-01\", 11.00,\n  \"51\", \"VA\", \"2023-01-01\", 12.00\n)",
      "confidence": 0.78
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        55,
        55
      ],
      "evidence": "A global RNG seed is set for the entire project. This is not fabrication by itself, but it can obscure where randomness enters the analysis. Here, randomness is used later for permutation inference (legitimate) and potentially for other operations (e.g., any sampling). Best practice is to set seeds locally near the stochastic procedure(s) (e.g., right before permutations) and avoid a global seed in a shared 'packages' script.: set.seed(42)",
      "confidence": 0.6
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "04_robustness.R",
      "lines": [
        28,
        79
      ],
      "evidence": "Randomness is used via within-time permutation of exposures for randomization inference. This is an acceptable and clearly-labeled robustness check (not data fabrication), and results are not used to generate the main estimates. Included here only because it matches fabrication-detection heuristics (sample()/seed).: set.seed(42)\nn_perms <- 500\n...\npanel_perm <- panel %>%\n  group_by(yearq) %>%\n  mutate(\n    network_mw_pop_perm = sample(network_mw_pop),\n    network_mw_prob_perm = sample(network_mw_prob)\n  ) %>%\n  ungroup()",
      "confidence": 0.9
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        240,
        333
      ],
      "evidence": "The comment says a 'subset for demonstration', but the code actually attempts to fetch all states (derived from centroids) for all years 2012\u20132022 and all 4 quarters (i.e., effectively the full panel). More importantly for provenance/integrity: failures are silently dropped (fetch_qwi returns NULL on any error), and if *all* calls fail it creates an empty tibble fallback without stopping. This can lead to non-reproducible or incomplete outcome data without a clear failure signal, depending on API availability/rate limits. While later scripts may stop if outcomes are missing, the presence of silent partial failure can change the analysis sample in hard-to-detect ways across runs/machines.: # Fetch QWI for sample of years (2012-2022) and quarters\n# Note: Full data would take long; fetching subset for demonstration\nyears_to_fetch <- 2012:2022\nquarters <- 1:4\n...\nfor (st in state_fips_list) {\n  for (yr in years_to_fetch) {\n    for (qtr in quarters) {\n      ...\n      result <- fetch_qwi(st, yr, qtr)\n      if (!is.null(result)) {\n        ...\n        qwi_list[[length(qwi_list) + 1]] <- result\n      }\n      Sys.sleep(0.1)\n    }\n  }\n}\n...\nif (length(qwi_list) > 0) {\n  qwi_raw <- bind_rows(qwi_list)\n} else {\n  cat(\"  WARNING: QWI API returned no data. Using fallback approach.\\n\")\n  qwi_raw <- tibble()\n}",
      "confidence": 0.82
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        400,
        470
      ],
      "evidence": "If the election data download fails, an empty placeholder dataset is written to disk. This is not fabrication, but it can mask a missing-data provenance problem because downstream code may read a valid-looking file that contains no observations. In the provided scripts, election data do not appear to be used in the main regressions, so the risk is limited; however, this pattern can lead to accidental omission of intended controls/heterogeneity analyses without obvious errors.: }, error = function(e) {\n  ...\n  # Fallback: Create minimal election data structure\n  election_wide <- tibble(\n    county_fips = character(),\n    state_fips = character(),\n    year = integer(),\n    REP = numeric(),\n    DEM = numeric(),\n    OTHER = numeric(),\n    total_votes = numeric(),\n    rep_share = numeric(),\n    dem_share = numeric()\n  )\n  saveRDS(election_wide, \"../data/raw_presidential_county.rds\")\n  cat(\"  Created empty election data placeholder\\n\")\n})",
      "confidence": 0.74
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "03_main_analysis.R",
      "lines": [
        34,
        44
      ],
      "evidence": "The regression sample is restricted to complete cases for both weighting schemes and their instruments, plus the outcome. This is a standard approach, but it implicitly imposes a joint sample restriction that could differ from the manuscript\u2019s reported sample construction if missingness is non-random. Given the manuscript claims near-universe coverage (~99.5% of county-quarters), this is likely fine, but it would be cleaner to report counts dropped by each condition (and confirm the final N matches the paper tables).: panel_reg <- panel %>%\n  filter(!is.na(network_mw_pop) & !is.na(network_mw_pop_out_state) &\n         !is.na(network_mw_prob) & !is.na(network_mw_prob_out_state) &\n         !is.na(log_emp) & !is.na(state_fips))",
      "confidence": 0.67
    }
  ],
  "file_verdicts": [
    {
      "file": "02b_add_pop_weights.R",
      "verdict": "CLEAN"
    },
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "CLEAN",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 0,
      "MEDIUM": 1,
      "LOW": 5
    },
    "one_liner": "Minor issues only",
    "executive_summary": "Minor code quality issues detected, but no evidence of data fabrication or manipulation.",
    "top_issues": [],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0190_scan.json"
  },
  "error": null
}