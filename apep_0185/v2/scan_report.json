{
  "paper_id": "apep_0186",
  "scan_date": "2026-02-06T13:00:08.780530+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 8,
  "flags": [
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        111,
        355
      ],
      "evidence": "State minimum wage histories are encoded as a manually curated in-script tribble rather than being programmatically retrieved from a versioned external source (DOL/NCSL/Vaghul-Zipperer). While the manuscript explicitly discloses manual curation and cites sources, this still creates a provenance/auditability weakness: (i) the full set of changes for all states is not visible here (many states appear missing), and (ii) missing entries default to the federal minimum, which can silently misclassify state-quarter wages and bias the constructed exposure measures. A stronger integrity pattern would be: store the curated panel as a CSV with explicit citations/notes per change, include all states, and include automated cross-checks against source data.: state_mw_changes <- tribble(\n  ~state_fips, ~state_abbr, ~date, ~min_wage,\n  # California\n  \"06\", \"CA\", \"2014-07-01\", 9.00,\n  ...\n  # Virginia\n  \"51\", \"VA\", \"2021-05-01\", 9.50,\n  \"51\", \"VA\", \"2022-01-01\", 11.00,\n  \"51\", \"VA\", \"2023-01-01\", 12.00\n)\n...\nget_min_wage <- function(st_fips, dt) {\n  applicable <- state_mw %>%\n    filter(state_fips == st_fips, date <= dt) %>%\n    arrange(desc(date))\n\n  if (nrow(applicable) > 0) {\n    return(applicable$min_wage[1])\n  } else {\n    return(federal_mw)\n  }\n}",
      "confidence": 0.74
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "01b_fetch_qcew.R",
      "lines": [
        27,
        33
      ],
      "evidence": "The manuscript describes a 2010\u20132023 county-by-quarter panel (56 quarters). This script only downloads QCEW data for 2015\u20132022. If the released analysis panel (analysis_panel.rds) is built from this script (or if results use this outcome data), the outcome sample period would not match the paper's stated panel horizon. This can materially affect descriptive time series and any illustrative regressions/event-study claims tied to 2010\u20132023.: # Sample years to download (keeping manageable)\nyears <- 2015:2022",
      "confidence": 0.84
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "01_fetch_data.R",
      "lines": [
        262,
        281
      ],
      "evidence": "The manuscript frames the constructed exposure panel as covering 2010\u20132023. This script fetches QWI outcomes only for 2012\u20132022 and explicitly labels it a subset 'for demonstration'. If the main analysis panel merges exposures with QWI outcomes (as 02_clean_data.R does when raw_qwi.rds exists), then any employment regressions/event study are necessarily limited to 2012\u20132022 (and potentially to those counties/industries successfully returned by the API), not the full stated window. This mismatch should be reconciled explicitly in the paper and code outputs (e.g., separate 'exposure-only' 2010\u20132023 panel vs. 'outcome' panel 2012\u20132022).: # Fetch QWI for sample of years (2012-2022) and quarters\n# Note: Full data would take long; fetching subset for demonstration\nyears_to_fetch <- 2012:2022",
      "confidence": 0.86
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "MEDIUM",
      "file": "05_figures.R",
      "lines": [
        20,
        28
      ],
      "evidence": "The code drops county-quarter observations with network exposure < $7.00. The manuscript discloses this exact rule and motivates it as filtering anomalous constructed values, which reduces suspicion. However, this is still a researcher-chosen threshold that changes the analysis sample and could affect summary statistics and any illustrative regressions; the repository should ensure that all downstream tables/regressions use the same filtered sample (or report both). Also, the paper text mentions ~8% dropped; this should be mechanically verified in code outputs (e.g., write out counts by state/rurality to show it is not selectively removing unfavorable units).: valid_exposure_threshold <- log(7.00)  # Remove clear outliers below $7.00\nn_before <- nrow(panel)\npanel <- panel %>%\n  filter(social_exposure >= valid_exposure_threshold)",
      "confidence": 0.72
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "02_clean_data.R",
      "lines": [
        205,
        223
      ],
      "evidence": "Network communities are computed on a truncated graph (top 50 links per county) for speed, rather than on the full SCI network. The manuscript describes Louvain clustering on the full SCI graph; truncation can change modularity structure and the number/composition of communities. This affects any results using 'network communities' (including alternative clustering). If full-graph clustering is infeasible, the paper should state the truncation rule and report robustness to different cutoffs (e.g., top 25/top 100).: top_sci <- sci_normalized %>%\n  group_by(county_fips_1) %>%\n  slice_max(order_by = sci, n = 50) %>%  # Top 50 connections per county\n  ungroup()\n...\nlouvain <- cluster_louvain(g, weights = E(g)$weight)",
      "confidence": 0.68
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        55,
        55
      ],
      "evidence": "A global seed is set on package load. There is no direct simulated data generation used as primary inputs; the seed mainly affects permutation inference and any random sampling/truncation steps (e.g., potential future sampling). This is not evidence of fabrication, but auditors typically prefer seeds to be set locally where randomness is used (e.g., in 04_robustness.R) to avoid unintended dependence of results on script execution order.: set.seed(42)",
      "confidence": 0.6
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "04_robustness.R",
      "lines": [
        38,
        94
      ],
      "evidence": "The manuscript describes permuting exposure across counties within time; the implemented permutation does exactly that. However, there is also code scaffolding suggesting a different permutation concept ('shuffle which states raised MW when') that is not actually executed and the function is not used. This raises a minor transparency risk: a reader might believe timing-permutation inference was performed when the actual test is a within-period exposure permutation. Recommendation: remove the unused timing-permutation scaffolding or clearly separate/label it as not implemented, and ensure the manuscript description matches the exact implemented procedure.: # Function to recompute exposure with permuted timing\ncompute_permuted_exposure <- function(panel, state_mw_panel, seed) {\n  set.seed(seed)\n  ...\n  # This is a simplified version - in practice would need to\n  # recompute full exposure. Here we approximate by permuting\n  # the exposure values across counties within each time period\n  panel_perm <- panel %>%\n    group_by(yearq) %>%\n    mutate(\n      social_exposure_perm = sample(social_exposure)\n    ) %>%\n    ungroup()\n  return(panel_perm)\n}\n...\n# Run permutation inference\n...\npanel_perm <- panel %>%\n  group_by(yearq) %>%\n  mutate(social_exposure_perm = sample(social_exposure)) %>%\n  ungroup()",
      "confidence": 0.63
    }
  ],
  "file_verdicts": [
    {
      "file": "01b_fetch_qcew.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "01_fetch_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 2,
      "MEDIUM": 3,
      "LOW": 2
    },
    "one_liner": "method mismatch",
    "executive_summary": "The code does not match the manuscript\u2019s stated data coverage: although the paper claims a 2010\u20132023 county-by-quarter panel (56 quarters), `01b_fetch_qcew.R` only downloads QCEW data for 2015\u20132022. Similarly, `01_fetch_data.R` retrieves QWI outcomes only for 2012\u20132022 and explicitly labels the pull as a \u201csubset for demonstration,\u201d which is inconsistent with building the full 2010\u20132023 exposure/outcome panel described in the paper.",
    "top_issues": [
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript describes a 2010\u20132023 county-by-quarter pa...",
        "file": "01b_fetch_qcew.R",
        "lines": [
          27,
          33
        ],
        "github_url": "/apep_0186/code/01b_fetch_qcew.R#L27-L33"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript frames the constructed exposure panel as c...",
        "file": "01_fetch_data.R",
        "lines": [
          262,
          281
        ],
        "github_url": "/apep_0186/code/01_fetch_data.R#L262-L281"
      }
    ],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0186_scan.json"
  },
  "error": null
}