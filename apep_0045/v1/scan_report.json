{
  "paper_id": "apep_0045",
  "scan_date": "2026-02-06T12:35:21.030972+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 7,
  "flags": [
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "05_figures.R",
      "lines": [
        175,
        185
      ],
      "evidence": "Figure 4 ('Heterogeneous Effects') uses hard-coded placeholder ATT/SE values rather than estimates computed from the data/models. If this figure is included in the paper, it is not evidence-based and can mislead readers about subgroup effects.: hetero_data <- tibble(\n  subgroup = c(\"Young (25-40)\", \"Older (41-64)\", \"No BA\", \"BA+\"),\n  category = c(\"Age\", \"Age\", \"Education\", \"Education\"),\n  att = c(0.02, 0.01, 0.025, 0.008),  # Placeholder - replace with actual estimates\n  se = c(0.008, 0.006, 0.007, 0.005)  # Placeholder\n)",
      "confidence": 0.95
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "04_robustness.R",
      "lines": [
        72,
        74
      ],
      "evidence": "The script labels an output as a 'Gardner two-stage' robustness check, but assigns it to the simple TWFE aggregated regression (twfe_agg) instead of running did2s (Gardner 2-stage) properly. Downstream Table 4 (06_tables.R) reports a 'Gardner Two-Stage' row using this object, which is methodologically incorrect/mislabeled.: # Placeholder for gardner output\ngardner_out <- twfe_agg",
      "confidence": 0.9
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "paper.tex",
      "lines": [
        210,
        272
      ],
      "evidence": "Key regression results (coefficients/SEs/N/R-squared) are typed directly into the manuscript table rather than being dynamically generated/\\input from the code outputs. This creates a high risk of transcription errors or selective updating (paper not matching the current code/data). The codebase does generate LaTeX tables (e.g., figures/tab2_main_results.tex), but paper.tex does not appear to \\input them here.: \\begin{table}[H]\n... \nTreated $\\times$ Post & 0.0108 & 0.0106 \\\\\n& (0.0075) & (0.0070) \\\\\n...\nObservations & 632,730 & 632,730 \\\\\nR-squared & 0.024 & 0.052 \\\\\n\\bottomrule\n\\end{tabular}\n...",
      "confidence": 0.85
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "paper.tex",
      "lines": [
        303,
        336
      ],
      "evidence": "Cohort ATT/SE values are hard-coded in the manuscript. While the code computes cohort-specific effects via aggte(type='group'), there is no visible linkage ensuring these exact numbers come from the saved results objects. This again creates auditability and reproducibility risks.: \\begin{table}[H]\n...\n2018 & Oregon & 0.0213 & 0.0064 \\\\\n2019 & Illinois & 0.0158 & 0.0054 \\\\\n2020 & California & -0.0166 & 0.0065 \\\\\n2022 & CT, MD & -0.0203 & 0.0053 \\\\\n2023 & CO, VA & -0.0110 & 0.0065 \\\\\n2024 & ME, NJ & 0.0571 & 0.0120 \\\\\n\\midrule\nOverall & & 0.0051 & 0.0081 \\\\\n\\bottomrule\n\\end{tabular}",
      "confidence": 0.8
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "paper.tex",
      "lines": [
        353,
        381
      ],
      "evidence": "Robustness-check results are hard-coded into the manuscript. The code can generate a robustness table (figures/tab4_robustness.tex) from model objects, but the paper does not appear to consume that output here. Additionally, the code's 'Gardner Two-Stage' row is mis-implemented (see separate finding), so any corresponding manuscript row would be especially unreliable.: \\begin{table}[H]\n...\nMain (Callaway-Sant'Anna) & 0.0051 & 0.0081 & [-0.0107, 0.0209] \\\\\nTWFE (simple) & 0.0108 & 0.0075 & [-0.0039, 0.0255] \\\\\nTWFE (with controls) & 0.0106 & 0.0070 & [-0.0031, 0.0243] \\\\\nTWFE on state-year aggregates & -0.0033 & 0.0096 & [-0.0221, 0.0155] \\\\\nExcluding Oregon & -0.0001 & 0.0083 & [-0.0164, 0.0162] \\\\\nPlacebo: Workers WITH pension & -0.0126 & 0.0140 & [-0.0400, 0.0148] \\\\\n\\bottomrule\n\\end{tabular}",
      "confidence": 0.85
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        86,
        118
      ],
      "evidence": "Policy adoption dates are manually entered ('hand-collected') rather than programmatically sourced, which is common/acceptable, but provenance would be stronger with a machine-readable source file (CSV) plus citations/URLs per state and a brief verification log. The manuscript does cite Georgetown CRI, reducing severity.: # Hand-collected from Georgetown CRI and state program websites\nauto_ira_dates <- tribble(\n  ~statefip, ~state_name, ~program_name, ~launch_date, ~launch_year, ~launch_month,\n  41, \"Oregon\", \"OregonSaves\", \"2017-10-01\", 2017, 10,\n  ...\n)",
      "confidence": 0.75
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "paper.tex",
      "verdict": "SUSPICIOUS"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 4,
      "MEDIUM": 1,
      "LOW": 1
    },
    "one_liner": "hard-coded results",
    "executive_summary": "The workflow embeds key empirical results directly in the manuscript and figures instead of generating them from the underlying model outputs. Figure 4 in `05_figures.R` uses hard-coded placeholder ATT/SE values rather than estimates computed from the data, and `paper.tex` types in regression coefficients/SEs/N/R\u00b2, cohort ATT/SEs, and robustness-check numbers even though the code can produce these objects/tables. This breaks the reproducibility link between code and paper and creates a high risk that reported results are outdated, inconsistent with the actual analyses, or not evidence-based.",
    "top_issues": [
      {
        "category": "HARD_CODED_RESULTS",
        "severity": "HIGH",
        "short": "Figure 4 ('Heterogeneous Effects') uses hard-coded placeh...",
        "file": "05_figures.R",
        "lines": [
          175,
          185
        ],
        "github_url": "/apep_0045/code/05_figures.R#L175-L185"
      },
      {
        "category": "HARD_CODED_RESULTS",
        "severity": "HIGH",
        "short": "Key regression results (coefficients/SEs/N/R-squared) are...",
        "file": "paper.tex",
        "lines": [
          210,
          272
        ],
        "github_url": "/apep_0045/code/paper.tex#L210-L272"
      },
      {
        "category": "HARD_CODED_RESULTS",
        "severity": "HIGH",
        "short": "Cohort ATT/SE values are hard-coded in the manuscript. Wh...",
        "file": "paper.tex",
        "lines": [
          303,
          336
        ],
        "github_url": "/apep_0045/code/paper.tex#L303-L336"
      }
    ],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0045_scan.json"
  },
  "error": null
}