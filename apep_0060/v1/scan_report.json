{
  "paper_id": "apep_0060",
  "scan_date": "2026-02-06T12:38:19.377579+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 7,
  "flags": [
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        21
      ],
      "evidence": "A global random seed is set even though the provided analysis code does not appear to use random-number generation. This is not evidence of fabrication, but it is a reproducibility marker that can sometimes accompany simulation workflows; auditors typically verify no downstream simulated-data paths exist.: set.seed(42)",
      "confidence": 0.7
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "paper.tex",
      "lines": [
        214,
        217,
        223,
        226,
        233,
        268,
        271,
        277,
        280,
        287,
        322,
        325,
        331,
        334,
        341,
        360,
        363,
        366,
        369
      ],
      "evidence": "Key regression results (coefficients/SEs/R^2/observations) are manually typed into LaTeX tables rather than being programmatically generated from model objects. The code exports CSVs (e.g., demo_comp, city_year) and saves model objects to analysis_results.rds, but there is no script that converts those model objects into LaTeX tables or that guarantees the LaTeX numbers match the computed results. This creates an integrity risk because the manuscript\u2019s reported estimates could diverge from the code outputs without detection.: \\begin{table}[H]\n\\centering\n\\caption{Descriptive DiD Contrast: Male Share}\n\\label{tab:gender}\n\\begin{tabular}{lc}\n\\toprule\n& Male Indicator \\\\\n\\midrule\nDiD (SF $\\times$ Post) & 0.074 \\\\\n& (0.002) \\\\\nPost (1910) & -0.032 \\\\\n& (0.001) \\\\\nTreated (SF) & -0.030 \\\\\n& (0.002) \\\\\n\\midrule\nAge controls & Yes \\\\\nObservations & 1,281,674 \\\\\n$R^2$ & 0.003 \\\\\n\\bottomrule\n\\end{tabular}\n...",
      "confidence": 0.85
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "paper.tex",
      "lines": [
        258,
        286,
        313,
        343
      ],
      "evidence": "Additional main results tables (occupational score, foreign-born, heterogeneity) are also hard-coded in the LaTeX source. Without an automated table-generation step, it is not verifiable from the repository that these numbers are the ones produced by 04_main_analysis.R (m1\u2013m4 and subgroup LMs).: DiD (SF $\\times$ Post) & -1.658 \\\\\n& (0.053) \\\\\nPost (1910) & 1.016 \\\\\n& (0.039) \\\\\nTreated (SF) & 2.065 \\\\\n& (0.045) \\\\\n...\nObservations & 777,473 \\\\\n$R^2$ & 0.117",
      "confidence": 0.85
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "03_process_data.R",
      "lines": [
        112,
        118,
        121
      ],
      "evidence": "The processing script assumes a specific local filename (usa_00142.csv.gz). While the manuscript\u2019s appendix claims 'Extract 142', the IPUMS submission scripts (01_fetch_data.py) generate extract IDs dynamically and save them in extract_ids.json; the resulting download filename will be usa_{extract_id}.csv.gz and may not equal 00142. Unless the user manually renames the download or the extract ID actually is 142, the pipeline will fail or (worse) might process a different extract than the one described in the paper if a file with that name exists from another project/version. This is a provenance/reproducibility weakness (not fabrication evidence).: raw_file <- file.path(DATA_DIR, \"usa_00142.csv.gz\")\n\nif (!file.exists(raw_file)) {\n  msg(\"No IPUMS data file found: \", raw_file)\n  stop(\"Data not yet available\")\n}",
      "confidence": 0.8
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "LOW",
      "file": "paper.tex",
      "lines": [
        155,
        172,
        204,
        213
      ],
      "evidence": "The manuscript frames results as 'descriptive' due to only three geographic units, but still reports heteroskedasticity-robust SEs and R^2 as if from individual-level regressions. The code indeed runs individual-level regressions with HC1 SEs (feols(..., vcov = \"HC1\")). This is not a direct mismatch (the paper explicitly cautions against formal inference), but readers could still interpret SEs as valid DiD inference; the methodology would be closer to the manuscript if tables omitted SEs or clearly labeled them as within-sample precision only.: Given that we have only three geographic units, we present these as descriptive contrasts rather than formal statistical tests.\n...\nStandard errors in parentheses are heteroskedasticity-robust but should be interpreted cautiously given only 3 geographic units.",
      "confidence": 0.65
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "04_main_analysis.R",
      "lines": [
        58,
        95,
        161
      ],
      "evidence": "Occupational score analyses drop observations with OCCSCORE <= 0 (and require 'has_occupation'). This is plausibly standard cleaning (invalid/missing codes) and is symmetric in the sense it removes nonpositive/invalid values rather than trimming tails. However, because OCCSCORE can be missing systematically (e.g., non-labor-force, women, certain ages), this restriction changes composition and could affect comparisons if missingness differs by city-year. The manuscript notes occupational score is calculated for those with valid occupation codes, which partially mitigates concern.: mean_occscore = mean(OCCSCORE[OCCSCORE > 0], na.rm = TRUE)\n...\ndt_occ <- dt_work[has_occupation == 1 & OCCSCORE > 0]",
      "confidence": 0.6
    }
  ],
  "file_verdicts": [
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_process_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_check_status.py",
      "verdict": "CLEAN"
    },
    {
      "file": "01_fetch_data.py",
      "verdict": "CLEAN"
    },
    {
      "file": "02_wait_for_extracts.py",
      "verdict": "CLEAN"
    },
    {
      "file": "paper.tex",
      "verdict": "SUSPICIOUS"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 2,
      "MEDIUM": 1,
      "LOW": 3
    },
    "one_liner": "hard-coded results",
    "executive_summary": "The LaTeX source (`paper.tex`) manually types key regression outputs (coefficients, standard errors, R\u00b2, and observation counts) into tables instead of generating them directly from saved model objects, breaking the reproducible link between code and reported results. Several additional headline tables (e.g., occupational score, foreign-born, and heterogeneity results) are also hard-coded in LaTeX, while the scripts only export intermediate CSVs (e.g., `demo_comp`) without an automated step that converts those outputs into the exact tables shown in the paper. As a result, the reported numbers cannot be verified from the repository\u2019s computational pipeline.",
    "top_issues": [
      {
        "category": "HARD_CODED_RESULTS",
        "severity": "HIGH",
        "short": "Key regression results (coefficients/SEs/R^2/observations...",
        "file": "paper.tex",
        "lines": [
          214,
          217
        ],
        "github_url": "/apep_0060/code/paper.tex#L214-L369"
      },
      {
        "category": "HARD_CODED_RESULTS",
        "severity": "HIGH",
        "short": "Additional main results tables (occupational score, forei...",
        "file": "paper.tex",
        "lines": [
          258,
          286
        ],
        "github_url": "/apep_0060/code/paper.tex#L258-L343"
      }
    ],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0060_scan.json"
  },
  "error": null
}