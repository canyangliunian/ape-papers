{
  "paper_id": "apep_0059",
  "scan_date": "2026-02-06T12:38:19.364903+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 13,
  "flags": [
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "paper.tex",
      "lines": [
        150,
        240,
        305
      ],
      "evidence": "The manuscript describes a single-year (2022) ACS analysis using OLS with robust SEs, but the main analysis pipeline in the provided R scripts primarily implements AIPW (doubly robust) estimators and the data construction code fetches/uses 2018\u20132022 pooled data by default. This is a substantive mismatch between claimed and implemented methods and sample frame.: I use the American Community Survey (ACS) Public Use Microdata Sample for 2022...\n\nI estimate the association ... using ordinary least squares regression with robust standard errors:\n\nY_i = \\alpha + \\beta \\cdot SelfEmployed_i + X_i'\\gamma + \\delta_s + \\epsilon_i",
      "confidence": 0.85
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "01_fetch_data.R",
      "lines": [
        90,
        96,
        104
      ],
      "evidence": "This script fetches and stacks ACS PUMS for 2018\u20132022. The manuscript states the analysis uses 2022 only (\"Using data on ... from the 2022 ACS\" and \"I use ... PUMS for 2022\"). Unless there is an explicit later restriction to YEAR==2022 (not present in 02_clean_data.R), the implemented sample does not match the paper.: years <- 2018:2022\n...\npums_raw <- bind_rows(all_data)",
      "confidence": 0.9
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "02_clean_data.R",
      "lines": [
        12,
        19,
        210
      ],
      "evidence": "The cleaning pipeline loads the pooled 2018\u20132022 dataset and applies age/employment/class-of-worker restrictions but does not filter to YEAR==2022. If downstream analysis uses pums_clean.rds as created here, it contradicts the manuscript\u2019s single-year 2022 design and reported N for 2022.: pums_raw <- readRDS(file.path(data_dir, \"pums_raw_2018_2022.rds\"))\n...\npums <- pums_raw %>% ... filter(AGEP >= 25 & AGEP <= 64, ESR %in% c(\"1\", \"2\"), ...)\n...\npums_clean <- pums %>% ...",
      "confidence": 0.9
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "03_main_analysis.R",
      "lines": [
        1,
        86,
        110
      ],
      "evidence": "The main estimation script uses AIPW/SuperLearner to estimate ATT, while the manuscript frames the empirical strategy as OLS with robust (HC2) standard errors. AIPW can be a legitimate upgrade, but it is not described in the manuscript, and the reported table notes \"OLS\" rather than ATT/AIPW.: # Primary AIPW estimation ...\n...\naipw_fit <- AIPW$new(\n    Y = Y,\n    A = df_model$self_employed,\n    W = W,\n    Q.SL.library = sl_lib,\n    g.SL.library = sl_lib,\n    k_split = 5,\n    verbose = FALSE\n  )",
      "confidence": 0.8
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "paper.tex",
      "lines": [
        210,
        225,
        250
      ],
      "evidence": "Key regression results (coefficients/SEs/CIs/baselines) appear hard-coded directly into the LaTeX table rather than being auto-generated from the analysis outputs. The provided codebase does generate CSV/TeX tables (06_tables.R), but paper.tex itself contains literal numbers. This creates an integrity risk if the manuscript table is not programmatically linked to the computed outputs.: \\begin{table}[H]\n...\nAny insurance & -0.061*** & 0.001 & [-0.063, -0.059] & 92.0\\% \\\\\nEmployer-sponsored & -0.272*** & 0.001 & [-0.274, -0.269] & 76.3\\% \\\\\nDirect purchase & 0.183*** & 0.001 & [0.180, 0.185] & 9.2\\% \\\\\nMedicaid & 0.032*** & 0.001 & [0.030, 0.033] & 9.1\\% \\\\\n...",
      "confidence": 0.9
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "07_complete_outputs.R",
      "lines": [
        14,
        18,
        26
      ],
      "evidence": "Multiple scripts rely on author-specific absolute paths under /Users/... for inputs/outputs. This undermines replicability and makes it hard to verify that the data used matches the claimed Census API pulls in the repository. For an audit/replication package, paths should be relative and data should be produced by documented fetch scripts.: output_dir <- \"/Users/dyanag/auto-policy-evals/output/paper_1\"\n...\nfetch_pums <- function(year, state = \"*\") { ... }\n...\ndf <- read_csv(\"/Users/dyanag/auto-policy-evals/output/paper_1/data/pums_sample.csv\",",
      "confidence": 0.85
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "08_heterogeneity.R",
      "lines": [
        9,
        10
      ],
      "evidence": "This heterogeneity script reads an external CSV from an undocumented local absolute path rather than using the repository\u2019s data_dir outputs (e.g., data/pums_clean.rds). That breaks provenance tracking and creates ambiguity about which dataset version produced the reported subgroup estimates.: df <- read_csv(\"/Users/dyanag/auto-policy-evals/output/paper_1/data/pums_sample.csv\",\n               show_col_types = FALSE)",
      "confidence": 0.9
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "HIGH",
      "file": "07_complete_outputs.R",
      "lines": [
        78,
        83,
        115,
        120
      ],
      "evidence": "Two potentially bias-inducing redefinitions occur in this end-to-end script: (1) Medicaid expansion status is assigned via a hard-coded state list that appears incorrect (it includes many historically non-expansion states and uses a different definition than 01_fetch_data.R\u2019s KFF-style table), which can materially alter the key heterogeneity result (expansion vs non-expansion). (2) \"Income quintiles\" are not actual quintiles; they are fixed-dollar bins. This changes subgroup composition and can distort the inverted-U pattern emphasized in the manuscript.: expansion_states <- c(\"02\", \"04\", \"05\", \"06\", \"08\", \"09\", \"10\", \"11\", \"12\", ... , \"55\", \"56\")\n...\nincome_quintile = case_when(\n      income < 25000 ~ \"Q1\",\n      income < 45000 ~ \"Q2\",\n      income < 70000 ~ \"Q3\",\n      income < 110000 ~ \"Q4\",\n      TRUE ~ \"Q5\"\n    ),",
      "confidence": 0.9
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "MEDIUM",
      "file": "04_robustness.R",
      "lines": [
        135,
        136,
        137
      ],
      "evidence": "Propensity-score trimming (2%\u201398%) is a reasonable robustness check, but it can change the estimand/sample. The manuscript does not describe trimming or overlap diagnostics; if trimmed results influence reported point estimates (rather than being presented as robustness), this could be problematic. As written it is labeled as a robustness step in code, so concern is moderate and depends on what was reported.: trim_low <- 0.02\ntrim_high <- 0.98\nin_overlap <- ps >= trim_low & ps <= trim_high",
      "confidence": 0.65
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "03_analysis_fast.R",
      "lines": [
        14,
        18,
        19
      ],
      "evidence": "Several analysis scripts use random subsamples (300k, 500k, 10% samples) for speed. If manuscript results are presented as full-sample estimates (N\u22481.3M) but were actually taken from a subsample run, that would be selective reporting. The paper reports N=1,296,497; the presence of multiple subsampling scripts increases the risk of accidentally reporting a favorable or convenient run unless outputs are clearly tied to the full-sample pipeline.: set.seed(42)\ndf_sample <- df %>% slice_sample(n = 500000)\ncat(\"Using sample of\", nrow(df_sample), \"observations\\n\")",
      "confidence": 0.7
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "03_analysis.R",
      "lines": [
        14,
        18,
        19
      ],
      "evidence": "This AIPW script explicitly estimates effects on a 300k random subsample. Without strict output management, there is a risk that these numbers could be copied into manuscript tables that claim full-sample inference. This is not fabrication (it uses real data), but it is a reporting risk.: set.seed(42)\ndf_sample <- df %>% slice_sample(n = 300000)\ncat(\"Using sample of\", nrow(df_sample), \"observations for analysis\\n\")",
      "confidence": 0.75
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        26
      ],
      "evidence": "A global RNG seed is set. No simulated data generation is found, but seeds are also used alongside random subsampling and cross-fitting. This is generally good practice; flagged only because multiple subsampling scripts exist and could affect what gets reported.: set.seed(20260125)",
      "confidence": 0.55
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "08_heterogeneity.R",
      "verdict": "CLEAN"
    },
    {
      "file": "07_complete_outputs.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "03_analysis_fast.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.py",
      "verdict": "CLEAN"
    },
    {
      "file": "01_fetch_data.py",
      "verdict": "CLEAN"
    },
    {
      "file": "paper.tex",
      "verdict": "SUSPICIOUS"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 5,
      "MEDIUM": 6,
      "LOW": 1
    },
    "one_liner": "method mismatch",
    "executive_summary": "The code and manuscript describe materially different analyses: the paper claims a single-year (2022) ACS PUMS OLS setup with robust SEs, but the R pipeline primarily pools 2018\u20132022 ACS data (without filtering back to 2022 in cleaning) and implements AIPW/doubly robust estimators instead. Key regression numbers in the LaTeX tables are hard-coded rather than generated from the scripts, breaking reproducibility and making it unclear whether the reported results come from the provided code. The end-to-end output script also applies suspect data transformations, including a hard-coded Medicaid expansion state list that appears historically incorrect and could bias treatment assignment.",
    "top_issues": [
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript describes a single-year (2022) ACS analysi...",
        "file": "paper.tex",
        "lines": [
          150,
          240
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0059/code/paper.tex#L150-L305"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "This script fetches and stacks ACS PUMS for 2018\u20132022. Th...",
        "file": "01_fetch_data.R",
        "lines": [
          90,
          96
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0059/code/01_fetch_data.R#L90-L104"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The cleaning pipeline loads the pooled 2018\u20132022 dataset ...",
        "file": "02_clean_data.R",
        "lines": [
          12,
          19
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0059/code/02_clean_data.R#L12-L210"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0059_scan.json"
  },
  "error": null
}