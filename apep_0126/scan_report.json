{
  "paper_id": "apep_0126",
  "scan_date": "2026-02-06T12:48:11.898877+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 8,
  "flags": [
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "01_fetch_data.R",
      "lines": [
        98,
        134
      ],
      "evidence": "The manuscript defines treatment timing G_i as the first CPS ASEC survey year with any post-launch exposure (e.g., Oregon coded as G_i=2018 because March 2018 references 2017, which includes July\u2013Dec 2017 exposure). The code hard-codes first_treat_year to the calendar launch year (e.g., Oregon=2017) rather than the CPS exposure year used in the paper tables and narrative. This changes the post indicator and event-time alignment, and can materially affect ATT(g,t), pre-trend windows, and event-study coefficients.: first_treat_year = c(\n    2017,  # Oregon - July 2017 (100+ employers)\n    2018,  # Illinois - November 2018 (500+ initially, using 2018)\n    2019,  # California - July 2019 (100+ employers)\n    2021,  # Connecticut - March 2021 (100+ employers)\n    2022,  # Maryland - September 2022 (100+ employers)\n    ...\n  )",
      "confidence": 0.9
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "02_clean_data.R",
      "lines": [
        74,
        99
      ],
      "evidence": "Post-treatment is coded as year >= first_treat where first_treat is the (calendar) launch year from 01_fetch_data.R. In the manuscript, treatment is explicitly shifted to the first CPS survey year with exposure (e.g., Oregon launch July 2017 => G_i=2018). As written, the code treats 2017 as post for Oregon, but the CPS ASEC 2017 file references 2016 outcomes (pre-policy), so this coding likely misclassifies at least the first treated year for early adopters. This is a direct mismatch to the paper\u2019s stated treatment coding.: first_treat = case_when(\n      statefip %in% treat_lookup$statefip ~\n        treat_lookup$first_treat_year[match(statefip, treat_lookup$statefip)],\n      TRUE ~ 0  # Never treated\n    ),\n\n# Post-treatment indicator\npost = ifelse(first_treat > 0 & year >= first_treat, 1, 0)",
      "confidence": 0.92
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "03_main_analysis.R",
      "lines": [
        171,
        186
      ],
      "evidence": "The Sun-Abraham specification includes year fixed effects both inside the sunab() construction (which uses 'year' as the time index) and again as an explicit FE ('| state_id + year'). In fixest, Sun-Abraham typically uses unit and time FEs, but care is needed that the time FE is not redundantly included in a way that changes interpretation or causes collinearity/normalization differences. This may still run, but it risks implementing something different than intended or at minimum makes the exact estimand/normalization non-transparent relative to the manuscript\u2019s description.: sunab_out <- feols(\n  pension_rate ~ sunab(cohort, year) | state_id + year,\n  data = df_state_year,\n  cluster = ~statefip\n)",
      "confidence": 0.6
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "04b_randomization_inference.R",
      "lines": [
        13,
        31
      ],
      "evidence": "The manuscript\u2019s randomization inference is described for the estimation sample of 45 clusters with 5 treated states (excluding 6 contaminated states) and preserving the cohort structure (2018, 2019, 2020, 2022, 2023\u2014one per cohort). This script states 'only 11 treated states' and then operationally uses however many treated states are in df_state_year (which, given 01_fetch_data.R, is 11 including late adopters), not the paper\u2019s 5-state treated set. That is a substantive mismatch to the manuscript\u2019s described RI design and p-value.: # With only 11 treated states, standard asymptotic inference may be unreliable.\n# Randomization inference provides exact p-values by comparing the observed\n# treatment effect to the distribution of effects under random treatment\n# assignment.",
      "confidence": 0.85
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "04b_randomization_inference.R",
      "lines": [
        86,
        103
      ],
      "evidence": "The manuscript says RI assigns the observed treatment years (2018, 2019, 2020, 2022, 2023\u2014one per cohort) to randomly selected treated states, maintaining the same cohort structure. This code samples treatment years with replacement, which changes the null distribution (allowing repeated cohort years and potentially changing the number of cohorts represented per permutation). This is not the RI procedure described in the paper and can alter the reported p-value and randomization interval.: perm_treated_states <- sample(all_states, n_treated)\n\n# Randomly assign treatment years (from actual distribution)\nperm_treat_years <- sample(treatment_years, n_treated, replace = TRUE)",
      "confidence": 0.9
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "04b_randomization_inference.R",
      "lines": [
        62,
        75
      ],
      "evidence": "The manuscript frames RI around re-estimating the ATT using the Callaway\u2013Sant\u2019Anna estimator under permuted assignment. This script uses TWFE ('for speed') as the estimand being permuted. That is a different statistic than the paper\u2019s main ATT and will generally have a different sampling/permutation distribution, especially under staggered adoption with heterogeneous effects. If the manuscript\u2019s reported RI p-value (0.47) is claimed for the C-S ATT, but the code computes it for TWFE, that is a direct methodology mismatch.: # Actual ATT Estimate\n\n# Simple TWFE for speed\n...\nactual_model <- feols(\n  pension_rate ~ post | state_id + year,\n  data = df_state_year,\n  cluster = ~statefip\n)",
      "confidence": 0.8
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "00_packages.R",
      "lines": [
        84,
        96
      ],
      "evidence": "The workflow assumes a local project structure and relies on IPUMS extract submission/download in 01_fetch_data.R, but there is no code in the repository showing IPUMS API key setup (e.g., ipumsr::set_ipums_api_key()) or a pinned extract number/DOI. Reproducibility depends on external authentication and the dynamic extract generation, which can lead to provenance ambiguity if the extract definition changes or if different users download different versions.: project_dir <- here::here()\ndata_dir <- file.path(project_dir, \"data\")\nfig_dir <- file.path(project_dir, \"figures\")\ncode_dir <- file.path(project_dir, \"code\")",
      "confidence": 0.65
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        56,
        152
      ],
      "evidence": "Treatment timing and statute metadata are hard-coded rather than being read from a source file or scraped. In context this is often acceptable (policy timing is not an estimated result), and the manuscript appendix lists official sources. However, because treatment timing is central to identification, hard-coding increases risk of transcription/definition errors (and appears to contribute to the treatment-year coding mismatch with the manuscript).: treatment_data <- tibble(\n  statefip = c(41, 17, 6, 9, 24, 8, 51, 23, 10, 34, 50),\n  state_abbr = c(\"OR\", \"IL\", \"CA\", \"CT\", \"MD\", \"CO\", \"VA\", \"ME\", \"DE\", \"NJ\", \"VT\"),\n  ...\n  first_treat_year = c(2017, 2018, 2019, 2021, 2022, 2023, 2023, 2024, 2024, 2024, 2024),\n  enabling_statute = c(...)\n)",
      "confidence": 0.75
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "03b_ddd_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04b_randomization_inference.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 4,
      "MEDIUM": 3,
      "LOW": 1
    },
    "one_liner": "method mismatch",
    "executive_summary": "The code applies a different treatment-timing definition than the manuscript: it sets each state\u2019s first treatment year to the calendar launch year (e.g., Oregon = 2018) and then codes post-treatment as `year >= first_treat`, even though the paper defines \\(G_i\\) as the first CPS ASEC *survey* year with any post-launch exposure (accounting for ASEC\u2019s reference-year timing). The randomization inference implementation also diverges from the described design: it does not replicate the paper\u2019s estimation sample restrictions (45 clusters, 5 treated, excluding contaminated states) and does not preserve the stated cohort structure where the observed treatment years (2018, 2019, 2020, 2022, 2023\u2014one per cohort) are reassigned to randomly selected treated states.",
    "top_issues": [
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript defines treatment timing G_i as the first ...",
        "file": "01_fetch_data.R",
        "lines": [
          98,
          134
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0126/code/01_fetch_data.R#L98-L134"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "Post-treatment is coded as year >= first_treat where firs...",
        "file": "02_clean_data.R",
        "lines": [
          74,
          99
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0126/code/02_clean_data.R#L74-L99"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript\u2019s randomization inference is described for...",
        "file": "04b_randomization_inference.R",
        "lines": [
          13,
          31
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0126/code/04b_randomization_inference.R#L13-L31"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0126_scan.json"
  },
  "error": null
}