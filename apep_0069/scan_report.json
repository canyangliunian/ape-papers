{
  "paper_id": "apep_0069",
  "scan_date": "2026-02-06T12:40:06.689219+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 8,
  "flags": [
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "HIGH",
      "file": "04_robustness.R",
      "lines": [
        23,
        24,
        25
      ],
      "evidence": "This script depends on intermediate artifacts (rd_results.rds, placebo_votes_raw.rds) that are not generated anywhere in the provided code excerpt. If these files are produced by a missing script (or manually), key provenance for placebo tests and robustness outputs is unclear, which impairs auditability/reproducibility.: rd_results <- readRDS(file.path(data_dir, \"rd_results.rds\"))\nplacebo_votes <- readRDS(file.path(data_dir, \"placebo_votes_raw.rds\"))\ncanton_treatment <- readRDS(file.path(data_dir, \"canton_treatment.rds\"))",
      "confidence": 0.8
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        96,
        97,
        98,
        99,
        100,
        101,
        102,
        103,
        104,
        105,
        106,
        107,
        108,
        109,
        110,
        111,
        112,
        113,
        114,
        115,
        116,
        117,
        118,
        119,
        120,
        121,
        122,
        123,
        124
      ],
      "evidence": "Treatment assignment (which cantons are treated, adoption years, entry-into-force dates) is hard-coded rather than fetched from a cited machine-readable source. The manuscript treats timing/treated status as central to identification (RDD borders and staggered timing), so provenance should be tightened (e.g., include a script that downloads/parses LexFind or a pinned dataset snapshot; store source URLs/queries and version hashes). As-is, a reader cannot verify these values from within the replication workflow.: # Cantonal comprehensive energy laws enacted BEFORE May 21, 2017\n# Source: LexFind.ch + HuggingFace swiss_legislation\ncanton_treatment <- tribble(\n  ~canton_id, ~canton_abbr, ~adoption_year, ~entry_force_date, ~treated,\n  # Treated (energy law in force before May 2017)\n  18, \"GR\", 2010, \"2011-01-01\", TRUE,\n  2,  \"BE\", 2011, \"2012-01-01\", TRUE,\n  19, \"AG\", 2012, \"2013-01-01\", TRUE,\n  13, \"BL\", 2016, \"2016-07-01\", TRUE,\n  12, \"BS\", 2016, \"2017-01-01\", TRUE,\n  # Control cantons ...\n)",
      "confidence": 0.85
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "07_expanded_analysis.R",
      "lines": [
        704,
        705,
        706,
        707,
        708,
        709,
        710,
        711,
        712,
        713,
        714,
        715
      ],
      "evidence": "The manuscript explicitly describes staggered, time-varying treatment coding (D_ct equals 1 only if the law is in force at referendum t; BL treated in 2016/2017; BS excluded from C&S because first post period is final). In contrast, this code builds a panel using a single cross-sectional 'treated' indicator (treated by 2017) and defines post as year>=2010, then uses treat_post=treated*post. That is an 'ever-treated \u00d7 post' DiD, not the dynamic in-force indicator described. This can bias the panel estimates and undermines the claimed pre-trend/DiD interpretation.: canton_panel_long <- bind_rows(\n  votes_2000 %>% select(canton, yes_share, year),\n  votes_2003 %>% select(canton, yes_share, year),\n  votes_2016 %>% select(canton, yes_share, year),\n  votes_2017_panel\n) %>%\n  left_join(canton_2017 %>% select(canton, treated, language), by = \"canton\") %>%\n  mutate(\n    post = year >= 2010,\n    treat_post = treated * post\n  )",
      "confidence": 0.9
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "07_expanded_analysis.R",
      "lines": [
        794,
        795,
        796,
        797,
        798,
        799,
        800,
        801,
        802
      ],
      "evidence": "The manuscript defines treatment timing using 'in force' dates (e.g., GR in force Jan 2011, BE Jan 2012, AG Jan 2013, BL July 2016, BS Jan 2017) and notes Basel-Stadt should be excluded from Callaway\u2013Sant'Anna due to having only the final period post-treatment. Here, first_treat uses adoption years for GR/BE/AG (2010/2011/2012) rather than the in-force years (2011/2012/2013), and does not exclude BS. This is a material discrepancy with the paper\u2019s stated coding rules.: first_treat = case_when(\n  canton == \"GR\" ~ 2010,\n  canton == \"BE\" ~ 2011,\n  canton == \"AG\" ~ 2012,\n  canton == \"BL\" ~ 2016,\n  canton == \"BS\" ~ 2017,\n  TRUE ~ 0\n)",
      "confidence": 0.85
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "MEDIUM",
      "file": "07_expanded_analysis.R",
      "lines": [
        270,
        271,
        272
      ],
      "evidence": "The manuscript's 'same-language borders' RDD is defined as restricting to borders where language does not change discontinuously at the cutoff (German\u2013German border segments). This implementation instead filters municipalities by canton-level lang=='German' after distance-to-(pooled) policy border is computed. That does not actually guarantee the underlying cutoff corresponds to a German\u2013German border segment; it can still pool multiple borders and does not verify both sides of the local border are German-speaking. This can reintroduce language confounding or change the estimand vs. what the manuscript claims.: same_lang_sample <- rdd_sample %>%\n  filter(lang == \"German\")",
      "confidence": 0.75
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "07_expanded_analysis.R",
      "lines": [
        570,
        571,
        572,
        573,
        574,
        575,
        576,
        577
      ],
      "evidence": "Random assignment via sample() is used, but it is explicitly for permutation/randomization inference (a placebo/sensitivity check) as described in the manuscript. Not fabrication, but it is a deliberate synthetic reassignment step that should remain clearly separated from data acquisition/cleaning (it is).: perm_estimates <- replicate(n_permutations, {\n  treated_cantons_perm <- sample(all_cantons, n_treated)\n  gemeinde_data_perm <- gemeinde_data %>%\n    mutate(treated_perm = as.numeric(canton %in% treated_cantons_perm))\n  ...\n})",
      "confidence": 0.95
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        44,
        45
      ],
      "evidence": "A global seed is set at package-load time for the entire project. This is not inherently problematic, but it can unintentionally affect any downstream stochastic procedures (e.g., permutations, bandwidth searches if any randomness exists, resampling in other scripts) and make results depend on sourcing order. Prefer setting seeds locally right before randomized procedures (e.g., RI) rather than globally.: # Set seed for reproducibility\nset.seed(20260127)",
      "confidence": 0.7
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "07_expanded_analysis.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 2,
      "MEDIUM": 3,
      "LOW": 2
    },
    "one_liner": "unclear provenance; method mismatch",
    "executive_summary": "The robustness script (`04_robustness.R`) relies on intermediate `.rds` artifacts (`rd_results.rds`, `placebo_votes_raw.rds`) that are never created in the provided code, so key results cannot be reproduced or verified from the excerpt alone. The expanded analysis script (`07_expanded_analysis.R`) implements treatment coding/handling that does not align with the manuscript\u2019s stated staggered, time-varying design (e.g., \\(D_{ct}=1\\) only when the law is in force at referendum \\(t\\), specific timing for BL in 2016/2017, and exclusion rules for BS), creating a high-risk mismatch between the reported methodology and the code actually run.",
    "top_issues": [
      {
        "category": "DATA_PROVENANCE_MISSING",
        "severity": "HIGH",
        "short": "This script depends on intermediate artifacts (rd_results...",
        "file": "04_robustness.R",
        "lines": [
          23,
          24
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0069/code/04_robustness.R#L23-L25"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript explicitly describes staggered, time-varyi...",
        "file": "07_expanded_analysis.R",
        "lines": [
          704,
          705
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0069/code/07_expanded_analysis.R#L704-L715"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0069_scan.json"
  },
  "error": null
}