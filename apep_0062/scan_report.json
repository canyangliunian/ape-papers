{
  "paper_id": "apep_0062",
  "scan_date": "2026-02-06T12:38:51.961944+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 9,
  "flags": [
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "HIGH",
      "file": "03_clean_data.R",
      "lines": [
        12
      ],
      "evidence": "This script depends on an input file (qcew_gambling.csv) that is not produced by any provided fetch script. The provided fetch script (02_fetch_qcew.R) writes output/paper_80/data/qcew_panel.csv, not qcew_gambling.csv. This breaks the reproducible data lineage for the core outcome data used in the main analysis and makes it unclear what exact data version was analyzed.: qcew <- read_csv(\"output/paper_80/data/qcew_gambling.csv\", show_col_types = FALSE)",
      "confidence": 0.9
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_policy_dates.R",
      "lines": [
        75,
        155,
        232
      ],
      "evidence": "Policy timing (treatment dates and implementation types) is manually hard-coded into the script rather than fetched/parsed from a source. The manuscript does describe the source (Legal Sports Report + verification), which mitigates the concern, but without a raw-source import step or an audit trail (URLs, scrape logs, or citations per-state), this is harder to independently verify and easier to inadvertently (or intentionally) alter.: sports_betting_dates <- tibble(\n  state_abbr = c(\n    \"DE\", \"NJ\", \"MS\", \"WV\", \"PA\", \"RI\",\n    ...\n  ),\n  sb_first_date = as.Date(c(\n    \"2018-06-05\", \"2018-06-14\", ...\n  )),\n  implementation_type = c(\n    \"retail\", \"both\", ...\n  )\n)\n...\nwrite_csv(all_states, \"output/paper_80/data/policy_dates.csv\")",
      "confidence": 0.75
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "MEDIUM",
      "file": "04_main_analysis.R",
      "lines": [
        39,
        40
      ],
      "evidence": "The analysis drops all observations with employment equal to zero, in addition to missing. Dropping based on the outcome can bias estimates if zeros are genuine outcomes (e.g., states/years with no NAICS 7132 employment) rather than suppression codes. The manuscript notes suppression/zeros are excluded, which can be legitimate, but the code does not distinguish suppression from true zeros here and does not show sensitivity to including zeros (e.g., treating zeros as true zeros or separating by disclosure_code).: df_clean <- df %>%\n  filter(!is.na(employment), employment > 0)",
      "confidence": 0.8
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "03_clean_data.R",
      "lines": [
        24,
        27,
        30
      ],
      "evidence": "This script constructs a cleaned outcome variable (employment_clean) that sets suppressed/zero observations to NA, but the main analysis (04_main_analysis.R) uses yname = \"employment\" rather than employment_clean. That means the intended suppression handling is not actually used as the regression outcome; instead, the main analysis relies on filtering employment > 0. This is a substantive implementation difference and could change results if any non-suppressed zeros or miscoded values exist.: qcew_clean <- qcew %>%\n  mutate(\n    suppressed = disclosure_code == \"N\" | employment == 0,\n    employment_clean = if_else(suppressed, NA_real_, as.numeric(employment))\n  ) %>%\n  ...",
      "confidence": 0.85
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "04_main_analysis.R",
      "lines": [
        56,
        101,
        118
      ],
      "evidence": "The manuscript reports a joint pre-trends test p-value (e.g., p = 0.92). The code computes an approximate chi-squared joint test assuming independence across event-time coefficients. That assumption is generally false in DiD/event-study settings (coefficients are correlated), so the computed p-value may not match a correct Wald test using the full variance-covariance matrix. This is not fabrication, but it is a mismatch between the statistical test implied by the manuscript language and what is implemented here.: cs_never <- att_gt(\n  yname = \"employment\",\n  tname = \"year\",\n  idname = \"state_id\",\n  gname = \"G\",\n  data = df_clean,\n  control_group = \"nevertreated\",\n  bstrap = TRUE,\n  cband = TRUE,\n  biters = 1000,\n  base_period = \"varying\"\n)\n...\n# Pre-trend test\n# Simple chi-squared test (assuming independence for approximation)\nchi2_stat <- sum((pre_att / pre_se)^2)\n...\np_pretrend <- pchisq(chi2_stat, df = df_test, lower.tail = FALSE)",
      "confidence": 0.7
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "08_create_figures.R",
      "lines": [
        5
      ],
      "evidence": "Hard-coded local working directory makes the script non-reproducible on other machines/environments and obscures how paths are intended to resolve in replication. This is a reproducibility/provenance concern (not evidence of falsification), especially for an academic replication package.: setwd(\"/Users/dyanag/auto-policy-evals/output/paper_80\")",
      "confidence": 0.9
    }
  ],
  "file_verdicts": [
    {
      "file": "07_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_fetch_qcew.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_clean_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "01_policy_dates.R",
      "verdict": "CLEAN"
    },
    {
      "file": "08_create_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_main_analysis.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 1,
      "MEDIUM": 4,
      "LOW": 1
    },
    "one_liner": "unclear provenance",
    "executive_summary": "The data-cleaning pipeline in `03_clean_data.R` relies on an input file, `qcew_gambling.csv`, whose provenance is unclear because no provided retrieval script generates it. The only included fetch script, `02_fetch_qcew.R`, instead writes `output/paper_80/data/qcew_panel.csv`, leaving the expected input missing and making the analysis workflow non-reproducible without undocumented external data or manual steps.",
    "top_issues": [
      {
        "category": "DATA_PROVENANCE_MISSING",
        "severity": "HIGH",
        "short": "This script depends on an input file (qcew_gambling.csv) ...",
        "file": "03_clean_data.R",
        "lines": [
          12
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0062/code/03_clean_data.R#L12"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0062_scan.json"
  },
  "error": null
}