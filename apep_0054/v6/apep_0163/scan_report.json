{
  "paper_id": "apep_0163",
  "scan_date": "2026-02-06T12:55:22.525462+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "CLEAN",
  "files_scanned": 7,
  "flags": [
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_qwi_fast.R",
      "lines": [
        107,
        119,
        196,
        197
      ],
      "evidence": "Treatment timing (a key input that determines treated vs control and post vs pre) is hard-coded inside the data-fetch script and then written to data/transparency_laws.rds. In 00_packages.R the comment claims timing provenance is in 00_policy_data.R with \u201cofficial citations\u201d, but that script is not provided here and this script overwrites/creates the RDS without embedding provenance (URLs, bill IDs). This is not fabrication, but it weakens auditability because a central identifying assumption hinges on correct treatment dates.: treat_timing <- tribble(\n  ~state_fips, ~treat_qtr_num, ~state_abbr,\n  \"08\", 2021 * 4 + 1, \"CO\",   # 2021Q1\n  \"09\", 2021 * 4 + 4, \"CT\",   # 2021Q4\n  \"32\", 2021 * 4 + 4, \"NV\",   # 2021Q4\n  \"44\", 2023 * 4 + 1, \"RI\",   # 2023Q1\n  \"06\", 2023 * 4 + 1, \"CA\",   # 2023Q1\n  \"53\", 2023 * 4 + 1, \"WA\"    # 2023Q1\n)\n...\nsaveRDS(treat_timing, \"data/transparency_laws.rds\")",
      "confidence": 0.76
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "LOW",
      "file": "06_tables.R",
      "lines": [
        200,
        215
      ],
      "evidence": "Table of treatment timing and treated-county counts is hard-coded rather than computed from the analysis data (qwi_analysis.rds / transparency_laws.rds). Given the manuscript explicitly lists the same effective dates, this is likely transcription for a descriptive table, but it still introduces a manual step that can drift from the actual coded treatment variable if dates/counties change.: timing_table <- tibble(\n  State = c(\"Colorado\", \"Connecticut\", \"Nevada\", \"Rhode Island\",\n            \"California\", \"Washington\"),\n  Abbreviation = c(\"CO\", \"CT\", \"NV\", \"RI\", \"CA\", \"WA\"),\n  `Effective Date` = c(\"January 1, 2021\", \"October 1, 2021\", \"October 1, 2021\",\n                       \"January 1, 2023\", \"January 1, 2023\", \"January 1, 2023\"),\n  Cohort = c(\"2021Q1\", \"2021Q4\", \"2021Q4\", \"2023Q1\", \"2023Q1\", \"2023Q1\"),\n  `Post-Treatment Quarters` = c(12, 9, 9, 4, 4, 4),\n  Counties = c(64, 8, 17, 5, 58, 39)\n)",
      "confidence": 0.71
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "06_tables.R",
      "lines": [
        60,
        89
      ],
      "evidence": "The table reports N for Callaway-Sant\u2019Anna and TWFE as nrow(qwi), i.e., the county-quarter-sex dataset. But the main C-S and TWFE regressions in 03_main_analysis.R are run on a collapsed county-quarter panel (qwi_panel), so N in the regression is smaller than nrow(qwi). The manuscript\u2019s Table 1/2 numbers (e.g., 48,189 vs 24,139) indicate different Ns across specifications; this table-construction code is likely to misreport sample sizes and potentially clusters if the analysis sample differs from qwi (e.g., finite-log filtering). This is a reporting integrity risk (mislabeling the estimating sample), not a wrong estimator per se.: main_results <- tibble(\n  Specification = c(\n    \"Callaway-Sant'Anna\",\n    \"TWFE (fixest)\",\n    \"Border County-Pairs\"\n  ),\n  ...\n  N = c(\n    nrow(qwi),\n    nrow(qwi),\n    nrow(readRDS(\"data/qwi_border.rds\"))\n  ),\n  Clusters = c(\n    n_distinct(qwi$state_fips),\n    n_distinct(qwi$state_fips),\n    nrow(readRDS(\"data/border_pairs.rds\"))\n  )\n)",
      "confidence": 0.84
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "06_tables.R",
      "lines": [
        35,
        47
      ],
      "evidence": "Footnote claims the sample covers 1995\u20132023, but the fetch script is explicitly 2015:2023 and the manuscript states 2015Q1\u20132023Q4. This looks like a copy/paste documentation error, but it can mislead readers about the time window and available pre-periods.: add_footnote(\"Note: Sample includes all county-quarter-sex observations from 1995-2023.\nNew hire earnings and all earnings in dollars. Employment and hires are quarterly counts.\nTreated states: CA, CO, CT, NV, RI, WA.\",",
      "confidence": 0.8
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "LOW",
      "file": "04_robustness.R",
      "lines": [
        73,
        114
      ],
      "evidence": "Multiple robustness specifications are run and saved (border_did, border_ddd, placebo_twfe, twfe_never, twfe_no_cawa) but the manuscript\u2019s robustness table/summary is selective in the sense that it presents only a subset and does not clearly mention the border DDD output (border_ddd) in tabular form (it is described in text). This is not inherently problematic\u2014papers routinely summarize key checks\u2014but for transparency it would be better to either (i) include all run robustness results in an appendix table or (ii) explicitly state which robustness outputs were run but omitted and why.: # Robustness 2: Border Design by Sex\n...\n# Robustness 3: Placebo Tests\n...\n# Robustness 4: Alternative Control Groups (TWFE approach)\n...\n# Robustness 5: Exclude CA/WA (Largest States) - TWFE",
      "confidence": 0.55
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "02_clean_data.R",
      "lines": [
        21,
        33
      ],
      "evidence": "Border adjacency is constructed from TIGER/Line geometries fetched via tigris without pinning a specific TIGER vintage beyond year=2020 and without saving the shapefile inputs as raw data artifacts. This is generally acceptable (public data + declared year), but small geometry changes across versions can alter adjacency pairs. For full reproducibility, consider caching/saving the exact shapefile or at least recording TIGER release metadata.: counties_sf <- tigris::counties(cb = TRUE, year = 2020, progress_bar = FALSE) %>%\n  st_transform(4326) %>%\n  select(GEOID, STATEFP, NAME, geometry) %>%\n  rename(county_fips = GEOID, state_fips = STATEFP, county_name = NAME)\n\n# Find adjacent counties (share a border)\nadj_matrix <- st_touches(counties_sf, sparse = TRUE)",
      "confidence": 0.62
    }
  ],
  "file_verdicts": [
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "01_fetch_qwi_fast.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "CLEAN",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 0,
      "MEDIUM": 2,
      "LOW": 4
    },
    "one_liner": "Minor issues only",
    "executive_summary": "Minor code quality issues detected, but no evidence of data fabrication or manipulation.",
    "top_issues": [],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0163_scan.json"
  },
  "error": null
}