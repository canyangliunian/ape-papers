{
  "paper_id": "apep_0158",
  "scan_date": "2026-02-06T12:54:13.313066+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 9,
  "flags": [
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "07_tables.R",
      "lines": [
        82,
        83,
        95,
        111
      ],
      "evidence": "The manuscript frames the main estimates as coming from heterogeneity-robust staggered DiD (Callaway-Sant'Anna / Sun-Abraham), but the main LaTeX regression table generated here is TWFE-only (m1, m3, m4, m5). Although C-S ATT objects are loaded/extracted (cs_att/cs_se), they are not inserted into the exported table (etable only receives m1/m3/m4/m5). This can create a substantive mismatch between what the paper claims is reported and what the code actually outputs.: # (1) Simple TWFE\nm1 <- feols(y ~ treat_post | statefip + income_year, data = state_year, cluster = ~statefip)\n\n# (2) C-S Simple ATT\n# Extract from results\ncs_att <- results$att_simple$overall.att\ncs_se <- results$att_simple$overall.se\n\n# Export table\netable(m1, m3, m4, m5, ...)",
      "confidence": 0.84
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "01_fetch_data.R",
      "lines": [
        132,
        133,
        135,
        157
      ],
      "evidence": "The manuscript states minimum wage controls are incorporated (\"I also incorporate state minimum wage data from the Department of Labor to control for concurrent policy changes\"), but the code here only creates a skeletal state lookup and explicitly defers constructing actual minimum wage-by-year values. In the provided cleaning/analysis code (02_clean_data.R, 04_main_analysis.R), no minimum-wage panel is merged into df/state_year, and the main models shown in code do not include a min-wage control. This is a potential implementation-vs-claims discrepancy affecting identification/robustness.: # ---- State Minimum Wages ----\n# Source: DOL / UC Berkeley Labor Center\n# We'll use a simplified approach with known minimum wages\n\nstate_min_wage <- tibble(\n  statefip = 1:56,\n  state_abbr = c(\"AL\", \"AK\", \"AZ\", ... , \"WY\", NA, NA, NA, NA, NA)\n) %>%\n  filter(!is.na(state_abbr))\n\n# For now, we'll construct minimum wage data in the cleaning script\n# using known values for the analysis period",
      "confidence": 0.87
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        132,
        133,
        135,
        157
      ],
      "evidence": "State minimum wage data are referenced as coming from DOL / UC Berkeley but are not actually fetched (no URL download/API call) or stored as a versioned dataset. The script instead contains a placeholder mapping and a note that values will be constructed later. If minimum wages materially matter for the paper's control strategy, this is missing provenance and reduces reproducibility.: # Source: DOL / UC Berkeley Labor Center\n# We'll use a simplified approach with known minimum wages\n...\n# For now, we'll construct minimum wage data in the cleaning script\n# using known values for the analysis period",
      "confidence": 0.78
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "LOW",
      "file": "00_policy_data.R",
      "lines": [
        33,
        44,
        63,
        120
      ],
      "evidence": "Treatment timing and cohort coding are hard-coded as literals. In context this is largely justified (the manuscript provides detailed legislative citations and dates), but it still creates a potential integrity risk if dates are modified without trace. Consider adding a small provenance check (e.g., store a signed/hashed reference table, or include a script that scrapes/validates effective dates from the cited sources at build time).: transparency_laws <- tibble(\n  statefip = c(8, 9, 32, 44, 6, 53, 36, 15),\n  state = c(\"Colorado\", \"Connecticut\", \"Nevada\", \"Rhode Island\", \"California\", \"Washington\", \"New York\", \"Hawaii\"),\n  effective_date = as.Date(c(\"2021-01-01\", \"2021-10-01\", \"2021-10-01\", \"2023-01-01\", \"2023-01-01\", \"2023-01-01\", \"2023-09-17\", \"2024-01-01\")),\n  first_treat = c(2021, 2022, 2022, 2023, 2023, 2023, 2024, 2024),\n  employer_threshold = c(\"All employers\", \"All employers\", \"All employers\", \"All employers\", \"15+ employees\", \"15+ employees\", \"4+ employees\", \"50+ employees\"),\n  law_citation = c(...),\n  source_url = c(...)\n)",
      "confidence": 0.74
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "02_clean_data.R",
      "lines": [
        118,
        132,
        147,
        157
      ],
      "evidence": "The code trims the analysis sample by dropping wages outside [p1, p99] bounds computed from pre-treatment data. This is potentially consequential for estimated effects if policy changes affect the wage distribution tails. However, the manuscript explicitly discusses this choice and frames it as avoiding post-treatment outcome-conditioned trimming; applying pre-treatment bounds symmetrically to all years is a defensible approach. Still, it would be useful to (i) report sensitivity to no trimming, and/or (ii) winsorize instead of drop to avoid changing composition.: # Calculate bounds from pre-treatment period ONLY\npre_treatment_wages <- df %>%\n  filter(income_year <= 2020) %>%\n  pull(hourly_wage)\n\nwage_bounds <- quantile(pre_treatment_wages, c(0.01, 0.99), na.rm = TRUE)\n...\n# Apply same bounds to ALL observations\ndf <- df %>%\n  filter(hourly_wage >= wage_bounds[1], hourly_wage <= wage_bounds[2])",
      "confidence": 0.8
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "05_robustness.R",
      "lines": [
        330,
        337,
        356,
        365
      ],
      "evidence": "The robustness table computes a single 'Sun-Abraham ATT' as the mean of post-treatment event-time coefficients and the mean of their standard errors. This is not a standard aggregation and can materially differ from an appropriately weighted average post-treatment effect with correct uncertainty propagation. If this robustness number is reported in the manuscript, it risks overstating precision or changing magnitude relative to a proper aggregate. This looks like a reporting vulnerability (not necessarily intentional) because it creates discretion in how a single summary is formed from multiple coefficients.: robustness_summary <- tibble(\n  Specification = c(\n    \"Main (C-S, never-treated)\",\n    \"Sun-Abraham\",\n    ...\n  ),\n  ATT = c(\n    results$att_simple$overall.att,\n    mean(sa_coefs$att[sa_coefs$event_time >= 0], na.rm = TRUE),\n    ...\n  ),\n  SE = c(\n    results$att_simple$overall.se,\n    mean(sa_coefs$se[sa_coefs$event_time >= 0], na.rm = TRUE),\n    ...\n  )\n)",
      "confidence": 0.81
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "05_robustness.R",
      "lines": [
        438,
        456,
        468,
        516
      ],
      "evidence": "Random draws (set.seed/sample) are used to implement randomization inference/permutation tests. This is appropriate and explicitly described in the manuscript as randomization inference; not evidence of data fabrication. Flagged only because it matches common 'simulated data' patterns.: set.seed(20260203)  # Reproducible\nn_permutations <- 500\n...\nfake_treated_states <- sample(all_states, n_treated)\n...\nfake_years <- sample(actual_treated$first_treat, n_treated, replace = FALSE)\n...",
      "confidence": 0.9
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "07_tables.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_policy_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_descriptives.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 2,
      "MEDIUM": 2,
      "LOW": 3
    },
    "one_liner": "method mismatch",
    "executive_summary": "The code generating the main LaTeX regression table in `07_tables.R` reports only two-way fixed effects (TWFE) specifications (e.g., `m1`, `m3`) even though the manuscript presents the headline estimates as coming from heterogeneity-robust staggered DiD methods (Callaway\u2013Sant\u2019Anna / Sun\u2013Abraham). In addition, `01_fetch_data.R` does not actually incorporate the promised state minimum-wage control data from the Department of Labor, so the stated controls for concurrent policy changes are not implemented in the analysis pipeline.",
    "top_issues": [
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript frames the main estimates as coming from h...",
        "file": "07_tables.R",
        "lines": [
          82,
          83
        ],
        "github_url": "/apep_0158/code/07_tables.R#L82-L111"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript states minimum wage controls are incorpora...",
        "file": "01_fetch_data.R",
        "lines": [
          132,
          133
        ],
        "github_url": "/apep_0158/code/01_fetch_data.R#L132-L157"
      }
    ],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0158_scan.json"
  },
  "error": null
}