{
  "paper_id": "apep_0054",
  "scan_date": "2026-02-06T12:37:06.912279+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 8,
  "flags": [
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "paper.tex",
      "lines": [
        86,
        266
      ],
      "evidence": "The manuscript states a different CPS ASEC sample window than the code actually uses. This affects pre/post periods, treated exposure, and the number of post-treatment years available\u2014directly impacting estimates and reported sample sizes.: Paper claims: \"I use CPS ASEC surveys from 2016 through 2025, corresponding to income years 2015 through 2024.\"",
      "confidence": 0.85
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "01_fetch_data.R",
      "lines": [
        18,
        24
      ],
      "evidence": "Code downloads CPS ASEC March supplements 2015\u20132024, which correspond to income years 2014\u20132023 (because income_year = YEAR - 1). This conflicts with the paper\u2019s stated 2016\u20132025 surveys / 2015\u20132024 income years.: samples = paste0(\"cps\", 2015:2024, \"_03s\"),  # March ASEC supplements\n...\ndf_raw[, income_year := YEAR - 1]",
      "confidence": 0.9
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "MEDIUM",
      "file": "paper.tex",
      "lines": [
        316,
        318
      ],
      "evidence": "The manuscript claims winsorization (retaining observations but capping extremes), but the code implements trimming by dropping observations outside the 1st/99th percentiles. Trimming can change composition (especially in tails that may differ by state/year/gender) and can bias DiD estimates if treatment affects tail mass.: Paper states: \"I winsorize hourly wages at the 1st and 99th percentiles to reduce the influence of outliers.\"",
      "confidence": 0.9
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "MEDIUM",
      "file": "02_clean_data.R",
      "lines": [
        142,
        153
      ],
      "evidence": "Implements sample trimming (dropping the bottom/top 1%) rather than winsorization. Additionally, the quantiles are unweighted even though CPS ASEC weights are used in estimation; weighted trimming/winsorization could materially differ.: wage_bounds <- quantile(df$hourly_wage, c(0.01, 0.99), na.rm = TRUE)\n...\ndf <- df %>%\n  filter(hourly_wage >= wage_bounds[1], hourly_wage <= wage_bounds[2]) %>%\n  mutate(\n    log_hourly_wage = log(hourly_wage)\n  )",
      "confidence": 0.85
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "paper.tex",
      "lines": [
        300,
        308
      ],
      "evidence": "The provided code does not request or use CPS ASEC imputation flags (e.g., allocated/edited income indicators) and does not exclude imputed wage records. This is a stated sample restriction in the paper that appears not to be implemented in code.: Paper states: \"I exclude observations with imputed wage data to ensure measurement quality.\"",
      "confidence": 0.75
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        140,
        177
      ],
      "evidence": "The manuscript states state minimum wage controls are incorporated, but the code does not actually fetch or construct a state-year minimum wage series, nor merge it into the analysis dataset. It is only stubbed as a comment/placeholder. This is a provenance/implementation gap for a stated control variable.: # ---- State Minimum Wages ----\n# Source: DOL / UC Berkeley Labor Center\n# We'll use a simplified approach with known minimum wages\n\nstate_min_wage <- tibble(\n  statefip = 1:56,\n  state_abbr = c(\"AL\", \"AK\", ... , NA, NA, NA, NA, NA)\n) %>%\n  filter(!is.na(state_abbr))\n\n# For now, we'll construct minimum wage data in the cleaning script\n# using known values for the analysis period\n\n# ---- State Unemployment Rates ----\n# Would fetch from BLS LAUS API\n# For now, we'll use state-year fixed effects which absorb this",
      "confidence": 0.8
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        91,
        132
      ],
      "evidence": "Treatment timing and thresholds are hard-coded rather than pulled from a documented external file/source. This is not automatically problematic (treatment dates are often coded manually), but it does create room for untracked edits. Consider adding citations/URLs per state and/or reading from a versioned CSV in the repo with documented sources.: transparency_laws <- tibble(\n  state_name = c(\"Colorado\", \"Connecticut\", ...),\n  statefip = c(8, 9, 32, 44, 6, 53, 36, 15, 24, 17, 27, 34, 50, 25),\n  effective_date = as.Date(c(\"2021-01-01\", \"2021-10-01\", ...)),\n  employer_threshold = c(1, 1, 1, 1, 15, 15, 4, 50, 1, 15, 30, 10, 5, 25)\n) %>%\n  mutate(\n    income_year_first = case_when(\n      month(effective_date) <= 3 ~ year(effective_date),\n      TRUE ~ year(effective_date) + 1\n    ),\n    first_treat = income_year_first\n  )",
      "confidence": 0.7
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "paper.tex",
      "lines": [
        287,
        295
      ],
      "evidence": "The paper describes treatment timing compilation, but the code does not include source references (links/citations) for the hard-coded dates. This is a documentation/provenance weakness rather than evidence of wrongdoing.: Paper states: \"compiled from state legislative records, legal databases, and secondary sources\" (treatment timing).",
      "confidence": 0.65
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "05_robustness.R",
      "lines": [
        18,
        30
      ],
      "evidence": "In 05_robustness.R, `g` and `y` are referenced but are not constructed in this script before use (they are created in 04_main_analysis.R but not saved back into state_year_panel.rds). Unless state_year_panel.rds already contains `g`/`y`, this robustness script will error or silently use wrong variables if they exist from a prior environment. This undermines reproducibility and can lead to discrepancies between reported robustness checks and what actually ran.: state_year <- state_year %>%\n  mutate(\n    cohort = factor(ifelse(g == 0, Inf, g))  # sunab needs Inf for never-treated\n  )\n\nsa_result <- feols(\n  y ~ sunab(cohort, income_year) | statefip + income_year,\n  data = state_year,\n  cluster = ~statefip\n)",
      "confidence": 0.85
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "LOW",
      "file": "07_tables.R",
      "lines": [
        63,
        86
      ],
      "evidence": "Table-generation code computes/extracts Callaway-Sant\u2019Anna overall ATT (`cs_att`, `cs_se`) but does not include it in the exported main results table (etable only includes m1, m3, m4, m5). The manuscript emphasizes C-S as the primary estimator. This could be benign (ATT shown elsewhere), but it is a potential selective presentation / inconsistency risk if the paper\u2019s displayed table is supposed to include C-S but the code omits it.: # (2) C-S Simple ATT\n# Extract from results\ncs_att <- results$att_simple$overall.att\ncs_se <- results$att_simple$overall.se\n\n# Export table\netable(m1, m3, m4, m5, ...)",
      "confidence": 0.7
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "07_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_descriptives.R",
      "verdict": "CLEAN"
    },
    {
      "file": "paper.tex",
      "verdict": "SUSPICIOUS"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 2,
      "MEDIUM": 5,
      "LOW": 3
    },
    "one_liner": "method mismatch",
    "executive_summary": "The code uses a different CPS ASEC sample window than the manuscript describes: `01_fetch_data.R` downloads March ASEC supplements for 2015\u20132024 and then sets `income_year = YEAR - 1`, so the analysis actually covers income years 2014\u20132023 rather than the paper\u2019s stated 2016\u20132025 surveys / 2015\u20132024 income years. This discrepancy shifts the defined pre/post periods and treated exposure timing and changes how many post-treatment years are available, meaning the reported identification window and resulting estimates are not aligned with what is implemented in the code.",
    "top_issues": [
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript states a different CPS ASEC sample window ...",
        "file": "paper.tex",
        "lines": [
          86,
          266
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0054/code/paper.tex#L86-L266"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "Code downloads CPS ASEC March supplements 2015\u20132024, whic...",
        "file": "01_fetch_data.R",
        "lines": [
          18,
          24
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0054/code/01_fetch_data.R#L18-L24"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0054_scan.json"
  },
  "error": null
}