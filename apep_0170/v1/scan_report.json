{
  "paper_id": "apep_0170",
  "scan_date": "2026-02-06T12:56:44.335234+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 7,
  "flags": [
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "04_robustness.R",
      "lines": [
        34,
        35,
        36,
        37,
        38,
        39
      ],
      "evidence": "The manuscript describes an event study / pre-trends test comparing treated states to control states (parallel trends). This regression filters to treated states only, so it does not identify differential pre-trends relative to untreated controls. With only treated units, the event-time indicators are not anchored to a control-group counterfactual; at best it tests within-treated deviations relative to the chosen reference period, not pre-trend differences vs controls. This can materially overstate support for parallel trends if presented as a treated-vs-control event study.: es_reg <- feols(\n  p90_p10 ~ i(rel_time_factor, ref = \"-1\") | statefip + year,\n  data = state_year %>% filter(first_treat > 0),  # Treated states only\n  cluster = ~statefip\n)",
      "confidence": 0.9
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "MEDIUM",
      "file": "02_clean_data.R",
      "lines": [
        104,
        105,
        106,
        107,
        108,
        109
      ],
      "evidence": "State-year percentiles are computed unweighted even though ACS is a complex survey and the script also computes weighted means/variances. The comment says weights will be used \"in final\" but no weighted-quantile implementation appears in the provided code, so the final dependent variables (p90_p10, etc.) are effectively unweighted. The manuscript acknowledges unweighted percentiles as a limitation, which reduces severity, but the code comment is internally inconsistent and could mislead readers about what is actually implemented.: # Percentiles (unweighted for simplicity; weight in final)\n    p10 = quantile(log_wage, 0.10, na.rm = TRUE),\n    p50 = quantile(log_wage, 0.50, na.rm = TRUE),\n    p90 = quantile(log_wage, 0.90, na.rm = TRUE),",
      "confidence": 0.8
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "paper.tex",
      "lines": [
        450,
        451,
        452,
        453,
        454,
        455
      ],
      "evidence": "Key regression outputs (coefficients, SEs, R^2, N, p-values in notes) are hard-coded directly into LaTeX tables rather than being programmatically produced from model objects (e.g., via modelsummary/texreg) and included. The repository does write CSV outputs (tables/main_results.csv), but there is no code linking those outputs to the LaTeX tables, so transcription errors or selective transcription are possible. The manuscript is not a calibration/meta-analysis paper where hard-coding estimates would be expected.: SHB & -0.045 & -0.008 & 0.000 \\\\\n    & (0.038) & (0.011) & (0.018) \\\\\nObservations & 600 & 600 & 600 \\\\\n$R^2$ & 0.141 & 0.160 & 0.196 \\\\",
      "confidence": 0.75
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        78,
        79,
        80,
        81,
        82
      ],
      "evidence": "The script constructs the list of states via a hard-coded vector of FIPS codes plus treated-state FIPS codes. This is not inherently invalid, but it is an undocumented source of the analysis universe (e.g., omission of DC=11 appears intentional but is not stated). If the intent is \"all states\", it would be cleaner to derive from the microdata (STATEFIP present in IPUMS) or a canonical state list to avoid accidental omissions.: states <- unique(c(shb_dates$statefip, \n                   2, 4, 5, 12, 13, 16, 18, 19, 20, 21, 22, 26, 27, 28, 29,\n                   30, 31, 33, 35, 37, 38, 39, 40, 42, 45, 46, 47, 48, 49, 54, 55, 56))",
      "confidence": 0.6
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "01_fetch_data.py",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 1,
      "MEDIUM": 2,
      "LOW": 1
    },
    "one_liner": "method mismatch",
    "executive_summary": "The pre-trends / event-study regression in `04_robustness.R` filters the data to treated states only, even though the manuscript describes comparing treated states to control states to assess parallel trends. With no control group in the estimation sample, the regression cannot identify treatment effects or valid pre-trend differences, so it does not implement the stated methodology and undermines the credibility of the reported robustness check.",
    "top_issues": [
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript describes an event study / pre-trends test...",
        "file": "04_robustness.R",
        "lines": [
          34,
          35
        ],
        "github_url": "/apep_0170/code/04_robustness.R#L34-L39"
      }
    ],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0170_scan.json"
  },
  "error": null
}