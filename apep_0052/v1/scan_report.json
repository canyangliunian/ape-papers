{
  "paper_id": "apep_0052",
  "scan_date": "2026-02-06T12:36:56.968565+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 9,
  "flags": [
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "HIGH",
      "file": "01_fetch_data.R",
      "lines": [
        206
      ],
      "evidence": "Key outcome dataset (LocalView panel) is loaded from a local parquet path with no code in the repository that downloads/builds it from a documented source. This prevents an auditor from verifying provenance, inclusion/exclusion rules, and whether the data correspond to what the manuscript claims.: localview <- arrow::read_parquet(\"data/localview/panel_place_month.parquet\")",
      "confidence": 0.86
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "07_tables.R",
      "lines": [
        14,
        18
      ],
      "evidence": "Tables are built partly from an externally stored CSV (tables/main_results.csv) rather than exclusively from in-memory model objects. If this CSV is edited manually or generated by a different script/specification, the final tables may not reflect the stated estimation. This is an integrity risk unless the workflow is locked down (e.g., always regenerate main_results.csv from the preferred script in the same run).: load(\"data/cs_results.RData\")\n\n# Main results\nmain_results <- read_csv(\"tables/main_results.csv\", show_col_types = FALSE)",
      "confidence": 0.78
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        79,
        110,
        149,
        169
      ],
      "evidence": "Much of the data are fetched from APIs/URLs at runtime without pinning versions (e.g., checksums, archive links, or saving raw API responses). This is usually acceptable, but it reduces reproducibility and makes later forensic verification harder (API results can change with revisions).: download.file(ruggedness_url, \"data/rugged_data.dta\", mode = \"wb\", quiet = TRUE)\n...\ndownload.file(fcc_2019_url, \"data/fcc_county_dec2019.csv\", mode = \"wb\", quiet = TRUE)\n...\ndemo_raw <- fetch_acs(2018, demo_vars, state = \"*\", geography = \"place\")\n...\nplaces_sf <- tigris::places(year = 2019, cb = TRUE)",
      "confidence": 0.7
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "04_iv_analysis.R",
      "lines": [
        2,
        33,
        52,
        87,
        106
      ],
      "evidence": "The script header and comments frame the IV as \"Terrain Ruggedness\", but the implemented instrument is a constructed state-level rurality index interacted with post-2015. That is not terrain ruggedness, not plausibly exogenous on typical exclusion-restriction grounds, and is a materially different identification strategy. Unless the manuscript explicitly describes this proxy and motivates it, this is a major methodology mismatch.: # 04_iv_analysis.R - Instrumental Variables Analysis (Terrain Ruggedness)\n...\n# For now, we'll use state-level rural/urban classification as a weaker instrument\n...\nrural_index = scale(pct_small_places)[,1]\n...\ninstrument = rural_index * post2015\n...\niv_individual <- feols(\n  individualizing ~ 1 | place_id + year | broadband_rate ~ instrument,\n  data = analysis,\n  cluster = ~state_fips\n)",
      "confidence": 0.9
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "MEDIUM",
      "file": "02_clean_data.R",
      "lines": [
        90,
        137,
        139,
        141
      ],
      "evidence": "Sample restrictions may be reasonable, but they can be consequential. Filtering on n_meetings (a text-production/intensity measure plausibly correlated with moral-language outcomes) can induce selection on post-treatment behavior if broadband affects meeting frequency/transcription availability/word counts. This should be justified, tested for sensitivity, and (ideally) shown not to be affected by treatment (placebo/attrition checks).: analysis <- panel %>%\n  filter(year >= 2013, year <= 2022) %>%\n  # Require minimum meetings for reliability\n  filter(n_meetings >= 2) %>%\n  # Remove places with missing demographics\n  filter(!is.na(population))",
      "confidence": 0.74
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        247,
        248
      ],
      "evidence": "Adding 0.001 before logs is a defensible continuity correction, but it is an analyst-chosen constant that can matter when values are near zero. This should be disclosed and sensitivity-checked (e.g., 1e-4, 1e-3, 1e-2; or using inverse hyperbolic sine / log1p-style transforms).: univ_comm_ratio = individualizing / binding,\n# Log transform for ratio (avoid infinite values)\nlog_univ_comm = log(individualizing + 0.001) - log(binding + 0.001)",
      "confidence": 0.66
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "03_main_analysis.R",
      "lines": [
        300,
        303
      ],
      "evidence": "Multiple scripts generate outputs with overlapping filenames (e.g., tables/main_results.csv is written in 03_main_analysis.R, and also written in run_analysis.R with different estimators). Without a single orchestrated pipeline, it is easy to accidentally (or intentionally) present results from a different estimator/spec than claimed in the manuscript by running scripts in a particular order.: write_csv(results_summary, \"tables/main_results.csv\")\n...\nsave(..., file = \"data/cs_results.RData\")",
      "confidence": 0.8
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "run_analysis.R",
      "lines": [
        156,
        157
      ],
      "evidence": "This script overwrites the same main results CSV used by the table-building script, but it produces TWFE results (Estimator = \"TWFE\") rather than Callaway-Sant'Anna. If 07_tables.R is run after this, tables may silently report TWFE instead of the stated preferred estimator, or vice versa.: write_csv(results_table, \"tables/main_results.csv\")",
      "confidence": 0.85
    },
    {
      "category": "STATISTICAL_IMPOSSIBILITY",
      "severity": "LOW",
      "file": "run_analysis.R",
      "lines": [
        110,
        111,
        112,
        113
      ],
      "evidence": "The event-study dataset inserts a reference-period point with SE = 0. While the coefficient is normalized to 0 by construction, an SE of exactly 0 can create downstream issues (infinite t-stats if used mechanically, overly narrow ribbons if included). This is typically handled by omitting the reference period from uncertainty plots or setting SE to NA.: bind_rows(\n    data.frame(outcome = c(\"Individualizing\", \"Binding\"),\n               time = c(-1, -1), att = c(0, 0), se = c(0, 0))\n  )",
      "confidence": 0.83
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        55
      ],
      "evidence": "A global random seed is set for reproducibility. This is not evidence of fabrication by itself, but it interacts with bootstrap-based inference in did::att_gt (bstrap=TRUE) and any other stochastic steps. Documenting where randomness is used (bootstraps, sampling) would help auditing.: set.seed(20260122)",
      "confidence": 0.52
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "07_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_iv_analysis.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "run_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 2,
      "MEDIUM": 4,
      "LOW": 4
    },
    "one_liner": "unclear provenance; method mismatch",
    "executive_summary": "The code relies on a key outcome dataset (\u201cLocalView\u201d panel) loaded from a local Parquet file path, but provides no repository code to download, construct, or document the source of that dataset, making the results non-auditable and non-reproducible. In addition, the IV analysis script describes the instrument as \u201cTerrain Ruggedness,\u201d yet the implemented instrument is actually a constructed state-level rurality index interacted with a post\u20112015 indicator, indicating a substantive mismatch between the stated methodology and what is estimated.",
    "top_issues": [
      {
        "category": "DATA_PROVENANCE_MISSING",
        "severity": "HIGH",
        "short": "Key outcome dataset (LocalView panel) is loaded from a lo...",
        "file": "01_fetch_data.R",
        "lines": [
          206
        ],
        "github_url": "/apep_0052/code/01_fetch_data.R#L206"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The script header and comments frame the IV as \"Terrain R...",
        "file": "04_iv_analysis.R",
        "lines": [
          2,
          33
        ],
        "github_url": "/apep_0052/code/04_iv_analysis.R#L2-L106"
      }
    ],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0052_scan.json"
  },
  "error": null
}