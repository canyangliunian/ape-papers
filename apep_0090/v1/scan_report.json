{
  "paper_id": "apep_0090",
  "scan_date": "2026-02-06T12:44:41.793850+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 7,
  "flags": [
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "paper.tex",
      "lines": [
        44,
        45,
        129,
        130,
        398,
        399,
        440,
        441
      ],
      "evidence": "Key quantitative results (ATT=240, SE=65.6, p=0.999; plus TWFE numbers and CIs) are hard-coded into the manuscript rather than clearly being auto-generated (e.g., via \\input{../tables/table3_main_results.tex}). The repo does include code to generate LaTeX tables (06_tables.R), but paper.tex embeds numbers directly, which creates an integrity risk if manuscript values can drift from computed outputs without detection.: I find evidence of the opposite effect. ... experienced an increase of approximately 240 high-propensity business applications per month (11.1\\% ...), statistically significant at the 5\\% level. Pre-trend tests ... ($p = 0.999$).\n...\nThe main finding is surprising: ... approximately 240 additional high-propensity business applications per month ... (standard error of 65.6) ...\n...\nATT & 240$^{**}$ & 248$^{**}$ \\\\\n& (65.6) & (73.4) \\\\\n95\\% CI & [111, 368] & [103, 393] \\\\",
      "confidence": 0.74
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "01_fetch_data.R",
      "lines": [
        78,
        79,
        80,
        81,
        82,
        83,
        84,
        96,
        97
      ],
      "evidence": "The manuscript states the main outcome is BFS high-propensity applications with FRED naming convention \"HPBATOTALSAXX\" (Appendix 'Data Sources'). However, the code fetches series with prefixes \"BAHBATOTALSA\" and \"BABATOTALSA\" and constructs IDs as paste0(prefix, state_abbr, suffix). This appears inconsistent with the manuscript-described series IDs and may fetch the wrong series (or nothing), changing the outcome definition relative to what is claimed.: # Try total high-propensity first\nba_total <- fetch_fred_series(\"BAHBATOTALSA\")\n...\n# Also fetch total business applications (not just high-propensity)\nba_all <- fetch_fred_series(\"BABATOTALSA\")",
      "confidence": 0.84
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        13,
        14,
        15,
        16,
        17,
        18,
        19,
        20,
        21,
        22,
        23,
        24,
        25,
        26,
        27,
        28,
        29,
        30,
        31,
        32,
        33
      ],
      "evidence": "Privacy law effective dates and thresholds are manually encoded in-code. The manuscript says these are 'hand-collected' and cross-referenced with Bloomberg/NCSL/IAPP, but the code does not record citations/URLs per row, a scraping log, or a reproducible provenance artifact (e.g., raw source files + parsing). This does not imply fabrication, but it weakens traceability/auditability for the treatment definition.: privacy_laws <- tribble(\n  ~state, ~state_abbr, ~effective_date, ~threshold_consumers, ~notes,\n  \"California\", \"CA\", \"2020-01-01\", 50000, \"CCPA - first comprehensive state law\",\n  \"Virginia\", \"VA\", \"2023-01-01\", 100000, \"VCDPA - first of 2023 wave\",\n  ...\n  \"Minnesota\", \"MN\", \"2025-07-31\", 100000, \"MCDPA\"\n) %>%\n  mutate(effective_date = as.Date(effective_date))",
      "confidence": 0.7
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "04_robustness.R",
      "lines": [
        23,
        24,
        25,
        26,
        27,
        28
      ],
      "evidence": "Randomization (set.seed + sample) is used to create a placebo assignment. This is acceptable as a placebo/permutation-style robustness check and is labeled as such, but it is still a source of simulated structure in the codebase and should not be confused with real data construction.: # Create placebo by randomly assigning treatment dates\nset.seed(42)\n\nplacebo_sample <- analysis_sample %>%\n  mutate(\n    # Randomly permute treatment status across states\n    placebo_cohort = sample(cohort)\n  )",
      "confidence": 0.9
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "02_clean_data.R",
      "lines": [
        152,
        153,
        154,
        155,
        156,
        157
      ],
      "evidence": "The main analysis sample drops any state-month with missing outcomes. If outcome missingness is systematically related to treatment timing or state characteristics (e.g., if the wrong FRED series was fetched for some states), this could induce selection. The restriction is standard when missingness is benign, but combined with the potential series-ID mismatch in 01_fetch_data.R it becomes more concerning.: analysis_sample <- panel %>%\n  filter(\n    state_abbr != \"CA\",                    # Exclude CA (COVID confounded)\n    !is.na(business_apps),                 # Non-missing outcome\n    date >= \"2018-01-01\",                  # Start of pre-period\n    date <= \"2025-06-01\"                   # End of sample\n  )",
      "confidence": 0.62
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "LOW",
      "file": "06_tables.R",
      "lines": [
        82,
        83,
        84,
        85,
        86,
        87,
        88,
        89,
        90,
        91,
        92
      ],
      "evidence": "Table generation includes an explicit placeholder reusing the main ATT/SE for a 'C-S with covariates' row. If this row is later presented as a distinct specification, it would be a hard-coded/duplicated result rather than an estimated one. (Also, the CI computation references ATT/SE symbols that may not resolve as intended inside tibble construction, risking incorrect CIs.): if (file.exists(file.path(dir_data, \"did_results.RData\"))) {\n  load(file.path(dir_data, \"did_results.RData\"))\n\n  results_table <- tibble(\n    Specification = c(\n      \"(1) Callaway-Sant'Anna\",\n      \"(2) TWFE\",\n      \"(3) C-S with covariates\"\n    ),\n    ATT = c(\n      agg_simple$overall.att,\n      robustness$att[robustness$specification == \"TWFE (clustered SE)\"],\n      agg_simple$overall.att  # Placeholder\n    ),\n    SE = c(\n      agg_simple$overall.se,\n      robustness$se[robustness$specification == \"TWFE (clustered SE)\"],\n      agg_simple$overall.se  # Placeholder\n    ),\n    `95% CI` = paste0(\n      \"[\", round(ATT - 1.96 * SE, 2), \", \",\n      round(ATT + 1.96 * SE, 2), \"]\"\n    ),\n    ...\n  )\n}",
      "confidence": 0.67
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 1,
      "MEDIUM": 2,
      "LOW": 3
    },
    "one_liner": "method mismatch",
    "executive_summary": "In `01_fetch_data.R`, the data retrieval does not match the manuscript\u2019s stated primary outcome: the paper describes using BFS high\u2011propensity application series from FRED with the naming convention `HPBATOTALSAXX`, but the code instead pulls series with `BAHBAT...` prefixes. This discrepancy means the code is likely analyzing a different set of FRED series than the one documented, undermining the validity and reproducibility of the reported results.",
    "top_issues": [
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript states the main outcome is BFS high-propen...",
        "file": "01_fetch_data.R",
        "lines": [
          78,
          79
        ],
        "github_url": "/apep_0090/code/01_fetch_data.R#L78-L97"
      }
    ],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0090_scan.json"
  },
  "error": null
}