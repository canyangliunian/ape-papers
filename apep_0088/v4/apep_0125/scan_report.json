{
  "paper_id": "apep_0125",
  "scan_date": "2026-02-06T12:47:38.351363+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 13,
  "flags": [
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        120,
        145
      ],
      "evidence": "If the placebo-vote fetch fails, the script silently writes an empty tibble to placebo_votes_raw.rds. Downstream scripts (e.g., 04_robustness.R) will then run placebo logic on an empty dataset and may give the appearance that placebo tests were run but 'found nothing', when in fact the placebo data never loaded. This is a provenance/completeness risk for robustness checks (not the main outcome). Recommended: fail hard (stop) unless a cached non-empty file exists, or explicitly record a 'fetch_failed' flag and propagate it into tables/figures/logs.: placebo_votes_raw <- tryCatch({\n  ...\n  placebo_all <- get_nationalvotes(\n    from_date = \"2000-01-01\",\n    to_date = \"2009-12-31\",\n    geolevel = \"municipality\"\n  )\n  ...\n}, error = function(e) {\n  message(paste(\"   Error fetching placebo votes:\", e$message))\n  NULL\n})\n\n# Save placebo votes if successfully fetched\nif (!is.null(placebo_votes_raw) && nrow(placebo_votes_raw) > 0) {\n  saveRDS(placebo_votes_raw, file.path(data_dir, \"placebo_votes_raw.rds\"))\n  message(\"   Saved placebo_votes_raw.rds\")\n} else {\n  # Create empty placeholder to prevent downstream errors\n  placebo_votes_raw <- tibble()\n  saveRDS(placebo_votes_raw, file.path(data_dir, \"placebo_votes_raw.rds\"))\n  message(\"   Created empty placebo_votes_raw.rds (fetch failed)\")\n}",
      "confidence": 0.74
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "11_didisc_analysis.R",
      "lines": [
        188,
        206
      ],
      "evidence": "The manuscript defines DiDisc/TreatedPost using canton-specific in-force dates (staggered timing) and discusses careful timing (e.g., BL treated in 2016/2017; BS special handling). This code instead defines post purely as period == 'post' and sets treated_post = treated & post, which treats ALL treated cantons as treated in ALL post-period referendums regardless of whether the law was in force at that referendum. That is not the DiDisc/time-varying treatment described in paper.tex and can materially change estimates and inference.: panel_df <- panel_df %>%\n  mutate(\n    # For simplicity, use 2010 as the first treatment year (GR adopted)\n    # More sophisticated: use canton-specific adoption years\n    post = period == \"post\",\n    treated_post = treated & post,\n    year = year(votedate)\n  )",
      "confidence": 0.9
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "11_didisc_analysis.R",
      "lines": [
        269,
        318
      ],
      "evidence": "The manuscript reports wild cluster bootstrap inference for the preferred design (border-pair level / RDD context). Here, the bootstrap is applied to a simplified lm that omits the municipality fixed effects used in didisc_basic (feols with mun_id + vote_type FE). Omitting key fixed effects can change the coefficient and its sampling distribution, so this 'WCB p-value' is not clearly tied to the reported DiDisc specification. This is a mismatch between the inferential target and the bootstrap implementation.: # Prepare data for lm (fwildclusterboot works with lm objects)\npanel_for_wcb <- panel_near_border %>%\n  filter(!is.na(yes_share), !is.na(treated_post)) %>%\n  mutate(\n    mun_factor = as.factor(mun_id),\n    vote_factor = as.factor(vote_type),\n    treated_post_num = as.numeric(treated_post)\n  )\n\n# Simple model without absorbing FE (for WCB)\nwcb_model <- lm(yes_share ~ treated_post_num + vote_factor,\n                data = panel_for_wcb)\n\nwcb_result <- tryCatch({\n  boottest(\n    wcb_model,\n    param = \"treated_post_num\",\n    clustid = panel_for_wcb$canton_abbr,\n    B = 1000,\n    type = \"rademacher\"\n  )\n}, error = function(e) { ... })",
      "confidence": 0.78
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "MEDIUM",
      "file": "07_expanded_analysis.R",
      "lines": [
        238,
        255
      ],
      "evidence": "The paper\u2019s preferred RDD is 'same-language borders only' (German\u2013German borders), which conceptually requires restricting BORDER SEGMENTS where language does not change at the cutoff. Filtering municipalities by lang == 'German' does not ensure the relevant borders are German\u2013German (a German-language municipality could lie on a German\u2013French border on the BE side, etc.). This can contaminate the 'same-language' RDD with cross-language borders and bias the estimate. The manuscript explicitly emphasizes language discontinuities as a key threat, so this implementation detail matters.: # Specification 2: Same-language borders only (German-German borders)\n    same_lang_sample <- rdd_sample %>%\n      filter(lang == \"German\")",
      "confidence": 0.82
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        43
      ],
      "evidence": "A global seed is set. I did not see simulated outcome data generation, but a global seed can be relevant if any scripts perform randomization inference / permutations / bootstrap (the manuscript does). This is likely benign and for reproducibility; flagging only to confirm no pseudo-data are generated in the main analysis pipeline.: # Set seed for reproducibility\nset.seed(20260127)",
      "confidence": 0.55
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00b_verify_treatment.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "10_placebo_corrected.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "11_didisc_analysis.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "07_expanded_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "09_fix_rdd_sample.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "08_revision_fixes.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 1,
      "MEDIUM": 3,
      "LOW": 1
    },
    "one_liner": "method mismatch",
    "executive_summary": "The DiDisc setup in `11_didisc_analysis.R` does not implement the canton-specific \u201cin-force\u201d dates and staggered treatment timing described in the manuscript (e.g., Basel-Landschaft treated in 2016/2017 and special-case handling for Basel-Stadt). Instead, the treatment indicator appears to be constructed with a different timing rule (effectively collapsing or misaligning treatment timing across cantons), so the estimated DiDisc/treated-post effects do not correspond to the paper\u2019s stated identification strategy. This mismatch undermines the credibility of the reported causal estimates because units are treated as \u201cpost\u201d at the wrong dates relative to the policy rollout.",
    "top_issues": [
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript defines DiDisc/TreatedPost using canton-sp...",
        "file": "11_didisc_analysis.R",
        "lines": [
          188,
          206
        ],
        "github_url": "/apep_0125/code/11_didisc_analysis.R#L188-L206"
      }
    ],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0125_scan.json"
  },
  "error": null
}