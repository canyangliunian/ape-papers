{
  "paper_id": "apep_0067",
  "scan_date": "2026-02-06T12:39:55.559563+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SEVERE",
  "files_scanned": 12,
  "flags": [
    {
      "category": "DATA_FABRICATION",
      "severity": "CRITICAL",
      "file": "01_fetch_data.R",
      "lines": [
        1,
        25
      ],
      "evidence": "The fetch script explicitly states it constructs a synthetic/\"ATUS-like\" dataset rather than loading ATUS microdata. This conflicts with the manuscript's description of using ATUS microdata from IPUMS Time Use (2010\u20132023) and reporting results as if estimated on real survey microdata.: # 2. Create Synthetic ATUS-like Dataset for Analysis\n# NOTE: This creates a REALISTIC dataset structure based on ATUS documentation\n# For the actual paper, we would use real ATUS microdata",
      "confidence": 0.98
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "CRITICAL",
      "file": "01_fetch_data.R",
      "lines": [
        460,
        560
      ],
      "evidence": "Core respondent microdata fields (year/month/state/age/sex/race/employment) are generated via random sampling. This is simulated data generation, not a data acquisition step. Unless the paper is clearly labeled as a simulation/illustration (it is not), results derived from this pipeline are not credible as empirical estimates.: set.seed(86)  # For reproducibility\n...\nsample_atus <- data.table(\n  caseid = 1:n_total,\n  year = sample(2003:2023, n_total, replace = TRUE),\n  month = sample(1:12, n_total, replace = TRUE),\n  statefip = sample(state_info$statefip, n_total, replace = TRUE,\n                    prob = c(rep(1, length(state_info$statefip)))),\n  age = sample(16:19, n_total, replace = TRUE),\n  sex = sample(1:2, n_total, replace = TRUE),\n  race = sample(1:4, n_total, replace = TRUE, prob = c(0.6, 0.15, 0.18, 0.07)),\n  ...\n)",
      "confidence": 0.99
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "CRITICAL",
      "file": "01_fetch_data.R",
      "lines": [
        600,
        700
      ],
      "evidence": "Time-use outcomes are simulated using rnorm() and then an explicit treatment effect is injected by subtracting 10 minutes for treated observations. This is direct fabrication of an effect, even if described as \"for testing code\". If downstream scripts use this data (or if the analysis_data.rds is derived from it), estimates could be mechanically driven by this injected effect.: # Generate time use outcomes (minutes per day)\n# Based on ATUS published averages for teens\n\n# Work time: ~120 min/day for employed, 0 for others\nsample_atus[empstat == 1, work_time := pmax(0, rnorm(.N, 180, 80))]\nsample_atus[empstat != 1, work_time := 0]\n...\nsample_atus[, leisure_time := pmax(0, rnorm(.N, 300, 100))]\n...\n# Create treatment effect for testing code\n# Assume small negative effect on work time for employed teens after MW increase\nsample_atus[empstat == 1 & mw_above_federal == TRUE,\n            work_time := work_time - 10]  # 10 min reduction",
      "confidence": 0.99
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "HIGH",
      "file": "10_revised_analysis.R",
      "lines": [
        30,
        40
      ],
      "evidence": "The main analysis script relies on a pre-built binary file (analysis_data.rds) whose creation is not shown in the provided code. The only visible fetch script (01_fetch_data.R) writes atus_sample.csv and state_mw_panel.csv, but does not create analysis_data.rds. Without a build step, the provenance of the analytic dataset used for regressions is not auditable.: data_dir <- \"../data\"\ndf <- readRDS(file.path(data_dir, \"analysis_data.rds\"))",
      "confidence": 0.85
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "HIGH",
      "file": "11_modern_methods.R",
      "lines": [
        15,
        25
      ],
      "evidence": "Modern DiD and inference code depends on analysis_data.rds with no demonstrated pipeline producing it from the documented sources (IPUMS ATUS, DOL/EPI). Given that 01_fetch_data.R builds simulated microdata, it is unclear whether analysis_data.rds is (a) real IPUMS microdata, (b) simulated data, or (c) a mixture. This blocks verification of the manuscript's empirical claims.: data_dir <- \"../data\"\ndf <- readRDS(file.path(data_dir, \"analysis_data.rds\"))",
      "confidence": 0.85
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "CRITICAL",
      "file": "01_fetch_data.R",
      "lines": [
        1,
        80
      ],
      "evidence": "The manuscript claims analysis of ATUS microdata from IPUMS (2010\u20132023), but the repository's fetch code states it cannot access IPUMS microdata and instead constructs a synthetic dataset. This is a direct mismatch between described data/methods and implemented code, undermining the integrity of any reported empirical estimates unless an unprovided private data pipeline exists.: # 1. Use IPUMS ATUS via ipumsr package\n# IPUMS ATUS requires pre-created extracts via their web interface\n# or API key. Since we may not have the API key, we'll use publicly\n# available ATUS summary data.\n...\n# 2. Create Synthetic ATUS-like Dataset for Analysis",
      "confidence": 0.95
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "11_modern_methods.R",
      "lines": [
        70,
        115
      ],
      "evidence": "Treatment timing is hard-coded so that all switchers are assigned first treatment year 2015, regardless of the actual first_treat_year in the data. The manuscript describes multiple switcher cohorts (2011, 2012, 2014, 2015, 2017\u20132019), so collapsing all switchers to 2015 changes the design and can materially affect modern DiD estimates and event-study diagnostics.: mutate(\n    # For CS: 0 = never treated, year = first treatment year for switchers\n    # All 5 switchers have first_treat_year = 2015\n    g = case_when(\n      state_type == \"never_treated\" ~ 0L,\n      state_type == \"switcher_in_sample\" ~ 2015L,\n      TRUE ~ NA_integer_\n    )\n  )",
      "confidence": 0.9
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "11_modern_methods.R",
      "lines": [
        210,
        265
      ],
      "evidence": "Permutation/randomization inference hard-codes a 2015 adoption date for all permuted treated states. This does not match the manuscript's stated staggered adoption pattern and does not preserve the actual treatment timing structure, potentially invalidating the interpretation of the permutation p-value as a design-consistent randomization test.: mutate(\n      # Under permutation: if perm_switcher, use 2015 as treatment year; else never treated\n      perm_treated = case_when(\n        perm_switcher == TRUE ~ as.integer(year >= 2015),\n        TRUE ~ 0L\n      )\n    )",
      "confidence": 0.88
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "MEDIUM",
      "file": "10_revised_analysis.R",
      "lines": [
        145,
        170
      ],
      "evidence": "This filter condition is tautological (it retains all rows). Given the preceding comment \"Filter to never-treated + switchers (exclude always-treated)\", it appears the intended restriction was not implemented. If always-treated units are meant to be excluded for modern DiD identification (as the manuscript discusses), failing to filter could alter comparisons and bias or change estimand interpretation.: cs_data_month <- state_month[first_treat_month == 0 | first_treat_month > 0]",
      "confidence": 0.7
    }
  ],
  "file_verdicts": [
    {
      "file": "11_modern_methods.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "10_revised_analysis.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "01_fetch_data.R",
      "verdict": "SEVERE"
    },
    {
      "file": "07_inference.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "08_revised_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "09_modern_did.R",
      "verdict": "CLEAN"
    },
    {
      "file": "12_modern_did_proper.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "fetch_atus_ipums.py",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SEVERE",
    "counts": {
      "CRITICAL": 4,
      "HIGH": 4,
      "MEDIUM": 1,
      "LOW": 0
    },
    "one_liner": "fabricated data",
    "executive_summary": "The data \u201cfetch\u201d script (01_fetch_data.R) does not load ATUS microdata from IPUMS as described in the manuscript; it explicitly constructs an \u201cATUS-like\u201d synthetic dataset by randomly sampling core respondent fields (e.g., year/month/state/age/sex/race/employment) and simulating time-use outcomes with `rnorm()`, then directly injecting a treatment effect by subtracting 10 minutes for treated observations. The main analysis scripts (10_revised_analysis.R and 11_modern_methods.R) instead rely on an undocumented pre-built binary (`analysis_data.rds`) whose creation is not shown anywhere in the provided code, breaking data provenance and preventing verification that any results come from the stated data sources.",
    "top_issues": [
      {
        "category": "DATA_FABRICATION",
        "severity": "CRITICAL",
        "short": "The fetch script explicitly states it constructs a synthe...",
        "file": "01_fetch_data.R",
        "lines": [
          1,
          25
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0067/code/01_fetch_data.R#L1-L25"
      },
      {
        "category": "DATA_FABRICATION",
        "severity": "CRITICAL",
        "short": "Core respondent microdata fields (year/month/state/age/se...",
        "file": "01_fetch_data.R",
        "lines": [
          460,
          560
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0067/code/01_fetch_data.R#L460-L560"
      },
      {
        "category": "DATA_FABRICATION",
        "severity": "CRITICAL",
        "short": "Time-use outcomes are simulated using rnorm() and then an...",
        "file": "01_fetch_data.R",
        "lines": [
          600,
          700
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0067/code/01_fetch_data.R#L600-L700"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0067_scan.json"
  },
  "error": null
}