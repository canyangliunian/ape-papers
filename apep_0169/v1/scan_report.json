{
  "paper_id": "apep_0169",
  "scan_date": "2026-02-06T12:56:19.111547+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 7,
  "flags": [
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "03_main_analysis.R",
      "lines": [
        1,
        33,
        52,
        69,
        96,
        124
      ],
      "evidence": "The manuscript repeatedly describes a \"doubly robust\" / \"augmented inverse probability weighting (AIPW)\" estimator with cross-fitting and an explicit outcome model component. The implemented analysis is plain IPW weighting (WeightIt) followed by a weighted regression of Y on treatment only. There is no outcome regression model \u03bc\u03021(X), \u03bc\u03020(X), no augmentation term, and no cross-fitting. This is a substantive mismatch between claimed estimator and implemented estimator.: # Main causal analysis using Doubly Robust methods\n...\n# Method 1: IPW with propensity score weighting\n...\nps_model <- weightit(\n  ps_formula,\n  data = cps_complete,\n  method = \"ps\",\n  estimand = \"ATE\"\n)\n...\nipw_earnings <- lm(log_earnings ~ multiple_jobs,\n                   data = cps_complete,\n                   weights = ipw)",
      "confidence": 0.92
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "06_tables.R",
      "lines": [
        86,
        122,
        141
      ],
      "evidence": "Table-generation code explicitly labels results as coming from \"AIPW with 5-fold cross-fitting\" and \"influence-function based\" standard errors, but upstream code (03_main_analysis.R) does not implement AIPW, cross-fitting, or IF-based SEs; it uses lm() with weights and conventional vcov(). This mislabeling can mislead readers about the estimator and inference actually used.: \\item \\textit{Notes:} Estimates from augmented inverse probability weighting (AIPW) with 5-fold cross-fitting. ... Standard errors are influence-function based.\n...\nwriteLines(table2_latex, file.path(table_dir, \"table2_main_results.tex\"))",
      "confidence": 0.9
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "05_figures.R",
      "lines": [
        15,
        27,
        40,
        114
      ],
      "evidence": "Figures are labeled as CPS ASEC 2015\u20132024 and as \"Doubly robust AIPW\" estimates, but the data pipeline actually fetches ACS PUMS via Census API (01_fetch_data.R) and the estimation is IPW+WLS (03_main_analysis.R). This is a repeated mismatch between what outputs claim and what code does, raising integrity and reproducibility concerns.: labs(\n    title = \"Multiple Job Holding Rates in the United States, 2015-2024\",\n    ...\n    caption = \"Source: Current Population Survey ASEC. ...\"\n  )\n...\n  labs(\n    title = \"Effect of Holding Multiple Jobs on Labor Market Outcomes\",\n    subtitle = \"Doubly robust AIPW estimates with 95% confidence intervals\",\n    ...\n  )",
      "confidence": 0.88
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "paper.tex",
      "lines": [
        140,
        214,
        259,
        835
      ],
      "evidence": "The manuscript states the data source is IPUMS USA ACS PUMS (harmonized extract) for 2019\u20132022, and provides a detailed sample-construction flow with specific counts. The provided code does not download IPUMS extracts; it queries the Census ACS1 PUMS API. This is a different provenance channel (and typically different universe/structure/weighting conventions than an IPUMS extract). Unless the repository also contains an unshared IPUMS download/import script, the manuscript\u2019s provenance description is not supported by the code shown.: I use data from the American Community Survey (ACS) Public Use Microdata Sample (PUMS), accessed through IPUMS ...\n...\nMy analysis uses ACS PUMS data from 2019-2022 ...\n...\nAfter applying these restrictions, the analysis sample contains approximately 1.4 million observations\n...\nInitial extract ... from ACS PUMS 2019-2022 ... Initial extract: N = 4,823,456.",
      "confidence": 0.84
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "01_fetch_data.R",
      "lines": [
        74,
        77,
        80
      ],
      "evidence": "The manuscript\u2019s analysis window is 2019\u20132022 (explicitly including 2020 and 2021 as \"COVID period\" years). The fetch script skips 2020 entirely, so the implemented dataset cannot match the manuscript\u2019s stated time coverage and COVID-period definition. This affects sample size, covariate distributions, and any stated robustness checks about excluding COVID years.: # Fetch data for years 2019-2022 (recent, pre-COVID to post-COVID)\nyears <- c(2019, 2021, 2022)  # Skip 2020 (ACS issues)",
      "confidence": 0.95
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "paper.tex",
      "lines": [
        31,
        275,
        515,
        706
      ],
      "evidence": "Key coefficients/SEs/p-values and even intermediate robustness results are embedded as literal numbers in the LaTeX tables/text. The codebase does generate tables (06_tables.R), but those tables are for a different labeled project (multiple job holding/CPS wording) and do not appear to be used to populate the manuscript\u2019s ACS self-employment tables. Without an explicit LaTeX table export pipeline that writes these exact numbers into paper.tex (or included \\input{} files), these results are effectively hard-coded in the manuscript and not verifiably produced by the provided code.: I find that self-employed workers earn 5.77 log points less ... (standard error 0.004), with p-value less than 0.001.\n...\nSelf-Employed & -0.0577*** & -0.160*** & -1.62*** \\\\\n              & (0.004) & (0.003) & (0.12) \\\\\n...\nSelf-Employed & -0.0631*** & -0.0506*** \\\\\n              & (0.005) & (0.006) \\\\\n...\nTrim at 10\\% & -0.0554 & 0.004 & 1,257,844",
      "confidence": 0.78
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "06_tables.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "05_figures.R",
      "verdict": "SUSPICIOUS"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 4,
      "MEDIUM": 2,
      "LOW": 0
    },
    "one_liner": "method mismatch",
    "executive_summary": "The code does not implement the paper\u2019s stated \u201cdoubly robust\u201d AIPW estimator with cross-fitting and an explicit outcome model; instead, the main analysis appears to rely on IPW combined with WLS, while downstream table/figure scripts nonetheless label outputs as \u201cAIPW with 5-fold cross-fitting\u201d and \u201cinfluence-function\u201d standard errors. The data pipeline also contradicts the manuscript: figures are labeled as CPS ASEC 2015\u20132024, but the code fetches ACS PUMS via the Census API, and the intended 2019\u20132022 window (including COVID-period 2020\u20132021) cannot be reproduced because 2020 is skipped entirely in the fetch step.",
    "top_issues": [
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript repeatedly describes a \"doubly robust\" / \"...",
        "file": "03_main_analysis.R",
        "lines": [
          1,
          33
        ],
        "github_url": "/apep_0169/code/03_main_analysis.R#L1-L124"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "Table-generation code explicitly labels results as coming...",
        "file": "06_tables.R",
        "lines": [
          86,
          122
        ],
        "github_url": "/apep_0169/code/06_tables.R#L86-L141"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "Figures are labeled as CPS ASEC 2015\u20132024 and as \"Doubly ...",
        "file": "05_figures.R",
        "lines": [
          15,
          27
        ],
        "github_url": "/apep_0169/code/05_figures.R#L15-L114"
      }
    ],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0169_scan.json"
  },
  "error": null
}