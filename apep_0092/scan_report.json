{
  "paper_id": "apep_0092",
  "scan_date": "2026-02-06T12:45:03.122651+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 11,
  "flags": [
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "HIGH",
      "file": "09_fix_rdd_sample.R",
      "lines": [
        35,
        38
      ],
      "evidence": "The script requires a local file `data/gemeinde_spatial.rds`, but the data-fetch script (01_fetch_data.R) saves `gemeinde_boundaries.rds` and `voting_sf.rds` (and optionally `canton_boundaries.rds`). `gemeinde_spatial.rds` is not created in the provided codebase, creating a key reproducibility/provenance gap for the corrected RDD sample construction that the manuscript emphasizes. This is a serious integrity risk because the corrected sample can change the main estimate materially, and the pipeline to construct it is not fully auditable from the supplied scripts.: voting_data <- readRDS(file.path(data_dir, \"voting_data.rds\"))\ncanton_treatment <- readRDS(file.path(data_dir, \"canton_treatment.rds\"))\ngemeinde_sf <- readRDS(file.path(data_dir, \"gemeinde_spatial.rds\"))",
      "confidence": 0.86
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        86,
        121
      ],
      "evidence": "Treatment assignment/timing is hard-coded in code rather than constructed from a raw legal/source file or scraped database. The manuscript does document LexFind verification and in-force dates, which makes this more defensible, but for an integrity audit this remains a vulnerability: (i) it is easy to alter the treatment list/dates without leaving traces, and (ii) there is no automated cross-check against an external source within the pipeline.: canton_treatment <- tribble(\n  ~canton_id, ~canton_abbr, ~adoption_year, ~entry_force_date, ~treated,\n  # Treated (energy law in force before May 2017)\n  18, \"GR\", 2010, \"2011-01-01\", TRUE,\n  2,  \"BE\", 2011, \"2012-01-01\", TRUE,\n  19, \"AG\", 2012, \"2013-01-01\", TRUE,\n  13, \"BL\", 2016, \"2016-07-01\", TRUE,\n  12, \"BS\", 2016, \"2017-01-01\", TRUE,\n  # Control cantons ...\n)",
      "confidence": 0.78
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "LOW",
      "file": "07_expanded_analysis.R",
      "lines": [
        640,
        655
      ],
      "evidence": "In the Callaway-Sant'Anna DiD section, treatment timing is hard-coded. This is consistent with the paper's design and the treatment crosswalk in the appendix, so it is not inherently suspicious, but it should ideally be derived from the same `canton_treatment` table (single source of truth) to prevent inconsistencies across analyses.: first_treat = case_when(\n  canton == \"GR\" ~ 2010,\n  canton == \"BE\" ~ 2011,\n  canton == \"AG\" ~ 2012,\n  canton == \"BL\" ~ 2016,\n  canton == \"BS\" ~ 2017,\n  TRUE ~ 0\n)",
      "confidence": 0.74
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "07_expanded_analysis.R",
      "lines": [
        135,
        210
      ],
      "evidence": "The manuscript's preferred design is the *corrected* running variable: distance to each municipality's *own canton's treated-control border*, and restricting the sample to municipalities in cantons that directly share a treated-control border (and with additional adjacency restrictions). This script instead computes distance to a pooled union of treated-control borders (`get_policy_border`) and signs by treated status, which is the pre-correction/union-boundary approach the manuscript explicitly distinguishes from the corrected sample. If results/figures from this script feed the paper's main numbers, there is a risk that the reported 'corrected sample construction' estimate is not actually what was computed here.: border <- get_policy_border(canton_borders, treated_canton_ids, canton_id_col = \"canton_id\")\n...\nvoting_sf_clean <- voting_sf_clean %>%\n  mutate(\n    dist_to_border = as.numeric(st_distance(centroid, border)) / 1000,\n    distance_to_border = ifelse(treated, dist_to_border, -dist_to_border)\n  )",
      "confidence": 0.81
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "07_expanded_analysis.R",
      "lines": [
        235,
        265
      ],
      "evidence": "The paper's 'same-language borders' restriction is conceptually about borders where language does not change at the cutoff (German\u2013German border segments), not simply restricting to municipalities whose canton-majority language is German. `filter(lang == \"German\")` keeps all German-canton municipalities, including those on borders with French/Italian cantons (e.g., BE\u2013FR, GR\u2013TI) if they remain in the sample. Without explicit border-pair filtering, this may not implement the same-language border design as described in the manuscript.: same_lang_sample <- rdd_sample %>%\n  filter(lang == \"German\")\n\nrdd2 <- rdrobust(\n  y = same_lang_sample$yes_share,\n  x = same_lang_sample$distance_to_border,\n  c = 0\n)",
      "confidence": 0.77
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "07_expanded_analysis.R",
      "lines": [
        55,
        70
      ],
      "evidence": "The constructed 'urban_share' is actually a binary indicator based on eligible voters > 5000, and it is derived from the same referendum dataset rather than an external urbanization measure. This is not necessarily manipulative, but it is a nonstandard proxy that could be outcome-adjacent (eligible voters is closely related to municipality size and potentially political composition). If used as a covariate balance test or mechanism, it should be clearly labeled as a proxy and ideally replaced with an external urban/rural classification or population from BFS.: gemeinde_data <- voting_data %>%\n  ...\n  mutate(\n    treated = as.numeric(treated),\n    log_pop = log(voters + 1),\n    urban_share = ifelse(voters > 5000, 1, 0)\n  )",
      "confidence": 0.73
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        38,
        40
      ],
      "evidence": "A global random seed is set. On its own this is not evidence of fabrication; however, the project does run randomization inference/permutations elsewhere, so the seed affects inferential diagnostics. This is acceptable if disclosed, but it increases the importance of ensuring no simulated outcomes are used for main results (the provided code comments indicate a prior simulated pretrend was replaced with real data).: # Set seed for reproducibility\nset.seed(20260127)",
      "confidence": 0.66
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "LOW",
      "file": "07_expanded_analysis.R",
      "lines": [
        560,
        610
      ],
      "evidence": "The script conditionally runs the Callaway\u2013Sant'Anna estimator if the `did` package is available; otherwise it silently substitutes a TWFE estimate but still writes it to `callaway_santanna.csv`. This can lead to selective/accidental reporting depending on the local environment (different machines yield different 'C&S' outputs). For integrity, the pipeline should hard-fail if the package is missing, or output clearly labeled separate files for TWFE vs C&S.: has_did <- requireNamespace(\"did\", quietly = TRUE)\n...\nif (has_did) {\n  ... att_gt(...)\n} else {\n  # Create placeholder result based on TWFE\n  cs_summary <- tibble(\n    estimator = \"TWFE (standard)\",\n    att = coef(did_model)[1],\n    se = sqrt(vcov(did_model)[1,1]),\n    ...\n  )\n}",
      "confidence": 0.76
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "10_placebo_corrected.R",
      "lines": [
        40,
        90
      ],
      "evidence": "The placebo script expects placebo vote_types like `immigration_2016`, `usr_iii_2017`, etc. But `01_fetch_data.R` only creates `energy_2017`, `nuclear_2016`, and `green_2016` (and does not fetch the placebo referendums listed in the manuscript appendix). As written, the placebo analysis will often produce an empty dataset and then fall back to re-estimating the *main* energy 2017 RDD 'for demonstration'. This undermines the manuscript's claims about placebo tests unless there is another (missing) data-fetch step that populates those vote types.: voting_data <- readRDS(file.path(data_dir, \"voting_data.rds\"))\n...\nplacebo_votes <- c(\"immigration_2016\", \"durchsetzung_2016\",\n                   \"corporate_tax_2017\", \"usr_iii_2017\")\n\nplacebo_data <- voting_data %>%\n  filter(vote_type %in% placebo_votes)",
      "confidence": 0.84
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "10_placebo_corrected.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "07_expanded_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "09_fix_rdd_sample.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "08_revision_fixes.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 1,
      "MEDIUM": 4,
      "LOW": 4
    },
    "one_liner": "unclear provenance",
    "executive_summary": "The analysis script `09_fix_rdd_sample.R` depends on a local file `data/gemeinde_spatial.rds`, but the project\u2019s data retrieval pipeline (`01_fetch_data.R`) never produces this file\u2014instead it saves `gemeinde_boundaries.rds` and `voting_sf.rds` (and optionally `canton_boundaries.rds`). As written, the workflow cannot be reproduced from the provided code and will fail or rely on undocumented, externally prepared data, leaving the provenance of the key spatial dataset unclear.",
    "top_issues": [
      {
        "category": "DATA_PROVENANCE_MISSING",
        "severity": "HIGH",
        "short": "The script requires a local file `data/gemeinde_spatial.r...",
        "file": "09_fix_rdd_sample.R",
        "lines": [
          35,
          38
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0092/code/09_fix_rdd_sample.R#L35-L38"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0092_scan.json"
  },
  "error": null
}