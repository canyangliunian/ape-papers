{
  "paper_id": "apep_0079",
  "scan_date": "2026-02-06T12:42:26.657674+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 8,
  "flags": [
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "paper.tex",
      "lines": [
        420,
        525,
        566,
        610
      ],
      "evidence": "Key regression outputs (point estimates, SEs, CIs, p-values) are hard-coded directly in LaTeX tables rather than being programmatically generated from saved model objects. The code does write CSVs (e.g., output/paper_106/tables/main_results.csv, robustness_results.csv) and saves RDS model objects, but there is no script shown that converts these outputs into the LaTeX tables. This creates a transcription-risk channel (intentional or accidental) because the manuscript tables are not mechanically tied to the computed results.: \\begin{table}[H]\n\\centering\n\\caption{TWFE Estimates (Demonstration of Design Failure)}\n...\nTreated & 0.047** & 0.021** \\\\\n & (0.020) & (0.010) \\\\\n & {[}0.009, 0.085{]} & {[}0.002, 0.040{]} \\\\\n...\n\\end{table}\n\n\\begin{table}[H]\n\\centering\n\\caption{Callaway-Sant'Anna Estimates (2023 Cohort Only)}\n...\n2023 & 2023 & 0.046* & (0.023) \\\\\n2023 & 2024 & 0.057* & (0.024) \\\\\n...\n\\multicolumn{2}{l}{Aggregated ATT} & 0.052** & (0.024) \\\\\n\\multicolumn{2}{l}{95\\% CI} & \\multicolumn{2}{c}{[0.005, 0.099]} \\\\\n...\n\\end{table}\n\n\\begin{table}[H]\n\\centering\n\\caption{Triple-Difference Estimates (State$\\times$Year Fixed Effects)}\n...\nHas Children $\\times$ Treated & $-0.008$ \\\\\n & (0.013) \\\\\n & {[}$-0.034$, 0.018{]} \\\\\n...\n\\end{table}\n\n\\begin{table}[H]\n\\centering\n\\caption{Summary of Main Estimates}\n...\nTWFE (cluster-robust SE) & 0.047 & 0.017 & {[}0.009, 0.085{]} \\\\\nTWFE (randomization inference) & 0.047 & 0.015 & --- \\\\\nDDD (state$\\times$year FE) & $-0.008$ & 0.547 & {[}$-0.034$, 0.018{]} \\\\\n...\n\\end{table}",
      "confidence": 0.86
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "01b_fetch_ers_data.R",
      "lines": [
        65,
        160
      ],
      "evidence": "State-level ERS food insecurity rates are manually entered as literals in the code. If these numbers are used for analysis/claims (e.g., pre-trends/baselines), they should be directly read from the downloaded ERS file or accompanied by a transparent citation-to-cell mapping (sheet/range) to eliminate transcription risk. The script does download the ERS Excel/ZIP, but then bypasses it via a hard-coded table.: ers_state_rates <- tribble(\n  ~state_name, ~state_abbr, ~period, ~fi_rate, ~vlfs_rate,\n  # 2015-2017 averages (pre-treatment baseline)\n  \"California\", \"CA\", \"2015-17\", 11.0, 4.0,\n  ...\n  \"New Mexico\", \"NM\", \"2021-23\", 16.4, 6.6\n)\n\nnational_rates <- tribble(\n  ~period, ~fi_rate_national, ~vlfs_rate_national,\n  \"2015-17\", 12.5, 4.9,\n  \"2018-20\", 10.9, 4.0,\n  \"2021-23\", 12.8, 5.1\n)",
      "confidence": 0.78
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "07_simulation.R",
      "lines": [
        1,
        220
      ],
      "evidence": "This file simulates data using random draws (rnorm/rbinom) and produces Monte Carlo figures. In the manuscript, Section 2 explicitly describes Monte Carlo simulations calibrated to CPS-FSS and presents a simulation figure, so the synthetic data generation appears appropriately labeled and methodologically aligned. Not an integrity concern by itself.: set.seed(20260128)\n...\ntrend = rnorm(n_states, mean = 0, sd = 0.005)\n...\nshock = c(0, cumsum(rnorm(n_years - 1, 0, year_shock_sd)))\n...\nfood_insecure = rbinom(n(), 1, prob_insecure)",
      "confidence": 0.93
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        35,
        120
      ],
      "evidence": "The pipeline downloads CPS-FSS from NBER (2015\u20132021) and Census (2022\u20132024), which is good provenance. However, the cleaning script (02_clean_data.R) does not parse the downloaded NBER .dat files, meaning those years are not actually incorporated despite being fetched. This is more of a reproducibility/completeness issue than an integrity red flag, and the manuscript states the main analysis focuses on 2022\u20132024.: nber_urls <- list(\n  \"2015\" = \"http://data.nber.org/cps/cpsdec2015.zip\",\n  ...\n)\n...\ncensus_urls <- list(\n  \"2022\" = \"https://www2.census.gov/programs-surveys/cps/datasets/2022/supp/dec22pub.csv\",\n  ...\n)\n...\ndownload.file(url, dest_file, mode = \"wb\", quiet = TRUE)",
      "confidence": 0.7
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "LOW",
      "file": "02_clean_data.R",
      "lines": [
        145,
        190
      ],
      "evidence": "Comments indicate a 'simulated pre-treatment baseline' for code testing, but no such simulation is actually implemented below (the script proceeds with 2022\u20132024 CSVs). The manuscript\u2019s narrative is consistent with using 2022\u20132024 and emphasizing limitations, so this is not a substantive mismatch\u2014just potentially confusing commentary that could alarm readers/auditors.: # Create a simulated pre-treatment baseline using 2022 control states\n# This is for code testing - real analysis needs full time series\n\ncat(\"\\nNote: Full analysis requires parsing 2015-2021 .dat files\\n\")\ncat(\"Proceeding with 2022-2024 data for pipeline testing\\n\")",
      "confidence": 0.66
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "01b_fetch_ers_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "07_simulation.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "paper.tex",
      "verdict": "SUSPICIOUS"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 1,
      "MEDIUM": 1,
      "LOW": 3
    },
    "one_liner": "hard-coded results",
    "executive_summary": "The LaTeX source (`paper.tex`) hard-codes key regression results\u2014point estimates, standard errors, confidence intervals, and p-values\u2014directly into tables instead of generating them programmatically from saved model objects. Although the workflow writes results to CSVs, the manuscript tables are not clearly linked to those outputs, creating a high risk of transcription errors, inconsistencies with the underlying models, and results that cannot be reliably reproduced end-to-end.",
    "top_issues": [
      {
        "category": "HARD_CODED_RESULTS",
        "severity": "HIGH",
        "short": "Key regression outputs (point estimates, SEs, CIs, p-valu...",
        "file": "paper.tex",
        "lines": [
          420,
          525
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0079/code/paper.tex#L420-L610"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0079_scan.json"
  },
  "error": null
}