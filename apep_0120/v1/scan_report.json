{
  "paper_id": "apep_0120",
  "scan_date": "2026-02-06T12:46:44.842106+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 8,
  "flags": [
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "01_fetch_data.R",
      "lines": [
        109,
        149
      ],
      "evidence": "The manuscript states the analysis uses a balanced panel over 2015\u20132019 and 2021\u20132022 (excluding 2020 because ACS 1-year estimates were not released). However, the data-fetching code requests ACS 1-year data for 2010\u20132022 and then binds whatever is returned, without filtering to 2015+ and without enforcing a 2015\u20132019, 2021\u20132022 sample. Unless an (unseen) downstream script filters years, the estimation sample could include 2010\u20132014 and could be unbalanced depending on which years fail to download (notably 2020). This is a serious integrity risk because it can change cohort definitions, treated/control composition, and ATT estimates relative to what the paper claims.: acs_years <- 2010:2022\n...\nfor (yr in acs_years) {\n  url <- sprintf(\n    \"https://api.census.gov/data/%d/acs/acs1?get=%s&for=state:*\",\n    yr, var_string\n  )\n  raw <- fetch_json(url)\n  df  <- census_to_df(raw)\n  if (!is.null(df)) {\n    df$year <- yr\n    acs_list[[as.character(yr)]] <- df\n  }\n}\n...\nacs_panel <- acs_raw %>% ...",
      "confidence": 0.78
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        226,
        238,
        262
      ],
      "evidence": "Minimum-wage histories are embedded as extensive hard-coded literals rather than being programmatically sourced from the cited DOL/NCSL/EPI sources or a version-controlled raw-data file. The manuscript says these come from DOL historical tables (with supplementation), which makes hard-coding potentially acceptable, but without a machine-readable provenance (e.g., downloaded source file, scrape script, or citation-to-row mapping), it is difficult to audit correctness or rule out transcription errors/selection. This is especially important because treatment cohorts (first_treat) are derived directly from these literals.: # --- Part B: Building state minimum wage panel ---\n...\n# Create base panel: 51 entities x 13 years\nmw_base <- expand.grid(\n  state_fips = all_fips,\n  year       = 2010:2022,\n  stringsAsFactors = FALSE\n) %>%\n  mutate(state_mw = federal_mw)  # default: all states start at federal\n...\n# --- States with MW above federal (alphabetical by FIPS) ---\n# Alaska (02): ...\nmw_base <- set_mw(mw_base, \"02\", list(\n  \"2010\" = 7.75, ... \"2022\" = 10.34\n))\n# Arizona (04): ...\n# ... (many more hard-coded schedules) ...",
      "confidence": 0.74
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        238,
        424
      ],
      "evidence": "Not hard-coded regression outputs, but the key treatment variable (treated_yr/first_treat) is produced from hard-coded minimum-wage values. Given the manuscript explicitly describes using DOL historical tables and related sources, this is not inherently improper; however, it increases the risk of undetected transcription mistakes affecting reported treatment timing and estimates.: mw_base <- expand.grid(\n  state_fips = all_fips,\n  year       = 2010:2022,\n  stringsAsFactors = FALSE\n) %>%\n  mutate(state_mw = federal_mw)  # default: all states start at federal\n...\n# Alaska (02): ...\nmw_base <- set_mw(mw_base, \"02\", list(\n  \"2010\" = 7.75, ... \"2022\" = 10.34\n))\n...\n# --- Finalize MW panel ---\nmw_panel <- mw_base %>%\n  mutate(\n    ...\n    treated_yr   = as.integer(mw_gap >= 1.00)\n  )",
      "confidence": 0.66
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "00_packages.R",
      "lines": [
        44,
        60
      ],
      "evidence": "A global RNG seed is set for the entire pipeline. The manuscript states CS-DiD standard errors are obtained via clustered bootstrap (999 reps). Setting a seed is good practice for replicability, but it also means that if any upstream code (not shown here) uses RNG for steps beyond inference (e.g., ad hoc sampling, imputation), those steps would be deterministic and potentially hard to detect. No direct fabrication is seen in the provided files, but given the seed is set globally, auditors should confirm random-number usage is limited to stated procedures (bootstrap) and not used in data construction.: set.seed(20240101)                             # Reproducibility\n...\nPAPER_DIR <- here::here()",
      "confidence": 0.55
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "04_robustness.R",
      "lines": [
        105,
        128
      ],
      "evidence": "Printed output says the threshold is strictly greater than ($>$) the dollar value, but the actual treatment definition uses >= (mw_gap >= thresh). This is likely a documentation/print-string bug rather than a substantive problem, but it can create confusion about the precise treatment rule reported in logs and potentially in the manuscript if transcribed.: cat(sprintf(\"  Threshold: MW gap > $%.2f\\n\", thresh))\n...\nmutate(\n  treated_alt   = as.integer(mw_gap >= thresh),\n  first_treat_alt = {\n    treat_years <- year[mw_gap >= thresh]\n    if (length(treat_years) > 0) min(treat_years) else 0L\n  }\n)",
      "confidence": 0.7
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "07_revision_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 1,
      "MEDIUM": 3,
      "LOW": 1
    },
    "one_liner": "method mismatch",
    "executive_summary": "The data-fetching script in `01_fetch_data.R` requests ACS 1\u2011year estimates in a way that conflicts with the manuscript\u2019s stated design of a balanced panel covering 2015\u20132019 and 2021\u20132022 while explicitly excluding 2020 because ACS 1\u2011year estimates were not released. As written, the code appears to pull ACS 1\u2011year data for years that would break that panel definition (notably around 2020), making the implemented dataset inconsistent with the paper\u2019s described methodology and potentially altering which observations enter the analysis.",
    "top_issues": [
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript states the analysis uses a balanced panel ...",
        "file": "01_fetch_data.R",
        "lines": [
          109,
          149
        ],
        "github_url": "/apep_0120/code/01_fetch_data.R#L109-L149"
      }
    ],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0120_scan.json"
  },
  "error": null
}