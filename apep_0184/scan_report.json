{
  "paper_id": "apep_0184",
  "scan_date": "2026-02-06T12:59:48.356973+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 8,
  "flags": [
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "03_main_analysis.R",
      "lines": [
        206,
        207,
        208
      ],
      "evidence": "Random number generation is used, but it is for Monte Carlo uncertainty quantification (sampling published point estimates using reported SEs), which the manuscript explicitly describes as the inference approach. Not evidence of fabricated primary data.: set.seed(20231215)\nn_sims <- 10000\n\n# Draw from distributions of treatment effects\nconsumption_draws <- rnorm(n_sims, consumption_effect, consumption_se)\nearnings_draws <- rnorm(n_sims, earnings_effect, earnings_se)\nspillover_draws <- rnorm(n_sims, spillover_consumption, spillover_consumption_se)",
      "confidence": 0.88
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        49,
        50,
        51,
        52,
        55,
        56,
        59,
        60
      ],
      "evidence": "Key effect sizes/SEs/p-values are hard-coded as literals. In context this appears intentional transcription of published estimates (the manuscript explicitly states calibration from published studies), so this is not inherently an integrity problem. However, it increases transcription-risk and should ideally be backed by a documented extraction process (e.g., parsing replication tables or linking to exact table cells/pages).: haushofer_shapiro_effects <- tibble::tribble(\n  ~outcome, ~control_mean, ~treatment_effect, ~se, ~pvalue, ~n_obs,\n  \"Total consumption\", 158, 35, 8, 0.001, 1372,\n  \"Food consumption\", 92, 20, 5, 0.001, 1372,\n  \"Non-food consumption\", 66, 15, 4, 0.001, 1372,\n  ...\n  \"Non-agricultural revenue\", 48, 17, 7, 0.02, 1372,\n  ...\n)",
      "confidence": 0.8
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "06_tables.R",
      "lines": [
        208,
        209,
        210,
        211,
        212,
        213,
        214,
        215
      ],
      "evidence": "Sensitivity-analysis MVPF values (and percent changes) are hard-coded rather than computed from the robustness objects. This creates a concrete risk that the table can diverge from actual robustness computations (or be edited to preferred values) without detection. If `robustness_results.RData` exists, the table still ignores it and uses fixed numbers.: sensitivity_summary <- tibble(\n  parameter = c(\"Baseline\", \"VAT coverage 25%\", \"VAT coverage 100%\",\n                \"Formality rate 10%\", \"Formality rate 30%\",\n                \"Discount rate 3%\", \"Discount rate 10%\",\n                \"Persistence 2 years\", \"Persistence 5 years\"),\n  mvpf = c(baseline_mvpf, 0.87, 0.90, 0.86, 0.89, 0.88, 0.86, 0.86, 0.89),\n  change_from_baseline = c(0, 0, 3.4, -1.1, 2.3, 1.1, -1.1, -1.1, 2.3)\n)",
      "confidence": 0.93
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        1,
        2,
        3,
        20,
        21,
        22
      ],
      "evidence": "No automated provenance trail from the cited sources (Dataverse/Econometric Society supplements) to the hard-coded effect-size tables. Given the manuscript's stated approach (calibration from published estimates), this is not missing data in the usual sense, but it does mean reproducibility depends on manual transcription accuracy. Consider adding: (i) explicit table/page references for each row, and/or (ii) a script that downloads replication materials and extracts these values.: # This script compiles PUBLISHED treatment effect estimates from peer-reviewed\n# studies.\n...\n# All values below are transcribed from the original publications:",
      "confidence": 0.78
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "03_main_analysis.R",
      "lines": [
        44,
        45,
        46,
        47,
        48,
        49,
        50,
        51,
        52,
        53,
        54,
        55,
        56,
        57,
        58,
        59
      ],
      "evidence": "Manuscript claims uncertainty for fiscal parameters is propagated via beta distributions (VAT coverage, informality share, admin cost), but the code holds these parameters fixed in `params` and only randomizes treatment effects with `rnorm()`. This is a substantive mismatch: reported CIs/variance decompositions in the manuscript may not correspond to the implemented simulation unless those manuscript results come from different (unprovided) code.: params <- list(\n  ...\n  vat_coverage = 0.50,\n  informal_share = 0.80,\n  admin_cost_rate = 0.15,\n  ...\n  discount_rate = 0.05,\n  consumption_persist_years = 3,\n  consumption_decay = 0.50,\n  earnings_persist_years = 5,\n  earnings_decay = 0.25,\n  ...\n)",
      "confidence": 0.9
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "03_main_analysis.R",
      "lines": [
        146,
        147,
        148,
        149,
        150,
        151
      ],
      "evidence": "Spillover scaling is hard-coded to 0.5 (non-recipients per recipient) inside the core calculation. The manuscript describes the ratio as approximately 0.5 but also discusses saturation design details; in `02_clean_data.R` the ratio is computed as `(1-high_sat)/high_sat` (=0.5) but not used in the main analysis function. This is more a transparency/reproducibility issue than fabrication, but the ratio should be a named parameter and documented/derived in one place.: if (include_spillovers) {\n    spillover_usd <- spillover_consumption / params$ppp_factor\n    # Adjust for number of non-recipients relative to recipients\n    # In Egger et al., ratio is approximately 0.5 in high-saturation villages\n    spillover_wtp <- spillover_usd * 12 * 0.5 *\n      calc_pv_factor(params$consumption_persist_years,\n                     params$consumption_decay,\n                     params$discount_rate)\n    wtp_total <- wtp_direct + spillover_wtp\n  }",
      "confidence": 0.76
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "02_clean_data.R",
      "lines": [
        86,
        87,
        88,
        89,
        90,
        91
      ],
      "evidence": "The manuscript states fiscal externalities in the denominator are based on recipient treatment effects from Haushofer & Shapiro (2016) (to avoid overlap/double counting with GE spillovers), but `02_clean_data.R` uses Egger et al. recipient consumption and wage effects to compute VAT and income-tax externalities. This changes the calibration and contradicts the manuscript\u2019s stated accounting separation. (Note: `03_main_analysis.R` uses Haushofer for consumption/earnings effects, so the pipeline is internally inconsistent across scripts.): # 1. VAT on consumption increase\n# Recipients increase consumption by $293 PPP annually (Egger et al.)\n...\nconsumption_gain_ppp <- egger_ge_effects %>%\n  filter(outcome == \"Consumption\") %>%\n  pull(recipient_effect)",
      "confidence": 0.92
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "04_heterogeneity.R",
      "lines": [
        33,
        34
      ],
      "evidence": "Heterogeneity analysis seeds subgroup calculations from hard-coded base effects rather than pulling from `haushofer_shapiro_effects`. While these match the transcribed values, hard-coding increases risk of silent divergence if upstream calibration values change. This is not a biased transform per se, but it is a fragile practice for integrity/reproducibility.: # Base treatment effects\nbase_consumption <- 35  # Monthly, USD PPP\nbase_earnings <- 17     # Monthly, USD PPP",
      "confidence": 0.83
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "06_tables.R",
      "lines": [
        203,
        204,
        205,
        206,
        207,
        208
      ],
      "evidence": "Although robustness results may be loaded (`robustness_results.RData`), the reported sensitivity table is not computed from them. This is a selective-reporting risk: robustness might be run but the table can remain fixed to a curated set of outcomes.: # Create sensitivity summary from robustness results if available, otherwise use baseline variations\nbaseline_mvpf <- mvpf_results$mvpf_direct\nsensitivity_summary <- tibble(\n  parameter = c(\"Baseline\", \"VAT coverage 25%\", \"VAT coverage 100%\", ...),\n  mvpf = c(baseline_mvpf, 0.87, 0.90, 0.86, 0.89, 0.88, 0.86, 0.86, 0.89),\n  ...\n)",
      "confidence": 0.9
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "04_heterogeneity.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 2,
      "MEDIUM": 3,
      "LOW": 4
    },
    "one_liner": "method mismatch",
    "executive_summary": "The code does not implement the manuscript\u2019s stated uncertainty propagation for key fiscal parameters: VAT coverage, informality share, and administrative cost are set as fixed values in `params` in `03_main_analysis.R` rather than being drawn from beta distributions and carried through the analysis. In `02_clean_data.R`, the construction of the fiscal-externality denominator does not follow the manuscript\u2019s description of using recipient treatment effects from Haushofer & Shapiro (2016) to avoid overlap with general-equilibrium spillovers, implying the denominator is based on a different source/definition than claimed.",
    "top_issues": [
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "Manuscript claims uncertainty for fiscal parameters is pr...",
        "file": "03_main_analysis.R",
        "lines": [
          44,
          45
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0184/code/03_main_analysis.R#L44-L59"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript states fiscal externalities in the denomin...",
        "file": "02_clean_data.R",
        "lines": [
          86,
          87
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0184/code/02_clean_data.R#L86-L91"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0184_scan.json"
  },
  "error": null
}