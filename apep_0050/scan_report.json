{
  "paper_id": "apep_0050",
  "scan_date": "2026-02-06T12:36:22.432022+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 11,
  "flags": [
    {
      "category": "DATA_FABRICATION",
      "severity": "HIGH",
      "file": "04_robustness.R",
      "lines": [
        156,
        170
      ],
      "evidence": "The script generates synthetic state-level control variables (minimum wage and unemployment) using rnorm(). Even though commented as placeholders, this is still executable code that fabricates covariates and then merges them into the analysis dataset. If any downstream models accidentally use cps_controls (or if a user assumes these controls are real), estimates become non-reproducible and potentially misleading.: state_controls <- cps_did %>%\n  distinct(statefip, year) %>%\n  mutate(\n    # Placeholder: would merge actual data here\n    log_minwage = rnorm(n(), mean = 2.5, sd = 0.1),  # Placeholder\n    unemployment = rnorm(n(), mean = 5, sd = 1)  # Placeholder\n  )\n\ncps_controls <- cps_did %>%\n  left_join(state_controls, by = c(\"statefip\", \"year\"))\n\n# Run with controls using formula interface\nmessage(\"Note: Using placeholder state controls - replace with actual data\")",
      "confidence": 0.95
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "paper.tex",
      "lines": [
        240,
        260
      ],
      "evidence": "The manuscript LaTeX tables include literal coefficient and SE values rather than being programmatically generated/inserted (e.g., via \\input{...} of auto-generated tables). This is not inherently wrong, but it creates a code-to-paper traceability risk: readers cannot verify from the TeX source alone that these numbers come from the provided scripts/results objects.: Treated & $-0.039^{***}$ & $-0.042^{***}$ \\\\\n& (0.010) & (0.007) \\\\",
      "confidence": 0.7
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "paper.tex",
      "lines": [
        303,
        340
      ],
      "evidence": "The full event-study coefficient table is hard-coded in the manuscript. Given that the code (run_analysis.R / run_analysis_fast.R / 08_honestdid.R) computes event-study estimates, best practice is to export a LaTeX table from code and \\input{} it into the manuscript to prevent transcription or selective-updating errors.: Event Time & Estimate & SE & 95\\% CI & Stars \\\\\n\\midrule\n$-8$ & 0.000 & 0.027 & [$-0.052$, 0.052] & \\\\\n...\n$-3$ & 0.027 & 0.007 & [0.012, 0.041] & *** \\\\\n...\n1 & $-0.057$ & 0.007 & [$-0.072$, $-0.043$] & *** \\\\",
      "confidence": 0.75
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "run_analysis.R",
      "lines": [
        52,
        78
      ],
      "evidence": "The main pipeline assumes the CPS MORG .dta files already exist locally in data/. There is a separate fetch script (01_fetch_data.R) that downloads these, but run_analysis.R does not check/enforce that provenance step (e.g., verifying hashes, verifying URLs, or calling the fetch script). Reproducibility depends on an undocumented prior manual state of the data/ directory.: morg_files <- list.files(\"data\", pattern = \"morg.*\\\\.dta$\", full.names = TRUE)\ncat(\"Found\", length(morg_files), \"MORG files\\n\")\n...\ndf <- haven::read_dta(f)",
      "confidence": 0.7
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "02_clean_data.R",
      "lines": [
        44,
        62
      ],
      "evidence": "Comment and implementation conflict. The comment states: \"Treatment year = effective year if effective in first half, otherwise next year (for conservative approach)\", but the code sets treatment_year = treat_year in both branches. This affects treatment timing for mid-year laws (e.g., NY Sept 2023, DC June 2024, MD Oct 2024) and can materially change event-time indicators and ATT estimates relative to what the manuscript describes (paper.tex says they code mid-year effective dates to begin the following year).: treatment_year = case_when(\n  treat_month <= 6 ~ treat_year,\n  TRUE ~ treat_year  # Keep actual year for precision\n)",
      "confidence": 0.9
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "LOW",
      "file": "paper.tex",
      "lines": [
        170,
        190
      ],
      "evidence": "This manuscript statement conflicts with 02_clean_data.R's current treatment_year construction (which does not shift mid-year effective dates to the next year). Severity is reduced to LOW here because the mismatch is already flagged at the code location above; but it should be reconciled to ensure the published design matches implemented treatment timing.: For states with mid-year effective dates (e.g., New York on September 17, 2023), we code treatment as beginning the following year to ensure a clean pre/post comparison.",
      "confidence": 0.85
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "02_clean_data.R",
      "lines": [
        116,
        143
      ],
      "evidence": "The code drops all observations at/above 2885 rather than (i) top-coding them to 2885 or (ii) using CPS-provided top-code flags. Dropping top-coded observations can change distributional results and tail percentiles (p90) in ways that may not be innocuous. The manuscript says \"top-coded\"; dropping is a different operation than top-coding, though it may be defensible if documented.: cps_analysis <- cps %>%\n  filter(\n    ...\n    # Reasonable earnings (top-code handling)\n    earnweek < 2885  # Top code varies; use conservative threshold\n  )",
      "confidence": 0.75
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "LOW",
      "file": "06_tables.R",
      "lines": [
        249,
        280
      ],
      "evidence": "Table construction for the gender-gap section includes placeholders (NA) for descriptive pre-treatment gaps and then exports the table. This is not fabrication, but it creates a risk that an incomplete/partially filled table is still used in the paper or shared outputs without clear marking that values are missing.: gender_gap_table <- tibble(\n  Specification = c(\n    \"DiD (Treated \u00d7 Post)\",\n    \"Pre-treatment gender gap (treated)\",\n    \"Pre-treatment gender gap (control)\"\n  ),\n  Estimate = c(\n    coef(gap_did)[\"treated\"],\n    NA,  # Would compute from data\n    NA   # Would compute from data\n  ),\n  SE = c(\n    se(gap_did)[\"treated\"],\n    NA,\n    NA\n  )\n)",
      "confidence": 0.8
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "08_honestdid.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "run_analysis_fast.R",
      "verdict": "CLEAN"
    },
    {
      "file": "run_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "07_additional_exhibits.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 1,
      "MEDIUM": 4,
      "LOW": 3
    },
    "one_liner": "fabricated data",
    "executive_summary": "In `04_robustness.R`, the code fabricates key state-level control variables\u2014minimum wage and unemployment\u2014by drawing random values with `rnorm()`, meaning the robustness models can be run on synthetic covariates rather than real data. Although labeled as placeholders in comments, the lines are executable and can directly influence results, undermining the credibility and reproducibility of any findings that rely on these controls.",
    "top_issues": [
      {
        "category": "DATA_FABRICATION",
        "severity": "HIGH",
        "short": "The script generates synthetic state-level control variab...",
        "file": "04_robustness.R",
        "lines": [
          156,
          170
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0050/code/04_robustness.R#L156-L170"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0050_scan.json"
  },
  "error": null
}