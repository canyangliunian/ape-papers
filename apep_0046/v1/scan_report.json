{
  "paper_id": "apep_0046",
  "scan_date": "2026-02-06T12:35:25.112446+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 7,
  "flags": [
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "HIGH",
      "file": "03_policy_data.R",
      "lines": [
        12,
        13,
        14,
        15,
        16,
        17,
        18,
        19,
        20,
        21,
        22,
        23,
        24,
        25,
        26,
        27,
        28,
        29,
        30,
        31,
        32,
        33,
        34,
        35,
        36,
        37,
        38,
        39,
        40,
        41,
        42,
        43,
        44
      ],
      "evidence": "Key treatment timing (state EITC adoption years) is manually hard-coded into the analysis pipeline via a tribble, with no code that downloads/archives the underlying source data (ITEP/NCSL/TAXSIM) and no citation metadata embedded in the dataset itself. This is a central input to identification (treatment status, cohort, event time); without a provenance/replication artifact (e.g., a raw source file with checksum or a fetch script), results cannot be independently verified for transcription errors or selective updates.: eitc_adoption <- tribble(\n  ~statefip, ~state, ~year_enacted,\n  44, \"Rhode Island\", 1986,\n  24, \"Maryland\", 1987,\n  50, \"Vermont\", 1988,\n  ...\n  29, \"Missouri\", 2023\n)",
      "confidence": 0.88
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "03_policy_data.R",
      "lines": [
        107,
        108,
        109,
        110,
        111,
        112,
        113,
        114,
        115,
        116,
        117,
        118,
        119,
        120,
        121,
        122,
        123,
        124,
        125,
        126,
        127,
        128,
        129,
        130,
        131,
        132,
        133,
        134,
        135,
        136,
        137,
        138,
        139,
        140
      ],
      "evidence": "State EITC generosity and refundability are also hard-coded (as '2024 rates as proxy'), with no machine-readable source or versioning. While the manuscript\u2019s main treatment is a binary adoption indicator, these variables are still merged into the microdata and could be used in robustness/interpretation; their provenance is not reproducible from code alone.: eitc_generosity <- tribble(\n  ~statefip, ~rate_2024, ~refundable,\n  6, 47, 1,   # California\n  8, 50, 1,   # Colorado\n  ...\n  55, 15, 1   # Wisconsin (using avg of 4-34)\n)\n\nstate_year_panel <- state_year_panel %>%\n  left_join(eitc_generosity, by = \"statefip\") %>%\n  mutate(\n    rate_2024 = replace_na(rate_2024, 0),\n    refundable = replace_na(refundable, NA)\n  )",
      "confidence": 0.83
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "06_figures.R",
      "lines": [
        91,
        92,
        93,
        94,
        95,
        96
      ],
      "evidence": "Figure 4 is built from manually entered point estimates and standard errors rather than being programmatically extracted from the stored model objects (e.g., model2 and placebo_model in 05_main_analysis.R). This creates a transcription risk and opens the door to unintentional or intentional cherry-picking if the underlying estimates change.: coef_comparison <- tibble(\n  group = c(\"Single Mothers\\n(Main Sample)\", \"Childless Women\\n(Placebo)\"),\n  estimate = c(-0.0127, -0.0050),\n  std_error = c(0.0062, 0.0047),\n  ci_lower = estimate - 1.96 * std_error,\n  ci_upper = estimate + 1.96 * std_error\n)",
      "confidence": 0.9
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "05_main_analysis.R",
      "lines": [
        95,
        96,
        97,
        98,
        99,
        100,
        101,
        102,
        103,
        104,
        105,
        106,
        107
      ],
      "evidence": "The manuscript describes an event study used to assess parallel trends and also separately reports Sun-Abraham estimates. However, the event study implemented in the main analysis explicitly drops never-treated observations (event_time == -999). With state and year fixed effects, an event study that excludes never-treated (and not-yet-treated) units is not the standard DiD event-study design used to test parallel trends against a control group; it risks identifying dynamics largely from within-treated timing/functional-form rather than treated vs. control contrasts. This can materially change interpretation of pre-trends and post effects relative to what readers expect from a DiD event study.: event_study <- feols(\n  self_employed ~ i(event_time, ref = -1) + age + I(age^2) + low_education +\n    n_children_u18 | statefip + year,\n  data = main_sample %>% filter(event_time != -999),  # Exclude never-treated for event study\n  weights = ~weight,\n  cluster = ~statefip\n)",
      "confidence": 0.84
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "MEDIUM",
      "file": "05_main_analysis.R",
      "lines": [
        78,
        79,
        80,
        81,
        82,
        83,
        84,
        85,
        86,
        87,
        88
      ],
      "evidence": "Never-treated units are coded to a sentinel value (-999) and later dropped from the event-study regression. Separately, event time is top/bottom-coded to [-5, 5]. Binning is common, but dropping never-treated in the event study is a consequential sample transformation that can bias graphical and inferential diagnostics (pre-trend assessment) relative to a canonical DiD event study.: main_sample <- main_sample %>%\n  mutate(\n    event_time = year - cohort,\n    event_time = case_when(\n      is.na(event_time) | is.infinite(event_time) ~ -999,  # Never treated\n      event_time < -5 ~ -5,   # Bin early periods\n      event_time > 5 ~ 5,     # Bin late periods\n      TRUE ~ event_time\n    )\n  )",
      "confidence": 0.8
    },
    {
      "category": "STATISTICAL_IMPOSSIBILITY",
      "severity": "LOW",
      "file": "05_main_analysis.R",
      "lines": [
        124,
        125,
        126
      ],
      "evidence": "The reference-period row is appended with std_error = 0 and CI bounds exactly equal to 0. This is not a 'real' estimate/SE and can be misread if reused for inference or summary calculations (e.g., average SEs). It is likely only for plotting, so severity is low, but it is safer to store NA for SE/CI in the reference row or explicitly flag it as a reference.: event_coefs <- bind_rows(\n  event_coefs,\n  tibble(event_time = -1, estimate = 0, std_error = 0, ci_lower = 0, ci_upper = 0)\n) %>%\n  arrange(event_time)",
      "confidence": 0.76
    },
    {
      "category": "STATISTICAL_IMPOSSIBILITY",
      "severity": "LOW",
      "file": "07_robustness.R",
      "lines": [
        44,
        45
      ],
      "evidence": "The reported Sun-Abraham 'approx SE' is computed via a nonstandard approximation (root mean of squared per-period SEs), which is not a valid standard error for the average post-treatment effect in general (ignores covariance across event-time estimates and weighting). This is not impossible per se, but it can produce misleadingly small SEs and overstated significance if interpreted as a formal SE, especially since the manuscript reports a precise SE for Sun-Abraham.: sa_att_se <- sqrt(mean(post_coefs$std_error^2))  # Simplified SE",
      "confidence": 0.78
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        73,
        74,
        75,
        76
      ],
      "evidence": "Paths are hard-coded to a user-specific local directory, reducing portability and making it harder for an auditor to confirm that the same inputs were used elsewhere. This is more a reproducibility concern than integrity by itself, since the CPS microdata are fetched from a documented API in 01_fetch_data.R.: base_path <- \"/Users/celine/Dropbox (Personal)/SC_Diverse/auto-policy-evals/output/paper_63\"\ndata_path <- file.path(base_path, \"data\")\ncode_path <- file.path(base_path, \"code\")\nfig_path <- file.path(base_path, \"figures\")",
      "confidence": 0.86
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "07_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_policy_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "05_main_analysis.R",
      "verdict": "SUSPICIOUS"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 2,
      "MEDIUM": 3,
      "LOW": 3
    },
    "one_liner": "unclear provenance; method mismatch",
    "executive_summary": "The analysis hard-codes key treatment timing information (state EITC adoption years) directly into `03_policy_data.R` via a tribble, with no code to download, archive, or cite the underlying source dataset (e.g., ITEP/NCSL/TAXSIM), making the treatment assignment non-reproducible and difficult to verify. In `05_main_analysis.R`, the implemented event-study specification does not match the manuscript\u2019s description (it drops never-treated units), undermining the stated parallel-trends assessment and making the reported event-study and Sun\u2013Abraham results potentially incomparable or inconsistently defined.",
    "top_issues": [
      {
        "category": "DATA_PROVENANCE_MISSING",
        "severity": "HIGH",
        "short": "Key treatment timing (state EITC adoption years) is manua...",
        "file": "03_policy_data.R",
        "lines": [
          12,
          13
        ],
        "github_url": "/apep_0046/code/03_policy_data.R#L12-L44"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript describes an event study used to assess pa...",
        "file": "05_main_analysis.R",
        "lines": [
          95,
          96
        ],
        "github_url": "/apep_0046/code/05_main_analysis.R#L95-L107"
      }
    ],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0046_scan.json"
  },
  "error": null
}