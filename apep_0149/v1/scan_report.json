{
  "paper_id": "apep_0149",
  "scan_date": "2026-02-06T12:52:19.893832+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 7,
  "flags": [
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "01_fetch_data.R",
      "lines": [
        12,
        31,
        68
      ],
      "evidence": "The manuscript states ACS years used are 2017\u20132019 and 2021\u20132022 (explicitly excluding 2020; 2023 not available). This script fetches 2017\u20132023 inclusive, which (i) brings in 2020 despite the stated exclusion and (ii) brings in 2023 despite the stated unavailability. If these years remain in downstream analysis, estimates, placebo tests, and figures may not correspond to the paper as written.: cat(\"=== Fetching ACS PUMS data (2017-2023) ===\\n\")\n...\n# Years to fetch (2017-2023 for 1-year ACS PUMS)\nyears <- 2017:2023\n...\nall_data <- list()\nfor (yr in years) {\n  result <- fetch_acs_pums(yr, vars)\n  ...\n}",
      "confidence": 0.9
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "HIGH",
      "file": "02_clean_data.R",
      "lines": [
        67,
        88,
        93,
        97
      ],
      "evidence": "Treatment coding is driven by `max_sample_year = max(year)` in the fetched microdata. If 2023 is present (as in 01_fetch_data.R), then (a) states with adopt_year == 2023 become treated and `first_treat` becomes 2023 rather than 0, contradicting the manuscript\u2019s rule that 2023+ adopters must be coded not-yet-treated because ACS ends in 2022; and (b) a 'post-PHE' regime is implicitly introduced. This makes the analysis sample definition sensitive to whether 2023 was fetched (and whether 2020 was fetched), creating a high risk that results depend on unintended years rather than a pre-specified sample restriction.: mutate(\n  # Maximum year in sample (for restricting treatment to observable cohorts)\n  max_sample_year = max(year),\n\n  # Treatment: state has adopted by this year AND adoption is within sample\n  treated = as.integer(!is.na(adopt_year) & adopt_year <= max_sample_year & year >= adopt_year),\n\n  # For CS-DiD: first_treat = adoption year, 0 = never/not-yet treated\n  first_treat = ifelse(is.na(adopt_year) | adopt_year > max_sample_year, 0, adopt_year),\n\n  # Post-PHE indicator\n  post_phe = as.integer(year >= 2023),\n\n  # Clean treatment: adopted AND post-PHE (when extension has bite)\n  treated_post_phe = as.integer(treated == 1 & post_phe == 1)\n)",
      "confidence": 0.85
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "03_main_analysis.R",
      "lines": [
        63,
        69,
        77,
        85
      ],
      "evidence": "The manuscript describes a design using 22 controls that are 'not-yet-treated or never-treated' jurisdictions. In Callaway-Sant\u2019Anna, including not-yet-treated units as controls corresponds to `control_group = \"notyettreated\"` (or equivalent logic). Here the code sets `control_group = \"nevertreated\"` while `first_treat` is coded as 0 for both never-treated and not-yet-treated states (02_clean_data.R: `first_treat = ... 0 ...`). That combination risks mis-specifying the control group (treating not-yet-treated as never-treated), which can change ATT construction and invalidate the manuscript\u2019s stated comparison set. At minimum, this needs reconciliation: either (i) use `control_group = \"notyettreated\"` with correct cohort coding, or (ii) keep `nevertreated` but then the paper should not claim not-yet-treated are used as controls.: cs_medicaid <- att_gt(\n  yname = \"medicaid_rate\",\n  tname = \"year\",\n  idname = \"state_id\",\n  gname = \"first_treat\",\n  data = as.data.frame(state_year_pp),\n  control_group = \"nevertreated\",\n  weightsname = \"total_weight\",\n  bstrap = TRUE,\n  cband = TRUE,\n  biters = 1000\n)",
      "confidence": 0.75
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        94,
        105,
        113
      ],
      "evidence": "Policy adoption dates (treatment assignment) are hard-coded as a manual table with no machine-verifiable links (URLs, citations, archived sources) or a reproducible scrape. The manuscript says these were compiled from CMS/KFF/etc., which makes hard-coding plausible, but for auditability it is still a provenance gap: changes to this table can materially affect treatment coding, treated/control counts, and event-time alignment.: treatment_dates <- tribble(\n  ~state_fips, ~state_abbr, ~adopt_year, ~mechanism,\n  ...\n  17, \"IL\", 2021, \"Waiver\",\n  ...\n  48, \"TX\", 2024, \"SPA\",\n  49, \"UT\", 2024, \"SPA\",\n  16, \"ID\", 2025, \"SPA\",\n  19, \"IA\", 2025, \"SPA\",\n  32, \"NV\", 2024, \"SPA\"\n)",
      "confidence": 0.65
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        94,
        105,
        113
      ],
      "evidence": "Treatment cohort years are hard-coded rather than constructed from a raw source file or API. Given the manuscript\u2019s description (dates compiled from CMS/KFF/state records), this is not inherently improper, but it increases the risk of transcription error or post-hoc adjustments unless the underlying sources are documented/archived in-repo.: treatment_dates <- tribble(\n  ~state_fips, ~state_abbr, ~adopt_year, ~mechanism,\n  ...\n)",
      "confidence": 0.6
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 3,
      "MEDIUM": 1,
      "LOW": 1
    },
    "one_liner": "method mismatch; suspicious transforms",
    "executive_summary": "The data acquisition code pulls ACS microdata for 2017\u20132023 inclusive, contradicting the manuscript\u2019s stated sample window (2017\u20132019 and 2021\u20132022 only) and implicitly introducing 2020 (explicitly excluded) and 2023 (stated as unavailable). Downstream cleaning and treatment assignment are then anchored to `max(year)` in the downloaded data, so the presence of 2023 can flip treatment coding (e.g., classifying 2023 adopters as treated and shifting \u201cfirst treated\u201d timing) in a way that is sensitive to whatever years happen to be fetched rather than the paper\u2019s design. The main analysis also misaligns with the described identification strategy by not clearly enforcing the \u201cnot-yet-treated or never-treated\u201d control definition as implemented in Callaway\u2013Sant\u2019Anna (i.e., control group specification), making the estimated effects inconsistent with the stated methodology.",
    "top_issues": [
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript states ACS years used are 2017\u20132019 and 20...",
        "file": "01_fetch_data.R",
        "lines": [
          12,
          31
        ],
        "github_url": "/apep_0149/code/01_fetch_data.R#L12-L68"
      },
      {
        "category": "SUSPICIOUS_TRANSFORMS",
        "severity": "HIGH",
        "short": "Treatment coding is driven by `max_sample_year = max(year...",
        "file": "02_clean_data.R",
        "lines": [
          67,
          88
        ],
        "github_url": "/apep_0149/code/02_clean_data.R#L67-L97"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript describes a design using 22 controls that ...",
        "file": "03_main_analysis.R",
        "lines": [
          63,
          69
        ],
        "github_url": "/apep_0149/code/03_main_analysis.R#L63-L85"
      }
    ],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0149_scan.json"
  },
  "error": null
}