{
  "paper_id": "apep_0118",
  "scan_date": "2026-02-06T12:46:31.878794+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "CLEAN",
  "files_scanned": 8,
  "flags": [
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "01_fetch_qcew.R",
      "lines": [
        78,
        81
      ],
      "evidence": "The manuscript states the analysis sample spans 2014\u20132024 (\"Our analysis sample spans 2014--2024\"), but the code fetches QCEW data for 2010\u20132024. This can change the identifying variation (more pre-periods), affect event-study window availability, and may alter ATT estimates relative to what is described in the paper.: years <- 2010:2024\n\n# Fetch gambling industry employment\ngambling_list <- lapply(years, function(y) fetch_qcew_year(y, \"7132\"))",
      "confidence": 0.8
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "02_fetch_policy.R",
      "lines": [
        154,
        158
      ],
      "evidence": "The state-year policy panel is explicitly constructed for 2010\u20132024, while the manuscript describes the estimation sample as 2014\u20132024 (and claims \"four years of pre-treatment data\" pre-2018, i.e., 2014\u20132017). If the downstream analysis uses 2010\u20132013 data, the paper\u2019s stated sample window is inaccurate.: # Create state-year panel (2010-2024)\nstate_year_panel <- expand_grid(\n  state_abbr = state_fips$state_abbr,\n  year = 2010:2024\n)",
      "confidence": 0.75
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "03_clean_data.R",
      "lines": [
        120,
        124
      ],
      "evidence": "Descriptive statistics are labeled/structured as pre-treatment 2010\u20132017 in code comments and later table production, while the manuscript frames the pre-period as 2014\u20132017. This suggests the pipeline may be using additional early years (2010\u20132013) not acknowledged in the manuscript, affecting descriptive statistics and potentially identification checks.: # Pre-treatment period (2010-2017)\npre_treatment <- main_sample %>%\n  filter(year <= 2017) %>%\n  group_by(ever_treated = g > 0) %>%\n  summarise(",
      "confidence": 0.7
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "07_tables.R",
      "lines": [
        70,
        82
      ],
      "evidence": "Table creation substitutes a literal 0 when key estimated objects are NULL/NA (e.g., cs_overall$overall.att or overall.se). If estimation fails or returns NA, the LaTeX tables will silently report ATT=0 and SE=0 rather than erroring or flagging missingness. This is a hard-coding pathway that can mask failures and produce misleading \u2018results\u2019.: get_val <- function(x, default = NA) if (is.null(x) || is.na(x)) default else x\n\nmain_table <- tibble(\n  Specification = c(\n    \"Callaway-Sant'Anna (not-yet-treated)\",\n    \"TWFE (biased)\"\n  ),\n  ATT = c(\n    sprintf(\"%.4f\", get_val(cs_overall$overall.att, 0)),\n    sprintf(\"%.4f\", coef(twfe)[\"treated_sb\"])\n  ),\n  SE = c(\n    sprintf(\"(%.4f)\", get_val(cs_overall$overall.se, 0)),\n    sprintf(\"(%.4f)\", se(twfe)[\"treated_sb\"])\n  ),",
      "confidence": 0.85
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "LOW",
      "file": "07_tables.R",
      "lines": [
        44,
        49
      ],
      "evidence": "The summary table header hard-codes the pre-treatment window as 2010\u20132017. The manuscript describes 2014\u20132017 as the pre-treatment period (and 2014\u20132024 as the overall sample). This inconsistency can mislead readers about the actual sample used.: table1_tex <- kable(summary_table, format = \"latex\", booktabs = TRUE,\n                    caption = \"Summary Statistics: Gambling Industry Employment (NAICS 7132)\",\n                    label = \"tab:summary\") %>%\n  kable_styling(latex_options = c(\"hold_position\")) %>%\n  add_header_above(c(\" \" = 1, \"Pre-Treatment (2010-2017)\" = 6))",
      "confidence": 0.7
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "05_robustness.R",
      "lines": [
        28,
        44
      ],
      "evidence": "Sensitivity-analysis tuning parameters (Mvec) are chosen in code without being clearly tied to a justification in the provided manuscript text. This is not data provenance per se, but it is an unprovenanced/undocumented analytic choice that can materially affect sensitivity conclusions. Consider documenting why these bounds are meaningful in this application.: honest_result <- honest_did(\n    main_results$cs_result,\n    type = \"smoothness\",\n    Mvec = c(0, 0.5, 1, 2)  # Different bounds on M\n  )",
      "confidence": 0.55
    }
  ],
  "file_verdicts": [
    {
      "file": "07_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_fetch_policy.R",
      "verdict": "CLEAN"
    },
    {
      "file": "01_fetch_qcew.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_main_analysis.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "CLEAN",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 0,
      "MEDIUM": 4,
      "LOW": 2
    },
    "one_liner": "Minor issues only",
    "executive_summary": "Minor code quality issues detected, but no evidence of data fabrication or manipulation.",
    "top_issues": [],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0118_scan.json"
  },
  "error": null
}