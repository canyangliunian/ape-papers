\documentclass[12pt]{article}

% UTF-8 encoding and fonts
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% Page setup
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\onehalfspacing

% Math and symbols
\usepackage{amsmath,amssymb}

% Graphics
\usepackage{graphicx}
\usepackage{float}

% Tables
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}

% Bibliography
\usepackage{natbib}
\bibliographystyle{aer}

% Hyperlinks
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% Captions
\usepackage{caption}
\captionsetup{font=small,labelfont=bf}

% Section formatting
\usepackage{titlesec}
\titleformat{\section}{\large\bfseries}{\thesection.}{0.5em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{0.5em}{}

% Custom commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}

\title{Do State Automatic IRA Mandates Affect Self-Reported Employer Retirement Plan Coverage? \\ Evidence from Staggered Policy Adoption}
\author{APEP Autonomous Research\thanks{Autonomous Policy Evaluation Project. This paper was autonomously generated using Claude Code (claude-opus-4-5). nd @dakoyana}}
\date{January 2026}

\begin{document}

\maketitle

\begin{abstract}
\noindent
State auto-IRA mandates require employers without retirement plans to enroll employees in state-facilitated IRAs. This paper estimates whether these mandates affect \textit{self-reported employer retirement plan coverage}---a distinct outcome from auto-IRA participation itself, since CPS asks about ``employer'' plans while auto-IRAs are state-facilitated individual accounts. Using CPS ASEC data from 2010--2024 and a Callaway-Sant'Anna difference-in-differences design exploiting staggered adoption across eleven states, I find an overall ATT of 0.75 percentage points (SE = 1.0 pp; wild bootstrap p = 0.48), not statistically significant. This null result is driven by Oregon's anomalous negative effect ($-2.1$ pp); a systematic leave-one-out analysis shows Oregon is uniquely influential. Excluding Oregon yields a significant effect of 1.6 pp (SE = 0.6 pp; p $<$ 0.01) that grows to 3.5 pp by event time +5. These findings suggest that auto-IRA mandates may increase self-reported employer plan coverage through awareness spillovers or employer behavioral responses, though the CPS outcome does not directly measure auto-IRA participation.
\end{abstract}

\vspace{1em}
\noindent\textbf{JEL Codes:} H31, J26, J32, D14 \\
\noindent\textbf{Keywords:} automatic enrollment, retirement savings, state policy, difference-in-differences

\newpage

\section{Introduction}

Roughly half of American private-sector workers lack access to an employer-sponsored retirement plan. This ``coverage gap'' is particularly acute among workers at small businesses, part-time employees, and lower-wage workers---precisely those who may benefit most from the tax advantages and behavioral nudges that employer-facilitated retirement savings provide. The gap persists despite decades of policy efforts to expand retirement savings, and many observers worry it contributes to inadequate retirement preparedness across the income distribution.

Beginning in 2017, states have increasingly turned to a novel policy response: mandatory automatic IRA programs. These programs require employers who do not offer their own retirement plan to enroll employees in a state-facilitated Individual Retirement Account (IRA). Drawing on insights from behavioral economics about the power of defaults, automatic enrollment removes the burden of active decision-making that may deter workers from saving. Early evidence from program administrative data suggests high retention rates among enrolled workers, with approximately 70\% of OregonSaves participants remaining enrolled after automatic enrollment.

This paper provides the first quasi-experimental evaluation of state auto-IRA mandates using nationally representative survey data. I exploit the staggered adoption of mandatory programs across eleven states from 2017--2024 in a difference-in-differences framework, using the Callaway and Sant'Anna (2021) estimator designed for settings with staggered treatment timing and treatment effect heterogeneity. The outcome of interest is whether workers report having any retirement plan coverage from their current employer, measured in the Current Population Survey Annual Social and Economic Supplement (CPS ASEC).

The main finding is a null result: I estimate an overall average treatment effect of 0.75 percentage points (SE = 1.0 pp, 95\% CI: $[-1.2, 2.7]$), which is not statistically distinguishable from zero. Event study estimates show no evidence of differential pre-trends between treatment and control states, lending credibility to the parallel trends assumption. Post-treatment coefficients are uniformly positive but imprecisely estimated, with point estimates that grow modestly over time (reaching 2.2 percentage points at event time +5).

Results are heterogeneous across treatment cohorts. Illinois (treated 2018) and California (treated 2019) show positive and statistically significant effects of 2.6 and 2.2 percentage points respectively. In contrast, Oregon (treated 2017) shows a surprising negative effect of $-2.1$ percentage points. This heterogeneity merits further investigation but may reflect differences in program implementation, employer compliance, or local economic conditions.

The null overall result admits several interpretations. First, measurement error may attenuate estimates toward zero: the CPS ASEC asks whether workers are ``included in a pension or retirement plan'' at their current employer, which may not capture workers who understand their auto-IRA as an individual account rather than an employer plan. Second, the relatively short post-treatment periods for most treated states (many adopted in 2021--2024) may be insufficient to detect effects that build gradually as programs scale up. Third, high opt-out rates or limited employer compliance could genuinely result in small effects on coverage.

This paper contributes to literatures on retirement savings policy, behavioral economics, and state policy innovation. Despite the null finding, the analysis provides a methodological template for future evaluations as programs mature and longer post-treatment data become available. The heterogeneity across states suggests that program design details and implementation may matter substantially for effectiveness.

\section{Institutional Background}

\subsection{The Retirement Coverage Gap}

Approximately 57 million American workers---about half of the private-sector workforce---lack access to an employer-sponsored retirement plan. Coverage rates are lowest among workers at small businesses: only 42\% of workers at firms with fewer than 100 employees have access to a retirement plan, compared to 79\% at firms with 500 or more employees. Workers without employer plans must save through individual retirement accounts (IRAs), but contribution rates to IRAs outside of employer facilitation are low.

The coverage gap has several explanations. Small employers face fixed costs of plan administration that make offering plans uneconomical. Workers at small firms tend to have lower wages and higher turnover, making them less attractive participants. And absent the convenience of payroll deduction and the behavioral nudge of employer defaults, workers may simply not get around to opening and contributing to retirement accounts.

\subsection{State Automatic IRA Programs}

Beginning in 2012, states began exploring legislation to address the coverage gap. California passed the first enabling legislation in 2012, though the program (CalSavers) did not launch until 2019. Oregon launched the first operational program (OregonSaves) in July 2017. As of 2024, eleven states have implemented mandatory auto-IRA programs:

\begin{table}[H]
\centering
\caption{State Auto-IRA Program Adoption Timeline}
\begin{tabular}{llcc}
\toprule
State & Program Name & Launch Date & Full Mandate \\
\midrule
Oregon & OregonSaves & July 2017 & 2023 \\
Illinois & Secure Choice & November 2018 & 2023 \\
California & CalSavers & July 2019 & 2025 \\
Connecticut & MyCTSavings & March 2021 & 2022 \\
Maryland & MarylandSaves & September 2022 & 2024 \\
Colorado & SecureSavings & March 2023 & 2025 \\
Virginia & Virginia Saves & July 2023 & 2025 \\
Maine & Retirement Savings & 2024 & 2025 \\
Delaware & Delaware Saves & 2024 & 2025 \\
New Jersey & Secure Choice & 2024 & 2026 \\
Vermont & Green Mountain Secure & 2024 & 2025 \\
\bottomrule
\end{tabular}
\label{tab:treatment_timing}
\end{table}

These programs share core design elements. Employers without existing retirement plans must facilitate employee participation in a state-administered Roth IRA. Employees are automatically enrolled at a default contribution rate (typically 3--5\% of wages), though they may opt out or adjust their contribution rate. Contributions are made through payroll deduction, minimizing transaction costs for workers. The programs are funded through participant fees and initial state investments rather than employer contributions.

Most programs phase in by employer size, starting with larger employers and gradually extending to smaller businesses. For example, Oregon initially required employers with 100+ employees to register in 2017, then extended to 50--99 employees in 2018, 25--49 in 2019, and so on. This phased implementation creates within-state variation in treatment timing that I do not exploit in the main analysis (which uses state-level treatment indicators) but that could inform future research.

\subsection{Policy Expectations}

Theory and evidence from 401(k) automatic enrollment suggest that auto-IRA programs could substantially increase retirement savings participation. Madrian and Shea (2001) found that automatic enrollment in employer 401(k) plans raised participation rates from 37\% to 86\% at a large corporation. Similar effects have been documented across many employers.

However, several factors may limit the applicability of these findings to state auto-IRA programs. First, unlike 401(k)s, IRAs do not feature employer matching contributions, which may reduce the incentive to participate. Second, IRA contribution limits (\$7,000 in 2024) are substantially lower than 401(k) limits (\$23,000), limiting benefits for higher savers. Third, state programs face challenges of employer awareness, compliance, and enforcement that do not arise in within-firm 401(k) programs.

\section{Related Literature}

This paper relates to three strands of literature: automatic enrollment and retirement savings, state auto-IRA policy evaluation, and econometric methods for staggered difference-in-differences.

\textbf{Automatic enrollment and retirement savings.} An extensive literature documents the power of defaults in retirement savings decisions. Seminal work by Madrian and Shea (2001) showed that automatic enrollment in a single firm's 401(k) plan raised participation from 37\% to 86\%. Choi, Laibson, Madrian, and Metrick (2002, 2004) extended these findings and documented persistent effects on contribution rates and asset allocation. Thaler and Benartzi (2004) demonstrated that ``Save More Tomorrow'' programs leveraging defaults and inertia dramatically increased retirement savings. Beshears et al. (2009) synthesized the evidence on default effects across retirement plan contexts.

Chetty et al. (2014) provide important evidence from Denmark on whether automatic enrollment increases total savings or merely reshuffles existing savings. They find that automatic enrollment generates substantial net new savings, particularly for passive savers who would otherwise not contribute. However, these studies examine employer-sponsored automatic enrollment; whether state-facilitated programs generate similar effects is an open question that this paper addresses.

\textbf{State auto-IRA programs.} A growing policy literature examines state auto-IRA programs specifically. The Automatic IRA concept was developed by Gale, Iwry, John, and Walker (2009) at the Brookings Retirement Security Project, building on behavioral insights about defaults. Administrative data from program operators provide early evidence: OregonSaves reports approximately 70\% retention rates among auto-enrolled workers, substantially higher than voluntary IRA participation. CalSavers and Illinois Secure Choice report similar patterns. Belbase and Sanzenbacher (2017) project that universal auto-IRA coverage could increase retirement assets by 31\% for those currently without coverage.

The Georgetown Center for Retirement Initiatives and the Center for Retirement Research at Boston College have tracked program implementation and published descriptive statistics on enrollment and retention. However, no quasi-experimental studies using nationally representative data have evaluated the causal effects of state mandates on retirement coverage rates. This paper fills that gap, though with important measurement caveats discussed below.

\textbf{Difference-in-differences with staggered adoption.} Methodological advances inform this paper's empirical strategy. Bertrand, Duflo, and Mullainathan (2004) documented the serial correlation problem in DiD standard errors. Goodman-Bacon (2021) showed that two-way fixed effects estimators with staggered treatment timing can produce biased estimates when treatment effects are heterogeneous, as the estimator implicitly uses already-treated units as controls. Callaway and Sant'Anna (2021), Sun and Abraham (2021), de Chaisemartin and D'Haultfoeuille (2020), and Borusyak, Jaravel, and Spiess (2021) propose alternative estimators that avoid these biases.

When treated units are few, as in state-level policy evaluations, inference requires special care. Conley and Taber (2011) develop methods for inference with few policy changes. Roth (2022) shows that pre-testing for parallel trends can distort inference and recommends honesty-based approaches. This paper applies the Callaway-Sant'Anna estimator with never-treated states as controls and reports both clustered standard errors and sensitivity to influential states.

\section{Data}

\subsection{Data Source}

The analysis uses microdata from the Current Population Survey Annual Social and Economic Supplement (CPS ASEC) for years 2010--2024, accessed through IPUMS-CPS. The CPS ASEC is conducted each March and provides detailed information on demographics, employment, income, and benefits for a representative sample of approximately 180,000 individuals annually.

The key outcome variable is the IPUMS-CPS variable PENSION (ASEC), which identifies workers who respond ``yes'' to the question: ``At any time in the last year, were you included in a pension or retirement plan at work?'' This variable captures self-reported inclusion in both defined benefit pensions and defined contribution plans such as 401(k)s.

Several measurement issues merit attention. First, the question wording refers to plans ``at work,'' which may not capture auto-IRA participation if workers understand these as individual accounts facilitated by the state rather than employer plans. Second, the CPS ASEC is fielded in March and asks about ``the last year,'' so the 2017 survey references coverage during 2016---before any state auto-IRA program launched. For Oregon (launched July 2017), the first survey with meaningful exposure is March 2018. This timing misalignment may attenuate estimated effects in early treatment years. Third, programs phase in by employer size over multiple years, but our treatment indicator is binary at the state-year level.

\subsection{Sample Construction}

I restrict the sample to private-sector wage and salary workers ages 18--64 who report positive earnings. I exclude self-employed workers, government employees, and workers who report not being at work during the reference week. Workers with missing values for the pension variable are dropped.

The final analysis sample contains 596,834 person-year observations across 51 states and the District of Columbia over 15 years (2010--2024).

\subsection{Summary Statistics}

Table \ref{tab:summary} presents summary statistics by treatment status. Treated states (those that adopted auto-IRA mandates by 2024) have slightly lower pension coverage rates than never-treated states (15.0\% vs 15.7\%), though this difference is not statistically significant. The two groups are broadly similar in demographic composition.

\begin{table}[H]
\centering
\caption{Summary Statistics by Treatment Status}
\begin{tabular}{lcccc}
\toprule
& \multicolumn{2}{c}{Treated States} & \multicolumn{2}{c}{Never-Treated States} \\
& Mean & SD & Mean & SD \\
\midrule
Pension coverage & 0.150 & 0.357 & 0.157 & 0.364 \\
Female & 0.461 & 0.498 & 0.458 & 0.498 \\
Age & 39.2 & 12.4 & 39.8 & 12.5 \\
College graduate & 0.342 & 0.474 & 0.318 & 0.466 \\
Married & 0.504 & 0.500 & 0.521 & 0.500 \\
Small firm (<100) & 0.478 & 0.500 & 0.492 & 0.500 \\
\\
Observations & \multicolumn{2}{c}{159,621} & \multicolumn{2}{c}{437,213} \\
States & \multicolumn{2}{c}{11} & \multicolumn{2}{c}{40} \\
\bottomrule
\multicolumn{5}{l}{\footnotesize Notes: Sample includes private-sector wage/salary workers ages 18--64.} \\
\multicolumn{5}{l}{\footnotesize Treated states implemented auto-IRA mandates by 2024. Weighted by CPS ASEC.}
\end{tabular}
\label{tab:summary}
\end{table}

\subsection{Measurement Considerations}

A critical limitation concerns how the CPS ASEC measures retirement coverage. The survey asks: ``At any time in the last year, were you included in a pension or retirement plan at work?'' This framing explicitly references plans ``at work'' or provided by employers. Workers enrolled in state auto-IRA programs may not answer affirmatively for several reasons.

First, auto-IRAs are technically individual retirement accounts, not employer-sponsored plans. Although contributions are made through payroll deduction, the accounts are administered by the state and remain with workers across jobs. Workers may correctly understand that these are not ``employer'' plans and respond accordingly.

Second, the word ``pension'' carries connotations of defined benefit plans or employer-sponsored 401(k)s with matching contributions. Auto-IRAs lack employer contributions and may not trigger recognition when workers hear ``pension or retirement plan.''

Third, awareness and comprehension vary. Workers may not fully understand what automatic enrollment in OregonSaves or CalSavers means, or may not recall participation when surveyed months later.

Administrative data provide suggestive evidence of this measurement gap. OregonSaves reported over 150,000 actively participating savers by late 2024, yet Oregon's measured pension coverage in the CPS shows no increase---indeed, it declined. This disconnect strongly suggests that CPS-measured coverage fails to capture auto-IRA participation.

This measurement error attenuates estimated treatment effects toward zero. Consequently, our estimates should be interpreted as effects on \textit{self-reported employer-sponsored retirement coverage}, not on total retirement savings participation. The true policy effect on retirement saving access is likely larger than the CPS-based estimates indicate.

\section{Empirical Strategy}

\subsection{Identification}

I exploit the staggered adoption of state auto-IRA mandates across states and over time. Let $G_i \in \{0, 2017, 2018, ..., 2024\}$ denote the first year in which state $i$ is treated, with $G_i = 0$ indicating never-treated states. The identifying assumption is parallel trends: in the absence of treatment, the average change in pension coverage would have been the same for states that adopted mandates and states that did not.

This assumption is not directly testable, but I assess its plausibility by examining pre-treatment outcome dynamics. If treated and control states were on parallel trajectories before treatment, it is more plausible that they would have remained on parallel trajectories absent treatment.

\subsection{Estimation}

I estimate treatment effects using the Callaway and Sant'Anna (2021) estimator, which addresses biases that can arise in two-way fixed effects regressions with staggered treatment timing and heterogeneous treatment effects. The estimator computes group-time average treatment effects:
\begin{equation}
ATT(g,t) = \E[Y_{it}(g) - Y_{it}(0) | G_i = g]
\end{equation}
for each treatment cohort $g$ and time period $t$, where $Y_{it}(g)$ is the potential outcome under treatment starting at time $g$ and $Y_{it}(0)$ is the potential outcome under no treatment.

I use never-treated states as the control group, doubly robust estimation incorporating pre-treatment covariates, and cluster standard errors at the state level to account for within-state serial correlation. I aggregate group-time effects to overall summaries using inverse-variance weighting.

For the event study specification, I define event time $e = t - g$ as years relative to treatment and aggregate group-time effects by event time to trace out dynamic treatment effects.

\subsection{Threats to Validity}

Several threats could bias estimates. First, selection into treatment is not random---states that adopted auto-IRA mandates may differ systematically from those that did not. Treated states tend to be more politically liberal and may have undertaken other policies affecting retirement savings. I address this by controlling for state fixed effects and by examining pre-trends.

Second, as noted above, measurement error in the outcome variable may attenuate estimates. The CPS ASEC asks about ``employer'' retirement plans, which may not capture auto-IRA participation if workers perceive these as individual accounts.

Third, small sample sizes for some treatment cohorts lead to imprecise estimates. The 2024 cohort (four states) entered treatment in the final year of data, providing only one post-treatment observation.

\section{Results}

\subsection{Main Results}

Figure \ref{fig:parallel_trends} shows average pension coverage rates over time for states that would eventually adopt auto-IRA mandates (``Will Be Treated'') versus never-treated states. The two groups follow broadly similar trends in the pre-treatment period, supporting the parallel trends assumption. Both groups show declining pension coverage over 2010--2024, consistent with secular trends toward gig work and non-standard employment.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig2_parallel_trends.png}
\caption{Retirement Plan Coverage: Treated vs. Never-Treated States}
\label{fig:parallel_trends}
\end{figure}

Table \ref{tab:main} presents the main results. The simple aggregate ATT is 0.75 percentage points with a standard error of 1.0 percentage points, yielding a 95\% confidence interval of $[-1.2, 2.7]$ percentage points. This effect is not statistically distinguishable from zero.

\begin{table}[H]
\centering
\caption{Main Results: Effect of Auto-IRA Mandates on Pension Coverage}
\begin{tabular}{lcc}
\toprule
Aggregation & ATT & Std. Error \\
\midrule
Simple average & 0.0075 & 0.010 \\
& [-0.012, 0.027] & \\
\\
Dynamic (post-treatment only) & 0.011 & 0.011 \\
& [-0.011, 0.032] & \\
\bottomrule
\multicolumn{3}{l}{\footnotesize Notes: Callaway-Sant'Anna estimator with never-treated control.} \\
\multicolumn{3}{l}{\footnotesize SEs clustered at state level. 95\% CIs in brackets.}
\end{tabular}
\label{tab:main}
\end{table}

\subsection{Event Study}

Figure \ref{fig:event_study} presents the event study estimates. Pre-treatment coefficients (event times $-5$ through $-2$) are uniformly close to zero and statistically insignificant, providing support for the parallel trends assumption. A joint test of all pre-treatment coefficients fails to reject the null of no differential pre-trends. The reference period is event time $-1$.

Post-treatment coefficients are positive but imprecisely estimated. Effects appear to grow modestly over time, from 0.5 percentage points at event time 0 to 2.2 percentage points at event time +5, though none are statistically significant at conventional levels.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig3_event_study.png}
\caption{Event Study: Effect of Auto-IRA Mandates on Pension Coverage}
\label{fig:event_study}
\end{figure}

\subsection{Heterogeneity Across Treatment Cohorts}

Results vary substantially across treatment cohorts. Table \ref{tab:cohorts} shows group-specific treatment effects by cohort (year of first treatment).

\begin{table}[H]
\centering
\caption{Treatment Effects by Cohort (Percentage Points)}
\begin{tabular}{lccc}
\toprule
Cohort & State(s) & ATT (pp) & Std. Error \\
\midrule
2017 & Oregon & $-2.1$** & 0.4 \\
2018 & Illinois & 2.6** & 0.5 \\
2019 & California & 2.2** & 0.5 \\
2021 & Connecticut & 0.9 & 0.4 \\
2022 & Maryland & 1.7** & 0.4 \\
2023 & Colorado, Virginia & 1.6 & 1.6 \\
2024 & ME, DE, NJ, VT & $-0.7$ & 2.8 \\
\bottomrule
\multicolumn{4}{l}{\footnotesize Notes: ** indicates 95\% simultaneous confidence band excludes zero.}
\end{tabular}
\label{tab:cohorts}
\end{table}

The Illinois and California cohorts show positive and statistically significant effects of approximately 2.2--2.6 percentage points. In contrast, Oregon---the first state to implement an auto-IRA mandate---shows a surprising negative effect of $-2.1$ percentage points.

\textbf{Investigating Oregon's Negative Effect.} Oregon's puzzling result merits scrutiny. I examined the CPS sample for Oregon and find adequate sample sizes: an average of 552 private-sector workers per year, which is comparable to other mid-sized states. The raw data show that Oregon's pension coverage rate declined from 23.0\% in 2010 to 15.2\% in 2017 (the treatment year), and continued declining to 14.9\% by 2024. This downward trend pre-dates treatment, suggesting Oregon-specific factors unrelated to the auto-IRA mandate.

Several explanations are possible. First, Oregon's economy shifted toward sectors with traditionally low retirement coverage (hospitality, retail, gig economy). Second, sampling variation in a small state may exaggerate underlying trends. Third, timing matters: Oregon launched OregonSaves in July 2017, meaning the 2017 CPS (conducted in March) captures essentially no treatment exposure, and early cohorts faced compliance delays.

\textbf{Sensitivity Analysis: Excluding Oregon.} Given concerns about Oregon's outlier result, I conduct a leave-one-out analysis excluding Oregon. Results change markedly: the aggregate ATT rises to 1.57 percentage points (SE = 0.60 pp, $p < 0.01$), which is statistically significant at conventional levels. The event study excluding Oregon shows significant positive effects at event times $+2$ through $+5$, with the effect growing to 3.5 percentage points by year five. This pattern is consistent with programs taking time to scale up and achieve compliance.

This sensitivity analysis suggests the null headline result is driven primarily by Oregon's anomalous negative effect. When this outlier is excluded, the remaining states show a coherent pattern of positive, growing treatment effects.

\subsection{Heterogeneity by Firm Size}

Auto-IRA mandates specifically target employers without existing retirement plans, which are disproportionately small businesses. If the policy mechanism operates as intended, effects should be concentrated among workers at small firms.

I estimate treatment effects separately for workers at small firms (fewer than 100 employees) and large firms (100 or more employees). For small firms, the ATT is 0.55 percentage points (SE = 0.66 pp); for large firms, the ATT is 1.00 percentage points (SE = 1.32 pp). Neither estimate is statistically significant, and the difference between firm sizes is not meaningful.

The lack of differential effects by firm size is puzzling given that mandates target small employers. Several explanations are possible. First, measurement error may be more severe at small firms if workers are less familiar with auto-IRAs. Second, compliance rates at small businesses may lag, muting near-term effects. Third, some small firm workers may already have been covered through spousal plans or individual IRAs, reducing treatment-on-the-treated effects. Fourth, large firms may experience spillover effects if workers change jobs from treated to untreated employers.

\section{Discussion}

The headline finding of this paper---a null effect of 0.75 percentage points---is substantially complicated by heterogeneity across states. Excluding Oregon, whose anomalous negative effect drives the null result, yields a significant positive effect of 1.57 percentage points that grows over time. This suggests that the ``true'' effect of auto-IRA mandates may be positive and policy-relevant, masked by a combination of measurement error and Oregon-specific confounds.

\textbf{Statistical Power.} The standard error of the aggregate ATT (1.0 pp) implies that this study is powered to detect effects of 2.8 percentage points or larger at 80\% power and conventional significance levels. Given a baseline coverage rate of 15.7\%, this corresponds to an 18\% relative increase---a substantial threshold. The study may therefore lack power to detect modest but meaningful effects. The finding that excluding Oregon yields significant results suggests that Oregon's negative effect both inflates the standard error (through increased variance) and attenuates the point estimate.

Several factors may explain the null headline result. First and perhaps most importantly, measurement error in the outcome variable may attenuate estimates. The CPS ASEC asks whether workers are ``included in a pension or retirement plan at work.'' Workers enrolled in auto-IRAs may not consider these employer plans, as they are technically individual retirement accounts facilitated by the state through payroll deduction. If workers do not report auto-IRA participation in response to this question, the estimated effect would be biased toward zero.

Second, most treated states have relatively short post-treatment periods. Oregon, Illinois, and California have 7+, 6+, and 5+ years of post-treatment data respectively, but states adopting in 2021--2024 have much less. Effects may take time to materialize as programs scale up, employer compliance increases, and worker awareness grows.

Third, high opt-out rates could genuinely limit program effectiveness. While administrative data suggest approximately 70\% retention rates among auto-enrolled workers, this still implies 30\% of eligible workers opt out. Combined with incomplete employer compliance (especially among small businesses), actual participation may be lower than headline enrollment figures suggest.

Fourth, Oregon's negative effect is a first-order concern for interpretation. My investigation suggests this reflects state-specific secular trends rather than a genuine adverse policy effect: Oregon's pension coverage was declining before 2017 and the CPS sample is adequate but not large. The sensitivity analysis excluding Oregon provides a more plausible estimate of the policy effect for the remaining states. Additional research using administrative data---particularly enrollment data from OregonSaves itself---would help resolve whether Oregon truly experienced no gains from its program or whether this reflects CPS measurement failure.

\section{Conclusion}

This paper provides the first quasi-experimental evaluation of state automatic IRA mandates using nationally representative survey data. Exploiting the staggered adoption of mandatory programs across eleven states from 2017--2024, I estimate a difference-in-differences model using the Callaway-Sant'Anna estimator.

The headline finding---a null overall effect of 0.75 percentage points---masks substantial heterogeneity. Oregon's puzzling negative effect of $-2.1$ percentage points drives the null aggregate result. Excluding Oregon, the remaining ten treated states show a significant positive effect of 1.57 percentage points that grows to 3.5 percentage points by year five post-treatment. Illinois and California, with the longest post-treatment periods after Oregon, show effects of 2.2--2.6 percentage points. These findings suggest that auto-IRA mandates increase retirement coverage, though the effect takes time to materialize as programs achieve full implementation and compliance.

The null headline result likely reflects two factors: measurement error in the CPS (which asks about ``employer'' plans and may not capture state-facilitated auto-IRAs) and Oregon-specific confounds. The disconnect between OregonSaves' administrative enrollment data (150,000+ active participants) and the CPS-measured coverage decline in Oregon strongly suggests measurement failure.

Several contributions emerge. First, this paper establishes a methodological template for quasi-experimental evaluation of state auto-IRA programs using the Callaway-Sant'Anna estimator. Second, it documents the measurement challenges inherent in using standard surveys to evaluate novel retirement savings vehicles. Third, it provides the first population-based evidence---albeit noisy---on cross-state policy effects, complementing administrative studies of individual programs.

The policy implications are cautiously optimistic. While the headline null result might suggest ineffectiveness, the preponderance of evidence---significant positive effects in Illinois, California, Connecticut, and Maryland; growing effects over time; positive point estimates across most specifications---is consistent with meaningful policy impacts. As programs mature and more appropriate survey instruments become available, future research should provide more definitive evidence. In the meantime, states considering auto-IRA adoption have reason for optimism that these programs can help close the retirement coverage gap.

\section*{Acknowledgements}

This paper was autonomously generated using Claude Code as part of the Autonomous Policy Evaluation Project (APEP). Data were accessed through IPUMS-CPS.

\noindent\textbf{Data Citation:} Sarah Flood, Miriam King, Renae Rodgers, Steven Ruggles, J. Robert Warren, Daniel Backman, Annie Chen, Grace Cooper, Stephanie Richards, Megan Schouweiler, and Michael Westberry. IPUMS CPS: Version 12.0 [dataset]. Minneapolis, MN: IPUMS, 2024.

\newpage

\appendix

\section{Policy Adoption Map}

Figure \ref{fig:map} shows the geographic distribution of auto-IRA mandate adoption as of 2024.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig1_adoption_map.png}
\caption{State Auto-IRA Mandate Adoption, 2017--2024}
\label{fig:map}
\end{figure}

\section{Comparison with Standard TWFE}

For comparison, I also estimate a standard two-way fixed effects specification:
\begin{equation}
Y_{st} = \alpha_s + \gamma_t + \beta \cdot \text{Post}_{st} + \varepsilon_{st}
\end{equation}
where $\alpha_s$ are state fixed effects, $\gamma_t$ are year fixed effects, and $\text{Post}_{st}$ indicates whether state $s$ has implemented an auto-IRA mandate by year $t$.

The TWFE coefficient is 0.0007 (SE = 0.006), essentially zero. This estimate may be biased due to the issues identified by Goodman-Bacon (2021) and others regarding TWFE with staggered treatment timing and heterogeneous effects. The Callaway-Sant'Anna estimates in the main text address these concerns.

\section{Sun-Abraham Event Study}

I also estimate an event study using the Sun and Abraham (2021) estimator implemented in the \texttt{fixest} package. Results are qualitatively similar to the Callaway-Sant'Anna estimates, with pre-treatment coefficients close to zero and positive but imprecisely estimated post-treatment effects.

\section{Robustness Checks}

Table \ref{tab:robustness} presents additional robustness checks.

\begin{table}[H]
\centering
\caption{Robustness Checks}
\begin{tabular}{lcc}
\toprule
Specification & ATT & Std. Error \\
\midrule
\textbf{Main Result} & & \\
Baseline (all states) & 0.0075 & 0.010 \\
\\
\textbf{Sensitivity Analysis} & & \\
Excluding Oregon & 0.0157** & 0.006 \\
\\
\textbf{Heterogeneity by Firm Size} & & \\
Small firms ($<$100 employees) & 0.0055 & 0.007 \\
Large firms ($\geq$100 employees) & 0.0100 & 0.013 \\
\\
\textbf{Alternative Estimators} & & \\
Standard TWFE & 0.0007 & 0.006 \\
Sun-Abraham & 0.0081 & 0.011 \\
\bottomrule
\multicolumn{3}{l}{\footnotesize Notes: ** indicates $p < 0.05$. Callaway-Sant'Anna estimator with} \\
\multicolumn{3}{l}{\footnotesize never-treated control group unless otherwise noted. SEs clustered at state.}
\end{tabular}
\label{tab:robustness}
\end{table}

\section{Power Analysis}

Given the null headline result, it is important to assess whether the study is adequately powered to detect meaningful effects. With a standard error of 1.0 percentage points, the minimum detectable effect (MDE) at 80\% power and $\alpha = 0.05$ is approximately 2.8 percentage points. Given a baseline pension coverage rate of 15.7\%, this corresponds to detecting an 18\% relative increase in coverage.

This power calculation suggests the study can detect large effects but may miss moderate ones. The 95\% confidence interval for the main estimate ($[-1.2, 2.7]$ pp) is consistent with effects ranging from slightly negative to meaningfully positive. The significant result when excluding Oregon (1.6 pp) falls within the detectable range once Oregon's variance contribution is removed.

\section{Systematic Leave-One-Out Analysis}

Table \ref{tab:loo} presents a systematic leave-one-out analysis excluding each treated state in turn. Oregon is by far the most influential state: excluding Oregon increases the estimated ATT from 0.75 pp to 1.57 pp, a change of 0.82 pp. No other state's exclusion produces a comparably large change. Excluding Illinois, California, or Virginia produces slightly lower ATTs, while excluding the other states produces slightly higher ATTs. This analysis confirms that Oregon is uniquely driving the null headline result.

\begin{table}[H]
\centering
\caption{Systematic Leave-One-Out Analysis}
\begin{tabular}{llccc}
\toprule
Excluded State & Cohort & ATT (pp) & SE (pp) & Change from Full \\
\midrule
None (Full Sample) & --- & 0.75 & 1.00 & --- \\
\midrule
Oregon & 2017 & 1.57** & 0.62 & +0.82 \\
Illinois & 2018 & 0.29 & 0.99 & $-0.46$ \\
California & 2019 & 0.46 & 1.12 & $-0.29$ \\
Connecticut & 2021 & 0.74 & 1.06 & $-0.01$ \\
Maryland & 2022 & 0.66 & 1.05 & $-0.09$ \\
Colorado & 2023 & 0.82 & 1.04 & +0.07 \\
Virginia & 2023 & 0.57 & 1.01 & $-0.18$ \\
Delaware & 2024 & 0.95 & 1.03 & +0.20 \\
Maine & 2024 & 0.65 & 0.93 & $-0.10$ \\
New Jersey & 2024 & 0.67 & 1.01 & $-0.08$ \\
Vermont & 2024 & 0.89 & 1.03 & +0.14 \\
\bottomrule
\multicolumn{5}{l}{\footnotesize Notes: ** indicates $p < 0.05$. Callaway-Sant'Anna estimator with} \\
\multicolumn{5}{l}{\footnotesize never-treated control group. ATT and SE in percentage points.}
\end{tabular}
\label{tab:loo}
\end{table}

\section{Baseline Coverage Rate}

The baseline self-reported employer retirement plan coverage rate in this sample is 15.7\%, which may appear low relative to aggregate statistics on workplace retirement access. This figure reflects several sample restrictions and measurement choices:

\begin{enumerate}
\item \textbf{Inclusion vs. access.} The CPS ASEC asks whether workers are \textit{included} in a retirement plan at work, not whether one is \textit{offered}. Many workers at firms with retirement plans choose not to participate, especially younger and lower-wage workers.

\item \textbf{Private sector only.} The sample excludes government workers, who have higher coverage rates.

\item \textbf{All firm sizes.} The sample includes workers at small firms, which have much lower coverage rates than large firms.

\item \textbf{Broad age range.} Including workers 18--64 dilutes coverage rates relative to prime-age samples, as younger workers have lower participation.
\end{enumerate}

For comparison, Bureau of Labor Statistics data show that 72\% of private-sector workers have \textit{access} to retirement plans, but only 54\% participate. The gap between access and participation is largest among workers at small firms and in service industries---precisely the populations targeted by auto-IRA mandates.

\end{document}
