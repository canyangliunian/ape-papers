{
  "paper_id": "apep_0072",
  "scan_date": "2026-02-06T12:40:48.687792+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 6,
  "flags": [
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "01_policy_data.R",
      "lines": [
        29,
        45
      ],
      "evidence": "The manuscript\u2019s identification narrative and counts (10 pre-2012 always-treated; 27 adopting during 2012\u20132019; 14 never-treated) do not match the treatment-timing data encoded in the policy file. For example, California is listed in the paper as a pre-2012 adopter (always-treated) but is coded here as first treated in 2012. Kentucky is coded as adopting in 2000 but is not listed among always-treated states in the manuscript\u2019s pre-2012 list. Virginia is coded as 2010 here but is described in the appendix table as a 2012 adopter cohort in the manuscript. These discrepancies can change which states identify ATT, which cohorts exist, and the treated/control composition\u2014directly affecting the core DiD estimates.: telehealth_parity <- tribble(\n  ~state, ~state_fips, ~first_parity_law_year, ~parity_type, ~notes,\n\n  # Pre-2010 adopters (always treated in our sample)\n  \"California\",    \"06\", 2012, \"coverage\", \"AB 415 effective 2012; earlier Medicaid rules\",\n  ...\n  \"Kentucky\",      \"21\", 2000, \"coverage\", \"HB 737 effective 2000\",\n  ...\n  \"Virginia\",      \"51\", 2010, \"coverage\", \"HB 1280 effective 2010\",\n  ...\n)",
      "confidence": 0.9
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_policy_data.R",
      "lines": [
        29,
        120
      ],
      "evidence": "The key treatment timing dataset is manually constructed in-code with no reproducible ingest from CCHPCA/NCSL or archived source tables. While manual coding can be legitimate, the absence of a machine-readable provenance trail (download script, archived input files, or citations tied to each state-year) makes it difficult to audit correctness\u2014especially given the observed mismatches with the manuscript\u2019s stated adoption timing.: telehealth_parity <- tribble(\n  ~state, ~state_fips, ~first_parity_law_year, ~parity_type, ~notes,\n  ...\n  \"Washington\",    \"53\", NA, \"none\", \"No parity law through 2019\",\n  \"Wisconsin\",     \"55\", NA, \"none\", \"No parity law through 2019\",\n  \"Wyoming\",       \"56\", NA, \"none\", \"No parity law through 2019\"\n)\n...\nwrite_csv(telehealth_parity, \"../data/telehealth_parity_laws.csv\")",
      "confidence": 0.8
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "04_figures.R",
      "lines": [
        132,
        146
      ],
      "evidence": "Figure 4\u2019s cohort-specific ATTs/SEs are hard-coded rather than extracted from saved model objects (e.g., cs_results.rds). This creates a risk of transcription error or inconsistency with the actual estimation output, and it prevents readers/auditors from verifying that the plotted values correspond to the reported estimates in the manuscript tables.: cohort_effects <- tibble(\n  cohort = c(2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019),\n  att = c(-0.6944, -0.1066, -0.4149, -0.0765, -0.4484, -3.6072, -2.0938, -1.3357),\n  se = c(0.6956, 0.5473, 0.3668, 0.6966, 0.9269, 0.4000, 1.6760, 0.3540)\n) %>%\n  mutate(\n    ci_low = att - 1.96 * se,\n    ci_high = att + 1.96 * se,\n    significant = ci_high < 0 | ci_low > 0\n  )",
      "confidence": 0.95
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "04_figures.R",
      "lines": [
        95,
        104
      ],
      "evidence": "The reference period\u2019s ATT and SE are forcibly set to exactly 0 for plotting. Setting the reference ATT to 0 is standard normalization, but setting its SE to 0 is a presentation choice that can visually understate uncertainty at the omitted period (and can look like an exact estimate). Prefer leaving SE missing/NA for the reference period or clearly annotating that it is normalized.: event_plot_data <- event_study %>%\n  mutate(\n    att = ifelse(event_time == -1, 0, att),\n    se = ifelse(event_time == -1, 0, se),\n    ci_low = ifelse(event_time == -1, 0, att - 1.96 * se),\n    ci_high = ifelse(event_time == -1, 0, att + 1.96 * se)\n  )",
      "confidence": 0.75
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "02_fetch_nsduh.R",
      "lines": [
        170,
        210
      ],
      "evidence": "This script explicitly discusses constructing sample/supplemental data due to API limitations and creates a template with placeholders. It does not actually generate random/simulated outcome values, but the presence of a \u2018methodology demonstration\u2019 path raises an integrity risk if such placeholders were ever used in analysis outputs. In the provided analysis pipeline, the main outcome is fetched from CDC and does not appear to use these placeholders.: # Construct Sample Data for Methodology Demonstration\n# ...\n# THIS IS FOR METHODOLOGY DEMONSTRATION ONLY\n# The final paper MUST use actual NSDUH/BRFSS data\n...\nnsduh_template <- expand_grid(\n  state = state_lookup$state,\n  year = 2008:2019\n) %>%\n  mutate(\n    # Placeholder - to be filled with actual SAMHSA data\n    mh_treatment_pct = NA_real_,\n    ami_pct = NA_real_,\n    ...\n  )",
      "confidence": 0.7
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "04_figures.R",
      "lines": [
        1,
        20
      ],
      "evidence": "The script loads cs_results.rds (which presumably contains the cohort-specific estimates), but then does not use it for Figure 4 and instead plots a manually-entered subset. This pattern is consistent with selective manual curation risk (even if unintentional). A reproducible approach would programmatically extract cohort ATTs/SEs from cs_results$by_cohort and plot all cohorts returned by the estimator.: cs_results <- readRDS(\"../data/cs_results.rds\")\nanalysis_data <- read_csv(\"../data/analysis_panel.csv\", show_col_types = FALSE)\nevent_study <- read_csv(\"../data/event_study_estimates.csv\", show_col_types = FALSE)\n...\n# Extract cohort effects from results\ncohort_effects <- tibble(\n  cohort = c(2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019),\n  att = c(...),\n  se = c(...)\n)",
      "confidence": 0.8
    }
  ],
  "file_verdicts": [
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_fetch_nsduh.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_fetch_outcome_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "01_policy_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "04_figures.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 2,
      "MEDIUM": 3,
      "LOW": 1
    },
    "one_liner": "method mismatch; hard-coded results",
    "executive_summary": "The treatment-timing information encoded in `01_policy_data.R` is inconsistent with the manuscript\u2019s identification narrative and reported counts (10 always-treated pre\u20112012, 27 adopters during 2012\u20132019, 14 never-treated), indicating that units are being classified into cohorts differently than described. In `04_figures.R`, the cohort-specific ATT and standard error values used for Figure 4 are hard-coded instead of being pulled from saved estimation outputs (e.g., `cs_results.rds`), so the plotted results may not correspond to the models actually run and are vulnerable to transcription or selective-reporting errors.",
    "top_issues": [
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript\u2019s identification narrative and counts (10 ...",
        "file": "01_policy_data.R",
        "lines": [
          29,
          45
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0072/code/01_policy_data.R#L29-L45"
      },
      {
        "category": "HARD_CODED_RESULTS",
        "severity": "HIGH",
        "short": "Figure 4\u2019s cohort-specific ATTs/SEs are hard-coded rather...",
        "file": "04_figures.R",
        "lines": [
          132,
          146
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0072/code/04_figures.R#L132-L146"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0072_scan.json"
  },
  "error": null
}