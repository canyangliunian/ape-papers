{
  "paper_id": "apep_0061",
  "scan_date": "2026-02-06T12:38:32.132032+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "CLEAN",
  "files_scanned": 5,
  "flags": [
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "03_main_analysis.R",
      "lines": [
        196,
        197
      ],
      "evidence": "The main ATT/SE values shown on the event-study figure are hard-coded literals rather than being constructed from the computed objects (e.g., att_simple$overall.att and att_simple$overall.se). This creates a results-integrity risk: if the underlying estimates change (data refresh, code change, bootstrap randomness, different control group), the plot label can silently become inconsistent with the actual computed results.: annotate(\"text\", x = min(es_data$event_time) + 1, y = max(es_data$ci_upper) * 0.9,\n           label = paste0(\"Main ATT: 1.02 (SE: 1.16)\"),  # Fixed to match tables\n           hjust = 0, size = 3.5, fontface = \"italic\")",
      "confidence": 0.9
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "LOW",
      "file": "paper.tex",
      "lines": [
        285,
        286,
        287,
        560,
        561,
        562
      ],
      "evidence": "Manuscript tables contain numeric results typed directly into LaTeX. This is common in academic writing, but it is an integrity risk unless tables are programmatically produced from model outputs (or at minimum cross-checked). Given the codebase already exports CSV tables (e.g., table2_main_results.csv, table3_robustness.csv), a stronger integrity pattern would be to auto-generate LaTeX tables from those outputs to eliminate transcription drift.: C-S (never-treated controls) & 1.02 & 1.16 & [-1.26, 3.30] & 0.38 & 49 & 490 \\\\\nSun-Abraham & -0.91 & 1.14 & [-3.14, 1.32] & 0.43 & 49 & 490 \\\\\n...\nMain (corrected timing) & 1.02 & 1.16 & [-1.26, 3.30] & 0.38 & No & 490 \\\\\nStrong mandates (strength $\\geq$ 3)$^a$ & 4.23 & 0.76 & [2.73, 5.73] & $<$0.01 & Yes & 330 \\\\",
      "confidence": 0.7
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        140,
        141,
        142,
        143,
        144,
        145,
        146,
        147,
        148,
        149,
        150,
        151,
        152,
        153,
        154,
        155,
        156,
        157,
        158,
        159,
        160,
        161,
        162,
        163,
        164,
        165,
        166,
        167,
        168,
        169
      ],
      "evidence": "The key treatment dataset (state mandate years, effective months, and component indicators) is manually encoded in the script with no machine-verifiable source file, citation mapping, or scraping/ingestion step. The manuscript cites multiple sources (Dyslegia.com, State of Dyslexia, Education Week, statutes), but the code does not document record-level provenance (e.g., URL/statute bill ID per state) or include a reproducible pipeline to rebuild this table from raw sources. This makes the treatment timing correction and bundled/strength classifications difficult to audit independently and increases risk of inadvertent (or untracked) discretionary coding choices.: dyslexia_mandates <- tribble(\n  ~state_abbr, ~mandate_year, ~effective_month, ~mandate_type, ~universal, ~intervention_req, ~teacher_training, ~funding,\n  # Early adopters (pre-2015)\n  \"TX\", 1995, 9, \"screening\", TRUE, TRUE, TRUE, TRUE,  # Texas was early, strengthened 2019\n  \"VA\", 2010, 7, \"screening\", FALSE, FALSE, FALSE, FALSE,  # Initial law, strengthened 2023\n  ...\n  \"CA\", 2023, 7, \"screening\", TRUE, TRUE, FALSE, FALSE  # Implementation 2025-26\n)",
      "confidence": 0.85
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        270,
        271,
        272,
        273,
        274,
        275,
        276,
        277
      ],
      "evidence": "Concurrent-policy controls (Science of Reading laws and retention policies) are also manually encoded without a raw-source import or record-level provenance. The manuscript frames these primarily as robustness/sample restriction information, so the integrity risk is lower than for treatment timing, but auditability is still limited.: sor_laws <- tribble(\n  ~state_abbr, ~sor_year,\n  \"MS\", 2013,  # Literacy-Based Promotion Act\n  \"FL\", 2013,  # Just Read, Florida! strengthened\n  ...\n)\n\nretention_laws <- tribble(\n  ~state_abbr, ~retention_year,\n  \"FL\", 2002,\n  ...\n)",
      "confidence": 0.75
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "LOW",
      "file": "04_robustness.R",
      "lines": [
        367,
        368
      ],
      "evidence": "The manuscript and code elsewhere indicate Texas is excluded from causal estimation (49 clusters). Here the inference documentation prints '50' clusters unconditionally. This appears to be a reporting bug rather than an estimation bug (att_gt is clustered by state_id on whatever data subset is passed), but it can mislead readers about inference assumptions and effective cluster count.: cat(\"  Number of clusters: 50\\n\")\ncat(\"\\nNote: With 50 clusters, asymptotic approximations are reasonable,\\n\")",
      "confidence": 0.8
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        74,
        75,
        76,
        77
      ],
      "evidence": "Hard-coded local absolute paths reduce reproducibility across machines/environments and can lead to silent use of stale artifacts if directory contents already exist. This is not fabrication, but it is a replication-integrity weakness (better: project-relative paths via here::here() or an environment variable).: base_dir <- \"/Users/dyanag/auto-policy-evals/output/paper_78\"\ndata_dir <- file.path(base_dir, \"data\")\nfig_dir <- file.path(base_dir, \"figures\")\ntab_dir <- file.path(base_dir, \"tables\")",
      "confidence": 0.85
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_descriptives.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "CLEAN",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 0,
      "MEDIUM": 2,
      "LOW": 4
    },
    "one_liner": "Minor issues only",
    "executive_summary": "Minor code quality issues detected, but no evidence of data fabrication or manipulation.",
    "top_issues": [],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0061_scan.json"
  },
  "error": null
}