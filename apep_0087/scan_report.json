{
  "paper_id": "apep_0087",
  "scan_date": "2026-02-06T12:44:06.381771+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 14,
  "flags": [
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        55,
        120
      ],
      "evidence": "The script explicitly generates a synthetic ACS-like microdataset using random draws (sample(), rnorm(), runif()) and then re-generates the key outcome (ESR / NILF) as a function of occupation automation categories plus noise. This is 'fabricated' data in the sense of simulation, but the manuscript repeatedly and clearly discloses that the analysis uses synthetic microdata for methodological demonstration, so this is not an integrity violation by itself.: set.seed(42)\nn <- 100000\n\npums_data <- tibble(\n  PWGTP = sample(50:200, n, replace = TRUE),\n  AGEP = sample(55:70, n, replace = TRUE),\n  ...\n  PINCP = pmax(0, rnorm(n, 50000, 30000)),\n  ...\n)\n\n... \n nilf_prob = pmax(0.10, pmin(0.80,\n   0.25 + auto_effect + edu_effect + age_effect + rnorm(n, 0, 0.1))),\n ESR = ifelse(runif(n) < nilf_prob, \"6\",\n              sample(c(\"1\", \"2\", \"3\"), n, replace = TRUE, prob = c(0.85, 0.10, 0.05)))",
      "confidence": 0.95
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "02_clean_data.R",
      "lines": [
        175,
        205
      ],
      "evidence": "Adds random noise to a 'detailed' automation exposure variable. If this variable were later used in estimation, it would introduce additional simulation elements. In the provided analysis scripts, the main treatment uses automation_exposure and high_automation (not the noisy variable), so impact appears limited; still worth confirming it is not used downstream in any truncated/omitted scripts.: automation_tercile = ntile(automation_exposure, 3),\nhigh_automation = as.integer(automation_tercile == 3),\n\n# Add some noise for within-group variation (for demonstration)\n# In production, would use actual O*NET scores at detailed SOC level\nautomation_exposure_detailed = automation_exposure + rnorm(n(), 0, 0.05)",
      "confidence": 0.75
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        18,
        52
      ],
      "evidence": "Automation exposure values are hard-coded rather than computed from an included source file of Frey-Osborne/Autor-Dorn measures or a documented mapping procedure. However, the manuscript describes this as a constructed/calibrated index for a synthetic-data methodological demonstration, so hard-coding can be acceptable\u2014still, replication would be stronger if the mapping algorithm and its sources were provided or the table were generated from a cited dataset.: automation_scores <- tribble(\n  ~occ_group, ~occ_min, ~occ_max, ~automation_exposure, ~description,\n  \"Management\", 10, 440, 0.15, \"Low automation risk\",\n  ...\n  \"Office/Admin Support\", 5000, 5940, 0.85, \"Very high automation risk\",\n  ...\n)",
      "confidence": 0.8
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "04_robustness.R",
      "lines": [
        30,
        45
      ],
      "evidence": "The lower confidence limit for the E-value calculation is hard-coded as 1.01 rather than being derived from the estimated uncertainty (e.g., from a CI for the risk ratio implied by the AIPW/OLS estimate). This can materially change the reported E-value robustness summary and should be tied to an actual CI (or clearly labeled as an illustrative placeholder).: evalue_results <- tryCatch({\n  EValue::evalue(est = risk_ratio, lo = 1.01)  # assuming RR > 1\n}, error = function(e) {\n  ...\n})",
      "confidence": 0.7
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        53,
        58
      ],
      "evidence": "No code is provided to actually fetch ACS PUMS microdata; instead a synthetic dataset is generated. This is explicitly disclosed in both code comments and the manuscript, so provenance is not 'missing' for the analysis actually run, but real-data provenance/replication is not implemented.: # NOTE: Census PUMS API is unreliable with frequent timeouts.\n# For this demonstration, we create realistic synthetic data calibrated to\n# ACS PUMS statistics. For publication, download PUMS microdata directly from:\n# https://www.census.gov/programs-surveys/acs/microdata/access.html",
      "confidence": 0.9
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "repository (multiple files)",
      "lines": [
        1,
        50
      ],
      "evidence": "The codebase contains two substantively different projects under the same paper number/namespace: (a) an automation/NILF synthetic ACS AIPW analysis (01_fetch_data.R, 02_clean_data.R, 03_main_analysis.R, 04_robustness.R, 05_figures.R, 06_tables.R), and (b) a 'Price of Distance' cannabis-dispensary/FARS fatal crash analysis (01_fetch_fars.R, 04_build_analysis_data.R, 05_main_analysis.R, 06_figures.R, 07_tables.R). This creates a serious auditability risk: it is easy to run the wrong pipeline, overwrite shared filenames (e.g., analysis_data.rds, regression_results.rds), and produce outputs unrelated to the manuscript. The mismatch is not a statistical quibble; it is an integrity/traceability problem requiring repo cleanup or strict separation of folders/output paths.: 01_fetch_fars.R / 04_build_analysis_data.R / 05_main_analysis.R / 06_figures.R / 07_tables.R all begin with:\n\"Paper 110: The Price of Distance\" and implement FARS + dispensary-distance analysis,\nwhile paper.tex is \"Automation Exposure and Older Worker Labor Force Nonparticipation\" using ACS-like microdata and doubly robust estimation.",
      "confidence": 0.95
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        96,
        119
      ],
      "evidence": "The synthetic data generation explicitly bakes in a positive relationship between automation exposure and NILF via 'auto_effect', which can mechanically induce the main finding. The manuscript frames results as a methodological demonstration using synthetic microdata, so this can be acceptable if presented as illustrative. However, readers could misinterpret the reported 0.9pp 'association' as empirically discovered rather than embedded by construction; the paper should very clearly state whether the effect size was targeted/calibrated versus emerging from calibration constraints.: # Introduce realistic correlation between automation exposure and NILF\n# Higher automation \u2192 higher NILF probability\n...\n# Base probability + effects\nnilf_prob = pmax(0.10, pmin(0.80,\n  0.25 + auto_effect + edu_effect + age_effect + rnorm(n, 0, 0.1))),\n# Regenerate ESR based on calculated probability\nESR = ifelse(runif(n) < nilf_prob, \"6\", ...)",
      "confidence": 0.85
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "06_tables.R",
      "lines": [
        150,
        215
      ],
      "evidence": "Several robustness rows are reported without standard errors (shown as '\u2014'), even though the manuscript describes bootstrap inference. This is not fabrication, but it is a reporting asymmetry: some estimates have uncertainty and others do not, which can inadvertently (or intentionally) privilege point estimates. Either compute SEs consistently or label these as non-inferential descriptive checks.: robustness_table <- tribble(\n  ...\n  \"Excluding Disability\", \n    sprintf(\"%.4f\", robustness_results$subsamples$no_disability), \n    \"\u2014\", \n    \"Sample restriction\",\n  \"Men Only\", \n    sprintf(\"%.4f\", robustness_results$subsamples$men), \n    \"\u2014\", \n    \"Sample restriction\",\n  \"Women Only\", \n    sprintf(\"%.4f\", robustness_results$subsamples$women), \n    \"\u2014\", \n    \"Sample restriction\"\n)",
      "confidence": 0.75
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_fars.R",
      "verdict": "CLEAN"
    },
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "07_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_build_analysis_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_compute_driving_times.py",
      "verdict": "CLEAN"
    },
    {
      "file": "02_fetch_dispensaries.py",
      "verdict": "CLEAN"
    },
    {
      "file": "repository (multiple files)",
      "verdict": "SUSPICIOUS"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 1,
      "MEDIUM": 3,
      "LOW": 4
    },
    "one_liner": "method mismatch",
    "executive_summary": "The repository mixes two substantively different projects under the same paper ID/namespace: one set of scripts implements an automation/NILF synthetic ACS AIPW pipeline (e.g., `01_fetch_data.R`, `02_clean_data.R`, `03_main_analysis...`), while other files correspond to a different research workflow. This methodological mismatch makes it unclear which code actually produces the paper\u2019s reported results and indicates the codebase is not a coherent, single analysis tied to apep_0087.",
    "top_issues": [
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The codebase contains two substantively different project...",
        "file": "repository (multiple files)",
        "lines": [
          1,
          50
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0087/code/repository (multiple files)#L1-L50"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0087_scan.json"
  },
  "error": null
}