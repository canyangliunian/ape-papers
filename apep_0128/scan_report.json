{
  "paper_id": "apep_0128",
  "scan_date": "2026-02-06T12:48:21.694909+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "CLEAN",
  "files_scanned": 7,
  "flags": [
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "06_tables.R",
      "lines": [
        155,
        182
      ],
      "evidence": "The LaTeX placebo table generated by code explicitly truncates to the top 10 placebo countries by |post_gap|, whereas the manuscript's Table \\ref{tab:placebo} displays all 16 countries (Netherlands + 15 donors) and discusses Netherlands rank '11th out of 16'. If the PDF table in the paper indeed lists all countries, it was not produced by this script as written (or was manually edited afterward). This is a selective-reporting / reproducibility discrepancy: readers cannot reproduce the manuscript's placebo table from the provided table-generation code without modifying/removing the head(10) filter.: placebo <- robustness$placebo_summary %>%\n  arrange(desc(abs(post_gap))) %>%\n  head(10)\n...\n# Add Netherlands row\nnl_row <- data.frame(\n  country = \"Netherlands (Treated)\",\n  pre_rmse = estimates$pre_fit$rmse,\n  post_gap = estimates$post_effect$att,\n  ratio = abs(estimates$post_effect$att) / estimates$pre_fit$rmse\n)\n...\n# Only top 10 countries by $|$Post-Gap$|$ shown.",
      "confidence": 0.74
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        14,
        21
      ],
      "evidence": "Data access depends on a locally supplied FRED API key (environment variable) and does not include a project-provided .env template, credential setup instructions beyond the stop message, or a fallback data snapshot. This is not evidence of impropriety, but it weakens end-to-end reproducibility for auditors who cannot fetch the raw series. Consider adding a documented setup step and/or a frozen raw-data export with hashes.: fred_api_key <- Sys.getenv(\"FRED_API_KEY\")\n\nif (fred_api_key == \"\") {\n  stop(\"FRED_API_KEY environment variable not set. Please set it in .env file.\")\n}",
      "confidence": 0.62
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "CLEAN",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 0,
      "MEDIUM": 1,
      "LOW": 1
    },
    "one_liner": "Minor issues only",
    "executive_summary": "Minor code quality issues detected, but no evidence of data fabrication or manipulation.",
    "top_issues": [],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0128_scan.json"
  },
  "error": null
}