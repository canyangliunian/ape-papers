{
  "paper_id": "apep_0164",
  "scan_date": "2026-02-06T12:55:23.617803+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 7,
  "flags": [
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "01_fetch_data.R",
      "lines": [
        16,
        17,
        19
      ],
      "evidence": "The manuscript states the ACS 2020 1-year PUMS is excluded due to non-standard data collection, but the fetch script requests all years 2017\u20132024 inclusive (including 2020) and there is no subsequent explicit drop of 2020 in the provided cleaning script. If 2020 is indeed included in the analysis datasets, this is a substantive divergence from the stated sample definition and could affect both descriptive trends and DiD estimates.: years <- 2017:2024\n...\nfor (yr in years) {\n  vars <- if (yr >= 2023) vars_2023plus else vars_pre2023\n  result <- fetch_acs_pums(yr, vars)\n  if (!is.null(result)) {\n    all_data[[as.character(yr)]] <- result\n  }\n  Sys.sleep(1)\n}",
      "confidence": 0.85
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "02_clean_data.R",
      "lines": [
        54,
        55,
        56
      ],
      "evidence": "The manuscript describes a treatment-coding rule: a state is treated in survey year t if the effective date is on/before July 1 of year t (half-year exposure rule). The code instead uses a coarse annual rule (year >= adopt_year) with no adoption month/day and no July 1 threshold. This is not a cosmetic difference: it changes cohort assignment (G_s) and the treated indicator in early adoption years, and it is directly related to the paper\u2019s attenuation discussion.: treated = as.integer(!is.na(adopt_year) & adopt_year <= max_sample_year & year >= adopt_year),\nfirst_treat = ifelse(is.na(adopt_year) | adopt_year > max_sample_year, 0, adopt_year),",
      "confidence": 0.9
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "04_robustness.R",
      "lines": [
        612,
        613,
        614,
        615,
        616,
        617,
        618,
        619
      ],
      "evidence": "A key auxiliary dataset (KFF disenrollment/unwinding intensity) is hard-coded in the script as \u2018approximate rates\u2019 with no raw-file import, URL retrieval, versioning, or reproducible scraping. Even though the comments cite KFF, this creates provenance risk: readers cannot verify the exact numbers used or reproduce the index. This matters because the paper\u2019s narrative hinges on unwinding confounding, and this section performs correlation/interaction analyses using these values.: # KFF state-level disenrollment data (approximate rates from public reporting)\n# Source: KFF Medicaid Enrollment and Unwinding Tracker, 2024\n# These are approximate total disenrollment rates (% of enrollment) through Q1 2024\nkff_disenroll <- data.frame(\n  state_fips = c(1,2,4,5,6,8,9,10,11,12,13,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,44,45,46,47,48,49,50,51,53,54,55,56),\n  disenroll_rate = c(0.45, 0.30, 0.40, 0.35, 0.25, 0.35, 0.30, 0.35, 0.20, 0.50, 0.45, 0.25, 0.35, 0.30, 0.40, 0.35, 0.35, 0.40, 0.45, 0.25, 0.30, 0.20, 0.35, 0.25, 0.45, 0.40, 0.30, 0.35, 0.40, 0.25, 0.30, 0.40, 0.25, 0.40, 0.30, 0.35, 0.45, 0.30, 0.30, 0.25, 0.45, 0.35, 0.45, 0.50, 0.35, 0.20, 0.35, 0.30, 0.40, 0.35, 0.30),\n  stringsAsFactors = FALSE\n)",
      "confidence": 0.8
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "04_robustness.R",
      "lines": [
        612,
        621,
        650
      ],
      "evidence": "The unwinding-intensity analysis is driven by a hard-coded vector of state disenrollment rates, which then directly enters regression interaction terms. This is not \u2018hard-coded results\u2019 in the narrow sense (not coefficients), but it is hard-coded quantitative input that can materially affect reported robustness/mechanism results and should be sourced from a file/API with documentation and a checksum/version.: kff_disenroll <- data.frame(\n  state_fips = c(...),\n  disenroll_rate = c(0.45, 0.30, 0.40, ...)\n)\n...\nstate_year_pp_unwind <- state_year_pp %>%\n  left_join(kff_disenroll, by = \"state_fips\") %>%\n  mutate(\n    high_disenroll = as.integer(disenroll_rate > median(disenroll_rate, na.rm = TRUE)),\n    treated_x_high = treated * high_disenroll\n  )\n...\ntwfe_unwind_int <- feols(\n  medicaid_rate ~ treated * high_disenroll | state_fips + year,\n  data = state_year_pp_unwind %>% filter(year <= 2019 | year >= 2023),\n  weights = ~total_weight,\n  cluster = ~state_fips\n)",
      "confidence": 0.75
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "06_tables.R",
      "lines": [
        14,
        15,
        16,
        17,
        18,
        19
      ],
      "evidence": "Tables are built from precomputed serialized objects (main_results.rds / robustness_results.rds). This is fine if those objects are generated by scripts in the repo (e.g., 03_main_analysis.R / 04_robustness.R), but the main-analysis script is truncated in the provided materials, so the full provenance of every stored estimate cannot be verified in this audit snapshot.: results <- readRDS(file.path(data_dir, \"main_results.rds\"))\nrobustness <- tryCatch(readRDS(file.path(data_dir, \"robustness_results.rds\")),\n                       error = function(e) NULL)",
      "confidence": 0.55
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 2,
      "MEDIUM": 2,
      "LOW": 1
    },
    "one_liner": "method mismatch",
    "executive_summary": "The data retrieval code pulls ACS 1-year PUMS for every year 2017\u20132024, including 2020, contradicting the manuscript\u2019s claim that the 2020 1-year PUMS is excluded due to non-standard collection and showing no downstream step that removes it. The treatment construction in the cleaning script also does not implement the stated half-year exposure rule (treated in survey year *t* if the policy effective date is on/before July 1 of *t*), instead using a coarse year-level assignment that can misclassify treatment timing around mid-year effective dates.",
    "top_issues": [
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript states the ACS 2020 1-year PUMS is exclude...",
        "file": "01_fetch_data.R",
        "lines": [
          16,
          17
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0164/code/01_fetch_data.R#L16-L19"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The manuscript describes a treatment-coding rule",
        "file": "02_clean_data.R",
        "lines": [
          54,
          55
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0164/code/02_clean_data.R#L54-L56"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0164_scan.json"
  },
  "error": null
}