{
  "paper_id": "apep_0194",
  "scan_date": "2026-02-06T13:02:01.110674+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "CLEAN",
  "files_scanned": 7,
  "flags": [
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        89,
        90
      ],
      "evidence": "A global random seed is set when loading packages. This is not inherently problematic, but it can unintentionally make any later random procedures (e.g., randomization inference) appear more \"deterministic\" than intended if scripts are re-run in different orders. In this codebase, randomization inference explicitly sets its own seed later, so the integrity risk is low; still, best practice is to set seeds only in scripts that actually use randomness.: set.seed(42)\noptions(scipen = 999)",
      "confidence": 0.78
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        109,
        118,
        163,
        187
      ],
      "evidence": "The manuscript describes using quarterly state-level QCEW outcomes. The fetch script uses a quarterly API endpoint, but filters `agglvl_code %in% c(50, 54, 55, 56, 57, 58)` without documenting what each agglvl_code corresponds to. If any of these include non-state geographies (e.g., county/MSA aggregates or other non-state levels), the resulting `qcew_raw.csv` could mix geographic aggregation levels. This is a provenance/traceability issue because it affects whether the panel truly is \u201cstate \u00d7 quarter \u00d7 industry\u201d as claimed. Recommendation: restrict explicitly to the state aggregation level used by QCEW (and assert the expected `area_fips` pattern), and log/validate unique `agglvl_code` and `area_fips` formats retained.: # QCEW data from BLS bulk download: annual averages by state \u00d7 industry\n# ...\nfetch_qcew_bulk <- function(year) {\n  url <- sprintf(\"https://data.bls.gov/cew/data/files/%d/csv/%d_annual_by_area.zip\", year, year)\n  ...\n}\n...\nfor (yr in years) {\n  for (qtr in 1:4) {\n    for (i in seq_along(industries)) {\n      ...\n      url <- sprintf(\n        \"https://data.bls.gov/cew/data/api/%d/%d/industry/%s.csv\",\n        yr, qtr, ind\n      )\n      ...\n      df <- read_csv(txt, show_col_types = FALSE) %>%\n        filter(own_code == 5,\n               agglvl_code %in% c(50, 54, 55, 56, 57, 58)) %>%\n        mutate(year = yr, quarter = qtr, industry = ind_name,\n               naics = ind)\n      ...\n    }\n  }\n}",
      "confidence": 0.67
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "04_robustness.R",
      "lines": [
        68,
        83,
        120,
        149
      ],
      "evidence": "The manuscript reports Fisher randomization inference with 1,000 permutations (e.g., figure notes), while Table/footnote text elsewhere states 500 permutations (e.g., robustness table notes in the LaTeX). The code uses `n_perms <- 1000`. This inconsistency is not a computation error, but it is a method/reporting mismatch that can change reported RI p-values (especially with few treated units). Recommendation: harmonize manuscript/table notes with the implemented number of permutations (or parameterize it and export it into the table notes programmatically).: n_perms <- 1000\n...\nset.seed(12345)  # Reproducibility seed for randomization inference\n...\nri_results <- list(\n  actual_beta = actual_beta,\n  perm_betas = perm_betas,\n  ri_pvalue = ri_pvalue\n)",
      "confidence": 0.84
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "MEDIUM",
      "file": "03_main_analysis.R",
      "lines": [
        98,
        111,
        115,
        121
      ],
      "evidence": "Before running Callaway\u2013Sant\u2019Anna (att_gt), the code drops any state with zero within-state outcome variation (sd_y == 0) for the given industry. This can be legitimate (e.g., if a state\u2019s series is fully missing/suppressed or truly constant), but it can also introduce selection if constancy is caused by disclosure suppression patterns correlated with treatment status (or by post-processing). Given the manuscript emphasizes suppression for NAICS 5112, this filter could differentially drop small states and implicitly reweight treatment/control composition. Recommendation: (i) report how many states are dropped per industry and whether they are disproportionately treated, (ii) verify that \u201cconstant\u201d states are constant due to missingness/suppression rather than real constancy, and (iii) consider restricting to states with sufficient observed quarters rather than sd-based dropping.: # Drop states with all NA or constant outcomes\nstate_var <- df %>%\n  group_by(state_id) %>%\n  summarize(sd_y = sd(log_emp, na.rm = TRUE), .groups = \"drop\") %>%\n  filter(!is.na(sd_y), sd_y > 0)\n\ndf <- df %>% filter(state_id %in% state_var$state_id)",
      "confidence": 0.74
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        58,
        72,
        75,
        103
      ],
      "evidence": "The script first constructs an incorrect FIPS mapping using `match(state.abb, state.abb)` (which yields 1..50, not FIPS), and then overwrites it with a correct hard-coded mapping. Because it is overwritten before being written to disk, this likely does not affect results, but it is error-prone and creates ambiguity for auditors about which mapping is used. Recommendation: remove the incorrect block to avoid accidental use in refactors.: # State FIPS codes for merging\nstate_fips <- tibble(\n  state_abbr = state.abb,\n  state_name = state.name,\n  fips = sprintf(\"%02d\", match(state.abb, state.abb))\n) %>%\n  # Add DC\n  bind_rows(tibble(state_abbr = \"DC\", state_name = \"District of Columbia\", fips = \"11\"))\n\n# Proper FIPS codes\nstate_fips <- tibble(\n  state_abbr = c(\"AL\",\"AK\",...,\"DC\"),\n  ...,\n  fips = c(\"01\",\"02\",...,\"11\")\n)",
      "confidence": 0.86
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "LOW",
      "file": "03_main_analysis.R",
      "lines": [
        156,
        168,
        178
      ],
      "evidence": "The TWFE event-study construction assigns never-treated units `rel_time_binned = 0` (conceptually \u201cat treatment\u201d), and then interacts `i(rel_time_binned, treated_state, ...)`. This can work mechanically (controls contribute only to FE and baseline levels), but it is a nonstandard encoding that can be confusing and easy to misinterpret when parsing coefficient names or plotting. This is not fabrication, but it is a mild implementation/reporting risk. Recommendation: keep never-treated `rel_time_binned = NA` and rely on fixest\u2019s interaction structure (or explicitly define event time only for treated and use standard TWFE ES coding).: df <- qcew_panel %>%\n  filter(industry == ind, treated_state == 1) %>%\n  mutate(\n    # Bin relative time: cap at [-8, +8] quarters\n    rel_time_binned = pmin(pmax(rel_time, -8), 8)\n  ) %>%\n  bind_rows(\n    qcew_panel %>%\n      filter(industry == ind, treated_state == 0) %>%\n      mutate(rel_time_binned = 0)  # Never-treated are always \"at treatment\"\n  )\n...\nfit_es <- feols(\n  log_emp ~ i(rel_time_binned, treated_state, ref = -1) | state_f + time_f,\n  data = df %>% filter(!is.na(rel_time_binned) | treated_state == 0),\n  cluster = ~state_f\n)",
      "confidence": 0.7
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "CLEAN",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 0,
      "MEDIUM": 3,
      "LOW": 3
    },
    "one_liner": "Minor issues only",
    "executive_summary": "Minor code quality issues detected, but no evidence of data fabrication or manipulation.",
    "top_issues": [],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0194_scan.json"
  },
  "error": null
}