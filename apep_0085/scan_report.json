{
  "paper_id": "apep_0085",
  "scan_date": "2026-02-06T12:43:34.617128+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 12,
  "flags": [
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "HIGH",
      "file": "01_fetch_data.R",
      "lines": [
        118,
        135,
        147,
        151
      ],
      "evidence": "The script references variables that are never created in the pipeline (e.g., `labor_force`). `measure` only takes values `unemp_rate` or `employment`, so `pivot_wider()` cannot create a `labor_force` column. This implies the fetch/parse step as written would error or produce missing/wrong fields, undermining the provenance of `bls_state_panel.rds` (the foundation of the analysis). This is an integrity risk because the analysis panel depends on whether the fetched BLS series were correctly specified and mapped to variables.: bls_parsed <- bls_raw %>%\n  mutate(\n    statefip = as.integer(substr(series_id, 6, 7)),\n    measure = ifelse(grepl(\"03$\", series_id), \"unemp_rate\", \"employment\"),\n    month = as.integer(sub(\"M\", \"\", period))\n  )\n\nbls_march <- bls_parsed %>%\n  filter(month == 3) %>%\n  select(statefip, year, measure, value) %>%\n  pivot_wider(names_from = measure, values_from = value) %>%\n  mutate(\n    lfp_thousands = labor_force,\n    emp_thousands = employment,\n    unemp_thousands = labor_force - employment\n  )",
      "confidence": 0.95
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "01_fetch_data.R",
      "lines": [
        70,
        87,
        100,
        104,
        110
      ],
      "evidence": "Internal inconsistencies about which LAUS series correspond to which concept. Comments assert conflicting mappings (e.g., `...0003` described as labor force in one place and unemployment rate in another). The code constructs only two series (`...0003` and `...0005`) and later tries to compute labor force and unemployment levels. Given LAUS conventions (commonly: 03 unemployment rate, 04 unemployment level, 05 employment level, 06 labor force level), this mismatch can cause incorrect variable construction even if the script runs. This is a methodology/measurement mismatch risk because the paper\u2019s outcomes (log employment, unemployment rate) require correct series identification.: # Series ID structure: LASST{FIPS}0000000000003 = labor force level\n#                      LASST{FIPS}0000000000005 = employment level\n#                      LASST{FIPS}0000000000006 = unemployment rate\n...\n# LAUS series code endings:\n# 003 = Unemployment rate (%)\n# 005 = Employment (level)\n# 006 = Labor force (level)\n...\nlf_series <- sprintf(\"LASST%02d0000000000003\", state_fips_map$statefip)  # Unemp rate\nemp_series <- sprintf(\"LASST%02d0000000000005\", state_fips_map$statefip)  # Employment level",
      "confidence": 0.85
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        233,
        307
      ],
      "evidence": "Key inputs (PDMP mandate effective dates/full-exposure years; concurrent policy adoption years) are hard-coded rather than programmatically pulled from PDAPS/KFF/NCSL. The manuscript states these were compiled/cross-referenced from PDAPS and other sources, so hard-coding is not inherently improper, but it creates an auditability risk unless the underlying source documentation (e.g., PDAPS query exports or citations per state) is provided and versioned.: pdmp_mandates <- tribble(\n  ~state_abbr, ~statefip, ~mandate_effective_date, ~mandate_year_full_exposure,\n  \"KY\",  21, \"2012-07-20\", 2013,\n  ...\n  \"WY\",  56, \"2020-01-01\", 2021   # WY SF0047 mandatory query\n)\n...\nmedicaid_expansion <- tribble(\n  ~statefip, ~medicaid_expansion_year,\n  4, 2014, 5, 2014, 6, 2014, ...\n)\n\nrec_marijuana <- tribble(\n  ~statefip, ~rec_marijuana_year,\n  8, 2012, 53, 2012, 41, 2014, ...\n)",
      "confidence": 0.8
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "06_tables.R",
      "lines": [
        63,
        74
      ],
      "evidence": "The LaTeX adoption table can insert a hard-coded Wyoming effective date (\"2020-07-01\") if missing. This conflicts with `01_fetch_data.R`, which hard-codes Wyoming as \"2020-01-01\". Even if this affects only the paper\u2019s table (not treatment coding), it is a reproducibility/integrity concern because the manuscript\u2019s stated adoption timeline could disagree with the analysis inputs. This should be reconciled and sourced consistently.: adopt_table <- adopt_from_panel %>%\n  left_join(pdmp %>% select(state_abbr, mandate_effective_date, mandate_year_full_exposure),\n            by = \"state_abbr\") %>%\n  mutate(\n    # For WY: use known date if not in pdmp file\n    mandate_effective_date = ifelse(is.na(mandate_effective_date) & state_abbr == \"WY\",\n                                   \"2020-07-01\", mandate_effective_date),\n    mandate_year_full_exposure = ifelse(is.na(mandate_year_full_exposure),\n                                       first_treat, mandate_year_full_exposure)\n  )",
      "confidence": 0.9
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "04_robustness.R",
      "lines": [
        122,
        141,
        158
      ],
      "evidence": "Event-study window truncation (e=-6..+6) and placebo sample restriction are reasonable and consistent with the paper\u2019s discussion (anticipation=1; pre-trend focus on e<=-2), but these choices can affect what is visually/explicitly reported. Because the manuscript later discusses extended horizons (e up to +10) and shows sensitivity checks, the risk here is low; still, ensure that reported figures/tables match the widest-window objects actually computed/saved elsewhere.: att_nyt_dynamic <- aggte(cs_nyt, type = \"dynamic\", min_e = -6, max_e = 6)\n...\npanel_placebo_clean <- panel_placebo %>%\n  filter(first_treat == 0 | year < first_treat)",
      "confidence": 0.6
    }
  ],
  "file_verdicts": [
    {
      "file": "07_advisor_fixes.R",
      "verdict": "CLEAN"
    },
    {
      "file": "01_fetch_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "08_sensitivity.R",
      "verdict": "CLEAN"
    },
    {
      "file": "08_revision.R",
      "verdict": "CLEAN"
    },
    {
      "file": "07_first_stage.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "01b_fix_treatment.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 2,
      "MEDIUM": 2,
      "LOW": 1
    },
    "one_liner": "unclear provenance; method mismatch",
    "executive_summary": "The data pipeline in `01_fetch_data.R` is internally inconsistent and incomplete: it later references variables (e.g., `labor_force`) that are never created because `measure` only ever takes `unemp_rate` or `employment`, so `pivot_wider()` cannot produce a `labor_force` column. The script\u2019s comments and series mappings for LAUS codes also conflict with each other (e.g., the same `...0003` series is described as labor force in one place and unemployment rate in another), making it unclear which underlying data correspond to each concept and undermining reproducibility.",
    "top_issues": [
      {
        "category": "DATA_PROVENANCE_MISSING",
        "severity": "HIGH",
        "short": "The script references variables that are never created in...",
        "file": "01_fetch_data.R",
        "lines": [
          118,
          135
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0085/code/01_fetch_data.R#L118-L151"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "Internal inconsistencies about which LAUS series correspo...",
        "file": "01_fetch_data.R",
        "lines": [
          70,
          87
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0085/code/01_fetch_data.R#L70-L110"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0085_scan.json"
  },
  "error": null
}