{
  "paper_id": "apep_0063",
  "scan_date": "2026-02-06T12:39:02.056268+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 4,
  "flags": [
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        43,
        88
      ],
      "evidence": "National heat-death counts are manually entered as literals rather than being fetched from a documented, reproducible source (e.g., a downloaded CSV from EPA/BLS, or an API). The manuscript describes these as coming from EPA Climate Indicators/BLS, but the repository does not show a retrieval script or a pinned raw data file for audit/reproduction. This is a provenance gap (not necessarily wrongdoing) because a reader cannot independently verify these inputs.: national_heat_fatalities <- tribble(\n  ~year, ~national_heat_deaths,\n  1992, 11,\n  1993, 15,\n  ...\n  2023, 38  # Preliminary\n)",
      "confidence": 0.74
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        96,
        150
      ],
      "evidence": "State shares (from Arbury et al. 2016 per comments) are manually transcribed as hard-coded values without a machine-readable provenance artifact (e.g., a table extracted from the paper, a replication appendix, or a script showing how shares were computed). The manuscript acknowledges these are from Arbury et al. (2016), which lowers suspicion, but the lack of an auditable import path still prevents verification that the shares match the cited source.: state_shares_2000_2010 <- tribble(\n  ~state_abbr, ~deaths_2000_2010, ~share,\n  \"TX\", 43, 0.120,\n  \"CA\", 45, 0.125,\n  ...\n  \"NM\", 4, 0.011\n)\n# Remaining ~30% distributed across other states",
      "confidence": 0.78
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "HIGH",
      "file": "01_fetch_data.R",
      "lines": [
        153,
        166
      ],
      "evidence": "For all states not listed in Arbury et al. (2016), the code assigns an arbitrary equal share (0.003) and an arbitrary 2000\u20132010 death count (1), then renormalizes. This is not just missing data\u2014it is a strong modeling choice that deterministically constructs outcomes for ~half the states and can materially affect treated/control comparisons (especially for any treated state not in the explicitly-listed share table, and for the composition of \"never treated\" donors). The manuscript frames the whole exercise as illustrating identification failure, but this ad hoc completion rule is still a potentially biasing transform that should be explicitly justified/sensitivity-tested (e.g., allocate remainder proportionally to population/employment; use a Dirichlet prior; show robustness to alternative allocations).: other_shares <- tibble(\n  state_abbr = other_states,\n  deaths_2000_2010 = 1,\n  share = 0.003  # Small share each\n)\n\nstate_shares <- bind_rows(state_shares_2000_2010, other_shares) %>%\n  mutate(share = share / sum(share))  # Renormalize to sum to 1",
      "confidence": 0.86
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        224,
        240
      ],
      "evidence": "The code comment claims the imputed deaths \"will have variance but captures signal.\" The manuscript\u2019s core point is the opposite: fixed shares \u00d7 national totals mechanically remove state-specific treatment variation, making DiD unidentified/uninformative. The computation itself matches the manuscript (fixed shares), but the inline description is misleading relative to the paper\u2019s identification argument. This is a documentation mismatch rather than a computational mismatch.: mutate(\n  # Imputed heat deaths (will have variance but captures signal)\n  heat_deaths_imputed = national_heat_deaths * share,\n  # Rate per 100,000 workers (where employment available)\n  heat_rate = if_else(\n    !is.na(employment) & employment > 0,\n    (heat_deaths_imputed / employment) * 100000,\n    NA_real_\n  )\n)",
      "confidence": 0.71
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "paper.tex",
      "lines": [
        182,
        216
      ],
      "evidence": "Main regression table entries (coefficients/SEs/CIs/N) appear manually typed into LaTeX rather than being programmatically produced from the R outputs (e.g., via modelsummary/texreg/etable with saved objects). Because these are key reported numbers, this creates transcription risk and makes it harder to verify that the manuscript matches the code outputs. The manuscript explicitly says results are illustrative/uninformative, which reduces incentives for manipulation, but integrity auditing still flags hard-coded results as a reproducibility weakness.: \\begin{table}[H]\n\\centering\n\\caption{Regression Results Using Imputed Outcomes (Uninformative)}\n...\nTreatment Effect & 0.00026 & 0.00118 & $-$0.0011 & $-$0.0012 \\\\\n& (0.00148) & (0.00071) & (0.0011) & (0.0012) \\\\\n...\nObservations & 1,440 & 1,485 & 1,568 & 1,568 \\\\\nState$\\times$Cohort Units & 45 & 225 & 49 & 49 \\\\\n\\bottomrule\n\\end{tabular}",
      "confidence": 0.83
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "LOW",
      "file": "paper.tex",
      "lines": [
        246,
        258
      ],
      "evidence": "The manuscript asserts multiple robustness checks but does not provide corresponding code/output in the supplied scripts (03_main_analysis.R does not implement the stated high-heat controls restriction, alternative treatment definitions with fractional exposure, or dropping California as separate saved specifications). This is not proof of cherry-picking, but it is a selective-reporting risk because readers cannot see the full set of robustness results that are claimed.: The null findings are robust to several alternative specifications (results available upon request):\n\\begin{itemize}\n\\item Restricting to high-heat comparison states ...\n\\item Using alternative treatment definitions ...\n\\item Dropping California ...\n\\item Wild cluster bootstrap inference for TWFE\n\\end{itemize}",
      "confidence": 0.67
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 1,
      "MEDIUM": 3,
      "LOW": 2
    },
    "one_liner": "suspicious transforms",
    "executive_summary": "In `01_fetch_data.R`, states not listed in Arbury et al. (2016) are filled with fabricated values\u2014an equal share of 0.003 and a hard\u2011coded 2000\u20132010 death count of 1\u2014then the results are renormalized to look like valid proportions. This injects arbitrary pseudo-data rather than handling missingness, systematically biases downstream estimates, and makes the final distributions appear data-driven even though they are partly constructed by assumptions hidden in the code.",
    "top_issues": [
      {
        "category": "SUSPICIOUS_TRANSFORMS",
        "severity": "HIGH",
        "short": "For all states not listed in Arbury et al. (2016), the co...",
        "file": "01_fetch_data.R",
        "lines": [
          153,
          166
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0063/code/01_fetch_data.R#L153-L166"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0063_scan.json"
  },
  "error": null
}