{
  "paper_id": "apep_0044",
  "scan_date": "2026-02-06T12:35:19.583596+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "CLEAN",
  "files_scanned": 4,
  "flags": [
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "paper.tex",
      "lines": [
        170,
        206,
        207,
        212,
        213,
        218,
        219
      ],
      "evidence": "Main regression results in Table \\ref{tab:main} are manually typed into the LaTeX table rather than being programmatically produced from model objects (e.g., via fixest/etable or modelsummary). This is not evidence of fabrication by itself, but it creates an integrity risk: the manuscript can drift from the code output, and readers cannot verify the table is exactly what the code produced.: Post $\\times$ Treatment & 0.732*** & 0.149** \\\\\n& (0.240) & (0.060) \\\\\n...\nPost $\\times$ Treatment & 0.797*** & 0.374*** \\\\\n& (0.257) & (0.058) \\\\\n...\nPost $\\times$ Treatment & -0.065 & 0.330*** \\\\\n& (0.142) & (0.048) \\\\",
      "confidence": 0.78
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "03_main_analysis.R",
      "lines": [
        44,
        50,
        60,
        69
      ],
      "evidence": "The manuscript Table \\ref{tab:main} reports TWFE estimates for unemployment (Panel C, Column (1): -0.065 (0.142)), but the code never estimates a TWFE model for unemployment (there is twfe_emp and twfe_lfp, but no twfe_unemp). As written, the TWFE unemployment number in the paper cannot be reproduced from the provided analysis script and may have been computed elsewhere or entered manually.: twfe_emp <- feols(\n  emp_rate ~ post | state_fips + year,\n  data = panel_data,\n  cluster = ~state_fips\n)\n...\ntwfe_lfp <- feols(\n  lfp_rate ~ post | state_fips + year,\n  data = panel_data,\n  cluster = ~state_fips\n)\n...\n# Unemployment Rate\nsa_unemp <- feols(\n  unemp_rate ~ sunab(cohort, year, ref.p = -1) | state_fips + year,\n  data = panel_data,\n  cluster = ~state_fips\n)",
      "confidence": 0.87
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "paper.tex",
      "lines": [
        124,
        132,
        133
      ],
      "evidence": "The manuscript states the panel is 2011\u20132023 and describes 10 years of data, but the fetch script loops over 2010:2023 (14 years) and the produced panel will include all successfully fetched years in that range unless subsequently filtered (no filter is applied before saving). Also, 2011\u20132023 is 13 years, not 10. This inconsistency suggests the text does not match the code-generated dataset definition.: I construct a state-year panel from 2011 to 2023 using the Census Bureau's American Community Survey (ACS) 1-year estimates.\n...\nThe sample includes 52 states and territories (including DC and Puerto Rico) observed over 10 years where ACS data was successfully retrieved, yielding 520 state-year observations.",
      "confidence": 0.83
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "00_packages.R",
      "lines": [
        68,
        89
      ],
      "evidence": "Treatment timing (Clean Slate implementation years) is manually hard-coded as a tibble with only a high-level comment about sources (\"Clean Slate Initiative, CCRC, news sources\"), but there is no provenance file, citation list, or scraping/fetch code that documents where each date came from. Because treatment timing is central to identification in staggered DiD, this should be transparently sourced (e.g., a documented CSV with URLs/citations per state or a script that assembles the dates).: # Compiled from Clean Slate Initiative, CCRC, news sources\n# Using IMPLEMENTATION dates (when automatic expungement began), not passage dates\nclean_slate_dates <- tibble(\n  state = c(\"Pennsylvania\", \"Utah\", \"New Jersey\", \"Michigan\", \"Connecticut\",\n            \"Delaware\", \"Virginia\", \"Oklahoma\", \"Colorado\", \"California\",\n            \"Minnesota\", \"New York\"),\n  ...\n  treat_year = c(2019, 2022, 2020, 2023, 2023, 2024, 2026, 2025, 2025, 2023, 2025, 2027)\n)",
      "confidence": 0.8
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        109,
        116,
        127
      ],
      "evidence": "The manuscript claims inclusion of 52 states/territories including Puerto Rico, but the state FIPS-to-abbreviation mapping provided appears to cover the 50 states plus DC (51 total) and does not include Puerto Rico (FIPS 72). If ACS returns Puerto Rico in the API response, it will fail to match in this lookup and will have missing state_abbr, which can affect downstream summaries/plots and contradict the manuscript description.: state_abbr <- tibble(\n  state_fips = c(\"01\", \"02\", \"04\", ... , \"56\"),\n  state_abbr = c(\"AL\", \"AK\", \"AZ\", ... , \"WY\")\n)\n...\nleft_join(state_abbr, by = \"state_fips\")",
      "confidence": 0.7
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "05_figures.R",
      "lines": [
        90,
        91,
        92,
        93,
        94,
        95
      ],
      "evidence": "Figures restrict the event-time window to [-8, 4]. This may be defensible for readability, but the manuscript discusses significant pre-trends as far back as e = -11 (e.g., -1.81 pp at e=-11). If the plotted figure omits the most extreme pre-trend coefficients, readers may get a less severe visual impression than the full estimate vector. Consider showing the full available pre-periods (or explicitly stating the truncation rationale in the caption).: es_emp <- es_coefs %>%\n  filter(outcome == \"Employment Rate\", rel_time >= -8, rel_time <= 4) %>%\n  mutate(\n    ci_lower = estimate - 1.96 * se,\n    ci_upper = estimate + 1.96 * se\n  )",
      "confidence": 0.74
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "05_figures.R",
      "lines": [
        90,
        126,
        168
      ],
      "evidence": "Across all event-study figures, the code uniformly truncates to [-8, 4] despite the event-study output apparently containing more pre-period coefficients (the manuscript references e=-10 and e=-11). This is a mild selective-presentation risk: the code does not hide results computationally, but it can visually downweight the strongest pre-trend evidence in the main figures unless the appendix clearly shows the full window.: filter(outcome == \"Employment Rate\", rel_time >= -8, rel_time <= 4)\n...\nfilter(outcome == \"LFP Rate\", rel_time >= -8, rel_time <= 4)\n...\nfilter(rel_time >= -8, rel_time <= 4)",
      "confidence": 0.71
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        25
      ],
      "evidence": "A random seed is set globally, but the provided pipeline does not appear to use random number generation for estimation or data creation. This is not evidence of fabrication; it is likely boilerplate. However, global seeding can be a red flag in audits if combined with simulation\u2014here it looks unused.: # Set random seed for reproducibility\nset.seed(20260121)",
      "confidence": 0.55
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "CLEAN",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 0,
      "MEDIUM": 5,
      "LOW": 3
    },
    "one_liner": "Minor issues only",
    "executive_summary": "Minor code quality issues detected, but no evidence of data fabrication or manipulation.",
    "top_issues": [],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0044_scan.json"
  },
  "error": null
}