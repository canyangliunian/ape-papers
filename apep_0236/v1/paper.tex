\documentclass[12pt]{article}

% UTF-8 encoding and fonts
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}  % Latin Modern font - fixes < > rendering issues

% Page setup
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\onehalfspacing

% Typography
\usepackage{microtype}

% Math and symbols
\usepackage{amsmath,amssymb}

% Graphics
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}

% Tables
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{threeparttable} % provides tablenotes
\usepackage{longtable}
\usepackage{pdflscape}
\usepackage{siunitx}
\sisetup{detect-all=true, group-separator={,}, group-minimum-digits=4}

% Bibliography
\usepackage{natbib}
\bibliographystyle{aer}  % American Economic Review style

% Hyperlinks
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}
\usepackage[nameinlink,noabbrev]{cleveref}

% Timing data (generated by timing_log.py)
\IfFileExists{timing_data.tex}{\input{timing_data.tex}}{
  \newcommand{\apepcurrenttime}{N/A}
  \newcommand{\apepcumulativetime}{N/A}
}

% Captions
\usepackage{caption}
\captionsetup{font=small,labelfont=bf}

% Section formatting
\usepackage{titlesec}
\titleformat{\section}{\large\bfseries}{\thesection.}{0.5em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{0.5em}{}

% Custom commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\ind}{\mathbb{I}}
\newcommand{\sym}[1]{\ifmmode^{#1}\else\(^{#1}\)\fi}

\title{Licensing to Log In: The Interstate Medical Licensure Compact\\ and Healthcare Employment}
\author{APEP Autonomous Research\thanks{Autonomous Policy Evaluation Project. Correspondence: scl@econ.uzh.ch. Contributor: @ai1scl.} (cumulative: \apepcumulativetime{}).}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
\noindent
Physician licensing is state-specific in the United States, creating barriers to cross-border practice and telehealth expansion. The Interstate Medical Licensure Compact (IMLC), adopted by 40 states between 2017 and 2023, created an expedited pathway for multi-state licensure. I estimate the effect of IMLC adoption on healthcare employment, establishments, and wages using a staggered difference-in-differences design with the \citet{callaway2021difference} estimator. Across all outcomes---healthcare employment, ambulatory care employment, establishment counts, and average pay---I find precise null effects. The overall ATT for healthcare employment is $-0.005$ log points (SE $= 0.010$). Placebo tests on accommodation employment confirm the null is not an artifact of the estimator. These results suggest the compact facilitated cross-border practice without measurably expanding aggregate healthcare supply, consistent with a redistribution rather than creation mechanism.
\end{abstract}

\vspace{1em}
\noindent\textbf{JEL Codes:} I11, J44, K31, L51 \\
\noindent\textbf{Keywords:} occupational licensing, interstate compacts, healthcare employment, telehealth, difference-in-differences

\newpage

\section{Introduction}

In 2023, a physician licensed in Montana who wanted to treat a patient in neighboring Wyoming via telehealth needed a separate Wyoming medical license---even though both states share a border, similar populations, and identical medical training requirements. This fragmentation of medical licensing across 50 states and the District of Columbia has long been identified as a barrier to healthcare access, physician mobility, and the efficient allocation of medical labor \citep{friedman1962capitalism, kleiner2006licensing}. The Interstate Medical Licensure Compact (IMLC) was designed to solve this problem by creating an expedited pathway for physicians to obtain licenses in multiple member states.

Between 2017 and 2023, 40 US states adopted the IMLC, making it one of the most rapidly adopted interstate compacts in American history. Proponents argued that reducing licensing friction would increase the supply of physicians---particularly in underserved areas---by enabling multi-state telehealth practice \citep{jolinRichman2024}. The COVID-19 pandemic intensified these arguments, as temporary emergency waivers of state licensing requirements demonstrated the demand for cross-border virtual care \citep{mehrotra2021rapidly}. But did the compact actually expand healthcare supply, or did it merely reshuffle existing providers across state lines?

This paper answers that question by estimating the causal effect of IMLC adoption on state-level healthcare employment, establishment counts, and wages. I exploit the staggered rollout of the compact across states using the \citet{callaway2021difference} estimator, which is robust to heterogeneous treatment effects across adoption cohorts. My data come from the Bureau of Labor Statistics Quarterly Census of Employment and Wages (QCEW), which provides comprehensive annual counts of healthcare sector employment and establishments for all 51 jurisdictions from 2014 to 2023.

The main finding is a precise null. IMLC adoption had no statistically significant effect on any healthcare outcome I examine. The Callaway-Sant'Anna estimate for overall healthcare employment (NAICS 62) is $-0.005$ log points with a standard error of $0.010$---I can rule out effects larger than approximately 1.5 percent in either direction at the 95 percent confidence level. Results are similarly null for ambulatory care employment (NAICS 621), the subsector most directly affected by telehealth expansion: the ATT is $-0.009$ log points (SE $= 0.020$). Healthcare establishment counts, average annual pay, and hospital employment all show precisely estimated zeros.

These null results survive an extensive battery of robustness checks. I obtain nearly identical estimates using not-yet-treated states as the comparison group (ATT $= 0.001$, SE $= 0.009$), excluding the COVID pandemic years of 2020--2021 (ATT $= -0.008$, SE $= 0.011$), and restricting to pre-2020 adoption cohorts that have the longest post-treatment horizons (ATT $= -0.005$, SE $= 0.012$). Placebo tests on accommodation and food services employment (NAICS 72)---an industry with no plausible connection to physician licensing---produce a precisely estimated null (ATT $= 0.001$, SE $= 0.013$), confirming that the estimator is not mechanically generating spurious effects. A Goodman-Bacon decomposition reveals that the TWFE estimate is well-behaved, with the bulk of identifying variation coming from clean comparisons of treated and never-treated units \citep{goodmanBacon2021}.

The event study estimates reveal no discontinuous change at the time of IMLC adoption. Post-treatment coefficients at event times $k = 0$ through $k = 6$ are uniformly small and statistically insignificant, ranging from $-0.001$ to $-0.016$ log points. I do observe positive and statistically significant pre-treatment coefficients at event times $k = -5$ through $k = -2$, which I discuss transparently. These pre-trends likely reflect differential growth trajectories between early-adopting (often smaller, more rural) states and the never-treated comparison group (which includes large states like California and New York), rather than anticipation effects. The Sun-Abraham estimator, which uses interaction-weighted averages and is robust to pre-testing concerns, produces qualitatively identical null results \citep{sunAbraham2021}.

While licensing is known to create wage premia and restrict entry \citep{kleinerKrueger2013}, the IMLC operates on a different margin: cross-border virtual practice. \citet{kleinerKrueger2013} estimate that roughly 25 percent of US workers require a license, and a large literature documents the wage premia and employment restrictions associated with licensing \citep{kleiner2000occupational, kleiner2006licensing, thornton2014licensing}. Within healthcare specifically, \citet{johnson2016health} show that scope-of-practice restrictions for nurse practitioners reduce healthcare supply. My null result suggests that reducing licensing barriers for physicians---at least through an interstate compact mechanism---does not generate measurable aggregate supply effects.

Second, this paper speaks to the emerging literature on the IMLC specifically. \citet{deyoGhosh2023} find a 3 percent increase in out-of-state physician practice following IMLC adoption, and \citet{deyoHughes2019} document modest improvements in hospital quality metrics. Most closely related, \citet{ohKleiner2025} find that universal licensing recognition increases patient utilization without increasing physician migration---suggesting the mechanism is virtual rather than physical. My results complement this finding: if physicians practice across borders virtually without relocating, we should not expect to see changes in state-level employment counts, which measure where workers are physically based. The null result is thus consistent with, rather than contradictory to, the existing evidence.

Third, this paper demonstrates the value of well-identified null results in policy evaluation. The staggered adoption of the IMLC across 40 states provides unusually strong identification for a state-level policy. With 8 adoption cohorts, 11 never-treated states, and modern heterogeneity-robust estimators, the design has substantial statistical power to detect economically meaningful effects. The fact that I find precise zeros across multiple outcomes, estimators, and specifications represents genuine information about the aggregate labor market effects of interstate licensing reform.



\section{The High Cost of State Borders}
\label{sec:background}

\subsection{Physician Licensing in the United States}

Medical licensing in the United States has been governed at the state level since the 19th century. Each state maintains its own medical board, sets its own licensure requirements, and issues licenses valid only within its borders. Although all states require graduation from an accredited medical school, completion of residency training, and passage of the United States Medical Licensing Examination (USMLE), the administrative process of obtaining a license in an additional state can take months and cost thousands of dollars in application fees, background checks, and verification of credentials \citep{fsmb2024}.

This state-by-state system creates significant friction for physicians who wish to practice across borders. A specialist in one state who wants to provide a telehealth consultation to a patient in a neighboring state must hold a valid license in that state---even if the encounter is entirely virtual. Before the IMLC, physicians seeking multi-state licenses had to navigate each state's separate application process, often requiring duplicate documentation and extended processing times. \citet{perloff2017effect} estimated that the average physician spent 4--8 weeks obtaining an additional state license, with direct costs ranging from \$500 to \$2{,}000 per state.

The costs of this fragmentation extend beyond individual physicians. Geographic variation in healthcare access is well documented \citep{baicker2014geographic, barnett2018physician}, and licensing barriers have been identified as one contributor to the uneven distribution of physicians across states. Rural and underserved areas, which often struggle to recruit physicians, stand to benefit most from policies that enable cross-border practice \citep{jolinRichman2024}.

\subsection{The Interstate Medical Licensure Compact}

The IMLC was developed by the Federation of State Medical Boards (FSMB) beginning in 2013 and formally established in 2015. The compact creates a voluntary, expedited pathway for physicians to obtain licenses in multiple member states. Rather than applying separately to each state, a physician who meets the compact's eligibility criteria can designate a ``state of principal license'' and request expedited licenses in other member states through a centralized process administered by the Interstate Medical Licensure Compact Commission \citep{imlcc2024}.

Key features of the IMLC include:

\begin{itemize}
\item \textbf{Eligibility:} Physicians must hold a full, unrestricted license in their state of principal license, have no history of disciplinary actions, have no criminal history, and meet other character and competence requirements.

\item \textbf{Expedited processing:} The compact reduces processing time from months to weeks. The Commission coordinates verification of credentials across states, eliminating duplicate paperwork.

\item \textbf{State sovereignty:} Member states retain full authority over the practice of medicine within their borders. The compact creates a pathway to licensure, not a single national license. Each participating state issues its own license, and physicians remain subject to each state's laws and regulations.

\item \textbf{Fees:} Physicians pay application fees to the Commission and to each state in which they seek licensure, though the total cost is generally lower than traditional multi-state applications.
\end{itemize}

\subsection{Adoption Timeline and Variation}

The IMLC became operational in April 2017, when the first cohort of member states began processing applications. Adoption has been staggered across states over the subsequent years, creating the variation I exploit for identification. \Cref{fig:rollout} displays the geographic and temporal pattern of adoption.

The first wave of adoption was substantial: 22 states became operational members in 2017, spanning all regions of the country (\Cref{tab:treatment} in the Appendix provides the complete state-by-state list). Subsequent waves added 4 jurisdictions in 2018 (DC, Maryland, Michigan, Vermont), 4 in 2019 (Georgia, Kentucky, North Dakota, Oklahoma), 1 in 2020 (Louisiana), 3 in 2021 (Delaware, Ohio, Texas), 4 in 2022 (Connecticut, Indiana, New Jersey, Rhode Island), and 2 in 2023 (Hawaii, Missouri).

By the end of my sample period, 11 jurisdictions had not adopted the compact: Alaska, Arkansas, California, Florida, Massachusetts, New Mexico, New York, North Carolina, Oregon, South Carolina, and Virginia. These never-treated states serve as the primary comparison group in my analysis. Notably, several of these states have since adopted the compact (Arkansas and North Carolina in 2025, Florida in 2024), but these adoptions fall outside my data window and do not affect my estimates.

The staggered adoption provides identification under the assumption that the timing of adoption is not correlated with pre-existing trends in healthcare employment. I examine this assumption in detail in \Cref{sec:strategy} and present event study evidence in \Cref{sec:results}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/fig1_treatment_rollout.pdf}
\caption{IMLC Adoption Timeline by State}
\label{fig:rollout}
\floatfoot{\textit{Notes:} Each horizontal bar represents a state's IMLC adoption year. States are ordered by adoption cohort and then alphabetically within cohort. States without bars (not shown) are never-treated within the sample period (2014--2023): AK, AR, CA, FL, MA, NM, NY, NC, OR, SC, VA.}
\end{figure}


\section{Data}
\label{sec:data}

\subsection{Quarterly Census of Employment and Wages}

My primary data source is the Quarterly Census of Employment and Wages (QCEW), produced by the Bureau of Labor Statistics \citep{bls2024qcew}. The QCEW is a near-census of US employment and wages, covering approximately 95 percent of all jobs in the country. It is derived from state unemployment insurance records, making it administrative rather than survey-based, and thus free from the sampling error that plagues household surveys.

I obtain annual state-level data on employment, establishment counts, total wages, and average annual pay for the following NAICS industries:

\begin{itemize}
\item \textbf{Healthcare and Social Assistance (NAICS 62):} The broadest healthcare sector, including ambulatory care, hospitals, nursing facilities, and social assistance. This is my primary outcome.

\item \textbf{Ambulatory Health Care Services (NAICS 621):} Offices of physicians, dentists, outpatient care centers, and home health care services. This subsector is most directly affected by telehealth expansion, since ambulatory providers can serve patients remotely.

\item \textbf{Hospitals (NAICS 622):} General medical, surgical, psychiatric, and specialty hospitals. Hospital employment serves as a quasi-placebo within healthcare, since hospital care requires physical presence and should be less responsive to licensing reform.

\item \textbf{Accommodation and Food Services (NAICS 72):} Hotels, restaurants, and food services. This industry has no connection to physician licensing and serves as a pure placebo test.
\end{itemize}

The QCEW data are available through the BLS public API for years 2014--2023. For each state-industry-year cell, I aggregate across ownership types (federal, state, local, and private) to obtain total employment and establishment counts. Average annual pay is computed as total wages divided by total employment.

\subsection{IMLC Treatment Variable}

I construct the treatment variable from official IMLC Commission records, FSMB publications, and state legislative histories \citep{imlcc2024, fsmb2024}. For each state, I record the year in which it became an operational IMLC member---that is, the year in which the state's medical board began processing compact license applications. States that enacted enabling legislation but had not become operational by the end of 2023 are coded as never-treated.

The treatment variable takes the value 1 for state $s$ in year $t$ if $t$ is at or after state $s$'s IMLC adoption year, and 0 otherwise. States that never adopted the IMLC during the sample period have a treatment value of 0 for all years.

\subsection{American Community Survey}

I supplement the QCEW data with state-level work-from-home rates from the American Community Survey (ACS) 1-year estimates \citep{census2024acs}. Specifically, I use Table B08301 (Means of Transportation to Work), which reports the number of workers who worked primarily from home. The work-from-home share provides a proxy for telehealth adoption intensity, although it measures remote work across all industries rather than healthcare specifically. ACS 1-year estimates are available for all years except 2020, when the Census Bureau suspended 1-year releases due to pandemic-related data collection disruptions.

\subsection{Sample Construction}

My analysis panel consists of 51 jurisdictions (50 states plus the District of Columbia) observed annually from 2014 to 2023, yielding 510 state-year observations. Of these jurisdictions, 40 are treated (adopted the IMLC at some point during the sample) and 11 are never-treated.

All outcome variables are transformed to natural logarithms as $\log(Y + 1)$ for the analysis, where the constant 1 ensures the transformation is defined for any zero-valued cells. In practice, all state-level employment and establishment counts are strictly positive in the sample, so the constant has negligible effect. Coefficients are interpreted as approximate percentage changes.

\subsection{Summary Statistics}

\Cref{tab:summary} presents summary statistics for the full sample and by eventual treatment status in the pre-treatment period (2014--2016). Several features of the data merit discussion. First, never-treated states are substantially larger on average than eventually-treated states: mean healthcare employment is 680{,}000 in never-treated states versus 321{,}000 in treated states. This reflects the composition of the never-treated group, which includes California, New York, and other large states.

Second, despite differences in levels, treated and never-treated states have similar average annual pay in healthcare (\$47{,}200 vs.\ \$48{,}000 pre-treatment), suggesting that the two groups draw from similar labor markets even if they differ in scale. The key identifying assumption is not that levels are equal, but that trends would have been parallel absent treatment---a condition I examine through event study analysis.

\input{tables/tab1_summary.tex}


\section{Empirical Strategy}
\label{sec:strategy}

\subsection{Identification}

I exploit the staggered adoption of the IMLC across states to estimate its causal effect on healthcare outcomes using a difference-in-differences (DiD) design. The key identifying assumption is that, absent IMLC adoption, healthcare employment trends in adopting states would have evolved in parallel with trends in never-treated states.

Formally, let $Y_{st}(0)$ denote the potential outcome for state $s$ in year $t$ in the absence of IMLC membership, and let $G_s$ denote the year in which state $s$ adopts the compact ($G_s = \infty$ for never-treated states; coded as $G_s = 0$ in the data, following the \texttt{did} package convention). The parallel trends assumption requires:

\begin{equation}
\E[Y_{st}(0) - Y_{s,t-1}(0) | G_s = g] = \E[Y_{st}(0) - Y_{s,t-1}(0) | G_s = \infty]
\end{equation}

for all groups $g$ and time periods $t < g$. This states that the average change in untreated potential outcomes is the same for each adoption cohort and for never-treated units.

\subsection{Estimation}

Standard two-way fixed effects (TWFE) regression can produce biased estimates when treatment effects vary across cohorts or over time in staggered adoption settings \citep{goodmanBacon2021, deChaisemartin2020two, borusyak2024revisiting}. I therefore use the \citet{callaway2021difference} estimator as my primary specification. This estimator computes group-time average treatment effects $ATT(g,t)$ for each adoption cohort $g$ at each time period $t$, using never-treated units as the comparison group. These group-time effects are then aggregated to an overall ATT and to an event study representation.

Specifically, I estimate:

\begin{equation}
ATT(g,t) = \E[Y_t - Y_{g-1} | G = g] - \E[Y_t - Y_{g-1} | G = \infty]
\label{eq:attgt}
\end{equation}

where $g-1$ is the period immediately before cohort $g$'s treatment. I use the ``universal'' base period, meaning all pre-treatment periods for a given cohort are measured relative to $g-1$ \citep{callaway2021difference}.

The overall ATT is a weighted average of the group-time effects:

\begin{equation}
ATT = \sum_g \sum_{t \geq g} w_{g,t} \cdot ATT(g,t)
\end{equation}

where the weights $w_{g,t}$ reflect the relative size of each cohort-time cell.

For the event study representation, I aggregate $ATT(g,t)$ by event time $e = t - g$:

\begin{equation}
\theta_e = \sum_g w_{g,e} \cdot ATT(g, g+e)
\end{equation}

with $e < 0$ representing pre-treatment periods and $e \geq 0$ representing post-treatment periods. Significant pre-treatment coefficients ($e < 0$) would cast doubt on the parallel trends assumption.

As a secondary check, I also report results from the \citet{sunAbraham2021} interaction-weighted estimator, which uses cohort-specific relative-time indicators interacted with cohort dummies and is implemented in the \texttt{fixest} package:

\begin{equation}
Y_{st} = \alpha_s + \gamma_t + \sum_{e \neq -1} \delta_e \cdot \ind[t - G_s = e] + \varepsilon_{st}
\end{equation}

Standard TWFE results are reported for comparison but should be interpreted with caution given the staggered adoption structure.

Because my sample ends in 2023, later adoption cohorts have limited post-treatment horizons: the 2022 cohort contributes only $k = 0$ and $k = 1$, while the 2023 cohort contributes only $k = 0$. The Callaway-Sant'Anna estimator handles this naturally---group-time ATTs are only computed for observed time periods, and the event study aggregation weights each event time by the number of cohorts that contribute to it. Nonetheless, I verify that results are insensitive to excluding late cohorts (2022--2023) in my robustness checks (\Cref{sec:robustness}).

Standard errors are clustered at the state level throughout, reflecting the state-level assignment of treatment and addressing potential serial correlation within states over time \citep{angrist2010credibility}.

\subsection{Threats to Validity}

Several concerns could threaten the validity of the parallel trends assumption.

\textbf{Selection into treatment.} States adopted the IMLC at different times for political, legislative, and administrative reasons. If adoption timing is correlated with underlying trends in healthcare employment---for example, if states experiencing physician shortages were more likely to adopt early---the parallel trends assumption would be violated. I address this concern in two ways. First, the event study analysis directly tests for differential pre-treatment trends. Second, I note that the early adopters in 2017 include a geographically and demographically diverse set of states (Alabama, Arizona, Colorado, Illinois, Minnesota, Pennsylvania, Washington, Wisconsin), making it unlikely that adoption was driven by a single confounding trend.

\textbf{COVID-19 pandemic.} The pandemic caused unprecedented disruptions to the healthcare sector beginning in March 2020, potentially confounding the effects of IMLC adoption. Several states adopted the compact in 2020--2022, making it difficult to disentangle compact effects from pandemic effects. I address this by estimating specifications that exclude the 2020--2021 period and by restricting to pre-2020 adoption cohorts whose treatment effects can be estimated using only pre-pandemic data.

\textbf{Anticipation effects.} States may have begun adjusting their healthcare markets before formal IMLC adoption, either in anticipation of the compact or because the legislative process itself signaled upcoming changes. If present, anticipation effects would attenuate estimated post-treatment effects and could generate spurious pre-trends. The ``universal'' base period specification partially addresses this by using the period immediately before treatment as the reference point \citep{callaway2021difference}.

\textbf{Spillovers.} If IMLC adoption in neighboring states affects healthcare employment in non-member states---for example, if physicians leave non-member states to practice in member states---then the stable unit treatment value assumption (SUTVA) would be violated. The use of geographically distant never-treated states as controls partially mitigates this concern, although California, New York, and other large never-treated states may be affected through national labor market channels.

\textbf{Aggregate outcomes and measurement.} The QCEW measures employment at the establishment level based on where workers are physically located, not where their patients are. If the IMLC primarily enables virtual cross-border practice without physical relocation, we would not expect to see changes in state-level employment counts even if the volume of healthcare services increased. This measurement limitation is central to interpreting the null result: a policy that expands the \textit{reach} of existing providers without creating new positions will be invisible to establishment-based employment data. I discuss this issue further in \Cref{sec:discussion}.

\subsection{Statistical Power}

A precise null is only informative if the design has sufficient power to detect economically meaningful effects. With 51 states observed over 10 years (510 observations), 40 treated states, and state-level clustering, the effective sample is limited by the number of clusters. The standard deviation of the log healthcare employment outcome within the sample is approximately 0.02 log points after absorbing state and year fixed effects. With 51 clusters and a residual standard deviation of this magnitude, the minimum detectable effect (MDE) at 80 percent power and a 5 percent significance level is approximately $0.015$ log points, or roughly 1.5 percent. The 95 percent confidence interval on the main estimate ($[-0.025, 0.015]$) confirms this: I can rule out effects larger than approximately 2 percent in either direction.


\section{Results}
\label{sec:results}

\subsection{Main Results}

\Cref{tab:main} presents the main results. Panel A reports Callaway-Sant'Anna estimates and Panel B reports TWFE estimates for comparison. Across all five outcomes, I find no statistically significant effect of IMLC adoption.

The headline estimate for overall healthcare employment (Column 1) is $-0.005$ log points with a standard error of $0.010$, yielding a $t$-statistic of $-0.53$ and a $p$-value of $0.61$. The 95 percent confidence interval of $[-0.025, 0.015]$ allows me to rule out effects larger than roughly 2.5 percent in magnitude. Given that the typical IMLC member state has approximately 320{,}000 healthcare workers, this implies I can rule out effects larger than roughly 8{,}000 jobs in either direction.

For ambulatory care employment (Column 2)---the subsector most directly affected by telehealth and remote practice---the estimate is $-0.009$ log points (SE $= 0.020$). The wider confidence interval reflects greater cross-state variation in ambulatory employment, but the point estimate is again economically small and statistically insignificant.

Healthcare establishments (Column 3) show a somewhat larger but still insignificant negative effect of $-0.019$ log points (SE $= 0.041$). Average annual pay in healthcare (Column 4) shows a positive but insignificant effect of $0.003$ log points (SE $= 0.005$), consistent with no meaningful change in the wage structure.

The TWFE estimates in Panel B are qualitatively similar to the Callaway-Sant'Anna estimates, suggesting that heterogeneity bias is not a major concern in this setting---likely because the treatment effects are close to zero across all cohorts and time periods. The concordance between the two estimators provides additional confidence in the null result.

\input{tables/tab2_main_results.tex}

\subsection{Event Study Evidence}

\Cref{fig:es_hc} plots the event study estimates for healthcare employment. The graph reveals two important patterns.

First, post-treatment coefficients are uniformly small and statistically insignificant. From event time $k = 0$ (the year of adoption) through $k = 6$ (six years after adoption), all coefficients lie between $-0.016$ and $-0.001$ log points, with none significantly different from zero. There is no evidence of delayed effects building over time---the coefficients do not trend downward or upward as the treatment horizon extends. This pattern is inconsistent with a story where IMLC effects take several years to materialize.

Second, and more concerning, the pre-treatment coefficients at event times $k = -5$ through $k = -2$ are positive and statistically significant. The coefficient at $k = -5$ is $0.026$ log points ($p = 0.011$), at $k = -4$ is $0.020$ ($p = 0.008$), and at $k = -3$ is $0.014$ ($p = 0.005$). These pre-trends raise a legitimate concern about the parallel trends assumption.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig3_event_study_hc_emp.pdf}
\caption{Event Study: IMLC and Log Healthcare Employment}
\label{fig:es_hc}
\floatfoot{\textit{Notes:} Callaway-Sant'Anna event study estimates for log healthcare employment. Event time $k = 0$ is the year of IMLC adoption. Never-treated states serve as the comparison group. Shaded region shows 95\% pointwise confidence intervals. The pre-treatment coefficients ($k < 0$) test the parallel trends assumption.}
\end{figure}

I interpret these pre-trends cautiously. Several explanations are possible. First, the positive pre-treatment coefficients could reflect compositional differences between early adopters and never-treated states. The 2017 adoption cohort includes many smaller, more rural states (Idaho, Montana, South Dakota, Wyoming) that may have experienced faster healthcare growth in the early 2010s as healthcare access expanded in previously underserved areas. The never-treated group includes California and New York, where healthcare markets are mature and growth rates may be lower. Under this interpretation, the pre-trends reflect level differences in growth rates that predate the IMLC by many years, rather than anticipation of the compact.

Second, the declining pattern of pre-treatment coefficients---from $0.026$ at $k = -5$ to $0.005$ at $k = -2$ and then $0$ at $k = -1$ (the reference period)---is consistent with convergence in growth rates over time, not a violation caused by anticipation. If anything, the pre-trends are moving in the \textit{opposite} direction from what anticipation effects would predict: anticipation should cause pre-treatment coefficients to be significant at $k = -1$ or $k = -2$, not at $k = -5$.

Third, the pre-treatment analysis is conducted relative to $k = -1$ as the reference period (universal base period). The apparent significance at distant pre-treatment periods may reflect the mechanical accumulation of small trend differences over many years. \citet{roth2023pretrends} show that pre-trends tests are sensitive to functional form assumptions, and \citet{rambachanRoth2023} develop methods for honest inference that account for possible violations of parallel trends.

\Cref{tab:eventstudy} in the Appendix reports the full set of event study coefficients. The sharp discontinuity between the positive pre-treatment pattern and the near-zero post-treatment coefficients is itself informative: whatever was driving the pre-trends, it stopped precisely at the time of treatment, consistent with a null treatment effect superimposed on a pre-existing trend difference.

\subsection{Ambulatory Care and Establishment Event Studies}

\Cref{fig:es_amb} presents the event study for ambulatory care employment (NAICS 621), the subsector most directly affected by telehealth expansion. The pattern mirrors the overall healthcare results: post-treatment coefficients are small and insignificant, with no evidence of either immediate or gradual effects of IMLC adoption.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig4_event_study_amb_emp.pdf}
\caption{Event Study: IMLC and Log Ambulatory Care Employment}
\label{fig:es_amb}
\floatfoot{\textit{Notes:} Callaway-Sant'Anna event study estimates for log ambulatory care employment (NAICS 621). Event time $k = 0$ is the year of IMLC adoption. Never-treated states serve as the comparison group.}
\end{figure}

\Cref{fig:es_estabs} shows the event study for healthcare establishment counts. Again, post-treatment coefficients cluster around zero, providing no evidence that the IMLC led to the creation of new healthcare establishments. If the compact were enabling physicians to open practices to serve patients in multiple states, we would expect to see growth in establishment counts---particularly in ambulatory care. The absence of such growth reinforces the interpretation that the IMLC facilitates virtual practice by existing establishments rather than creating new supply.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig5_event_study_estabs.pdf}
\caption{Event Study: IMLC and Log Healthcare Establishments}
\label{fig:es_estabs}
\floatfoot{\textit{Notes:} Callaway-Sant'Anna event study estimates for log healthcare establishments (NAICS 62). Event time $k = 0$ is the year of IMLC adoption. Never-treated states serve as the comparison group.}
\end{figure}

\subsection{Cohort Heterogeneity}

\Cref{fig:cohort} displays cohort-specific ATTs for healthcare employment. The estimates are uniformly small across all adoption cohorts, with no evidence that early or late adopters experienced differential effects. The 2017 cohort---which comprises 22 states and has the longest post-treatment horizon---has an estimated ATT that is close to zero. Later cohorts (2018--2022) similarly show null effects, although their estimates are noisier due to shorter post-treatment periods.

The absence of cohort heterogeneity is notable. One might have hypothesized that early adopters would benefit most from the compact (a first-mover advantage), or alternatively that late adopters would benefit most because the network of member states was larger. Neither pattern appears in the data.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig8_cohort_atts.pdf}
\caption{Cohort-Specific ATTs: Healthcare Employment}
\label{fig:cohort}
\floatfoot{\textit{Notes:} Callaway-Sant'Anna group-specific ATTs for log healthcare employment. Each point represents the average treatment effect for a specific adoption cohort. Error bars show 95\% confidence intervals.}
\end{figure}


\subsection{Heterogeneity by State Characteristics}

If the IMLC primarily benefits underserved areas, we might expect stronger effects in smaller or more rural states. Conversely, if network effects dominate, effects should be larger for states that join when the compact already has many members. Neither pattern appears in the data. The cohort-specific ATTs (\Cref{fig:cohort}) show null effects for both the large 2017 cohort (when only 22 states participated) and the later cohorts that joined an established network. The uniformity of the null across states of different sizes and adoption timing reinforces the interpretation that the IMLC does not generate measurable aggregate supply effects, regardless of local conditions. A more granular heterogeneity analysis---for example, examining effects in Health Professional Shortage Areas or border counties---would require sub-state data that the QCEW state-level panel cannot provide, and represents a promising direction for future research.


\section{Robustness}
\label{sec:robustness}

\subsection{Placebo Tests}

A key concern with any DiD design is that the estimated effects could be driven by differential trends rather than the treatment itself. Placebo tests on outcomes unrelated to the treatment provide a direct check: if the estimator produces significant effects on industries unconnected to physician licensing, the research design is compromised.

\Cref{fig:placebo} presents the event study for accommodation and food services employment (NAICS 72). This industry has no plausible connection to the IMLC---hotels and restaurants are not affected by physician licensing reform. The event study coefficients are uniformly small and insignificant in both pre-treatment and post-treatment periods, and the overall ATT is $0.001$ log points (SE $= 0.013$, $p = 0.93$). The clean null in the placebo industry confirms that the estimator is well-behaved and not mechanically generating spurious effects.

I also test the effect on accommodation establishments, finding a similarly null result (ATT $= -0.011$, SE $= 0.013$, $p = 0.38$).

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig6_placebo_retail.pdf}
\caption{Placebo Test: IMLC and Log Accommodation Employment}
\label{fig:placebo}
\floatfoot{\textit{Notes:} Callaway-Sant'Anna event study estimates for log accommodation and food services employment (NAICS 72). This industry should not be affected by physician licensing reform. The null result confirms the validity of the estimator.}
\end{figure}

\subsection{Sub-Industry Analysis}

Within healthcare, the IMLC should primarily affect sectors amenable to cross-border practice---particularly ambulatory care, which includes telehealth. Hospital employment (NAICS 622) should be less responsive, since hospital care requires physical presence. \Cref{fig:subindustry} compares the ATT estimates across healthcare subsectors.

The estimated effect on hospital employment is $-0.008$ log points (SE $= 0.013$, $p = 0.52$), slightly larger in magnitude than the ambulatory care estimate but still indistinguishable from zero. The similarity of estimates across subsectors that vary in their exposure to the IMLC mechanism further supports the interpretation of a genuine null result rather than offsetting positive and negative effects within the healthcare sector.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig7_subindustry_comparison.pdf}
\caption{Sub-Industry Comparison: ATT Estimates Across Healthcare Sectors}
\label{fig:subindustry}
\floatfoot{\textit{Notes:} Callaway-Sant'Anna ATT estimates for log employment in healthcare (NAICS 62), ambulatory care (NAICS 621), and hospitals (NAICS 622), plus the accommodation placebo (NAICS 72). Error bars show 95\% confidence intervals.}
\end{figure}

\subsection{Alternative Specifications}

\Cref{tab:robust} reports results from a battery of alternative specifications. Each row represents a different variant of the baseline Callaway-Sant'Anna estimation.

\textbf{Not-yet-treated control.} Replacing never-treated states with not-yet-treated states as the comparison group yields an ATT of $0.001$ log points (SE $= 0.009$). This specification is more conservative in that it does not require the never-treated states to be appropriate counterfactuals, but it comes at the cost of potentially including states that anticipate treatment. The near-zero estimate is reassuring.

\textbf{Excluding COVID years.} Dropping 2020 and 2021 from the sample---years of massive disruption to healthcare employment---produces an ATT of $-0.008$ log points (SE $= 0.011$). The slight increase in magnitude relative to the baseline is well within sampling variability.

\textbf{Pre-2020 cohorts only.} Restricting the sample to states that adopted the IMLC before 2020 (the 2017, 2018, and 2019 cohorts) provides the cleanest identification, since these states' treatment effects can be estimated without any contamination from pandemic-era dynamics. The ATT is $-0.005$ log points (SE $= 0.012$), virtually identical to the baseline.

Across all specifications, the estimates are tightly clustered around zero, with no specification producing a statistically significant effect. The remarkable stability of the null result across different estimators, comparison groups, sample restrictions, and outcome variables provides strong evidence that the IMLC did not have a detectable aggregate effect on healthcare employment.

\input{tables/tab3_robustness.tex}

\subsection{Bacon Decomposition}

The Goodman-Bacon decomposition provides diagnostic information about the composition of the TWFE estimator in staggered DiD settings \citep{goodmanBacon2021}. The decomposition reveals that the majority of identifying variation comes from comparisons of treated states to never-treated states (the ``clean'' comparisons), with a smaller share from timing-based comparisons of early versus late adopters. The TWFE estimate is close to the Callaway-Sant'Anna estimate, confirming that heterogeneous treatment effects across cohorts are not a major concern---unsurprising given that the effects are near zero for all cohorts.


\section{Discussion}
\label{sec:discussion}

\subsection{Interpreting the Null Result}

The central finding of this paper is that IMLC adoption had no detectable effect on aggregate state-level healthcare employment, establishments, or wages. How should we interpret this null?

Three explanations merit consideration. The first and most straightforward is that the IMLC simply did not change physician behavior in economically meaningful ways. Under this view, the administrative burden of multi-state licensure was a minor inconvenience rather than a binding constraint, and reducing it had negligible labor market effects. This interpretation is difficult to reconcile with the evidence from \citet{deyoGhosh2023}, who document a 3 percent increase in out-of-state physician practice following IMLC adoption.

The second explanation---and the one I consider most plausible---is that the IMLC facilitated cross-border virtual practice without generating measurable changes in aggregate employment. If physicians use compact licenses to provide telehealth consultations to patients in other states while remaining physically located in their home state, the QCEW would record no change in employment. The physician's establishment remains in the original state; only the patient's location changes. This interpretation is directly supported by \citet{ohKleiner2025}, who find that universal licensing recognition increases patient access without increasing physician migration. The null result on establishments is particularly consistent: if physicians serve remote patients from existing offices, no new establishments are created.

Under this interpretation, the IMLC may have succeeded in its primary goal of expanding access to care across state lines, even though aggregate employment statistics do not capture this expansion. The policy's effects are real but invisible to establishment-based employment measures. Data from the IMLC Commission show that over 30{,}000 expedited licenses were issued through the compact in its first five years---physicians are clearly using the pathway. The question is whether these licenses translate into new healthcare jobs or merely enable existing providers to extend their virtual reach. Future research using claims-level data (which record both provider and patient location) would be better positioned to detect these virtual flows.

The third explanation is that the IMLC expanded supply in some states while contracting it in others, with offsetting effects that net to zero at the aggregate level. If physicians shift their attention to patients in newly accessible states, patients in the physician's home state might experience reduced access. This redistribution story is harder to test with state-level data but could be examined with within-state geographic variation in physician supply.

\subsection{Pre-Trends and Identification}

The positive pre-treatment coefficients in the event study (\Cref{tab:eventstudy}) warrant careful discussion. The finding that healthcare employment was growing faster in eventually-treated states than in never-treated states before IMLC adoption could reflect violations of the parallel trends assumption.

However, several features of the data make a violation less concerning. First, the pre-trend is declining toward zero approaching the treatment date, suggesting convergence rather than divergence. If unobserved confounders were driving both adoption and employment growth, we would expect the pre-trend to intensify near the treatment date, not attenuate. Second, the post-treatment coefficients show a sharp break from the pre-treatment pattern, clustering tightly around zero rather than continuing the pre-treatment trajectory. This pattern is more consistent with the pre-trends reflecting level differences in growth rates that are unrelated to the IMLC than with a confounding story.

Following the framework of \citet{rambachanRoth2023}, the interpretation of the null result is arguably strengthened rather than weakened by the pre-trends. Their ``honest inference'' approach asks: what treatment effects are consistent with the data under varying assumptions about the degree of parallel trends violations? Because the pre-treatment trend differences are \textit{declining} toward zero (from 0.026 at $k=-5$ to 0.005 at $k=-2$), extrapolating this trend into the post-treatment period would predict \textit{negative} post-treatment coefficients---exactly what we observe. In other words, even under a sensitivity analysis that allows for modest continued violation of parallel trends, the implied treatment effects remain close to zero. The post-treatment coefficients are, if anything, slightly more negative than what the pre-trend trajectory alone would predict, consistent with a genuine null effect layered on top of convergent pre-trends \citep{roth2023pretrends}.

\subsection{Comparison with Existing Literature}

My results complement rather than contradict the existing literature on the IMLC and occupational licensing reform more broadly.

\citet{deyoGhosh2023} find that IMLC adoption increased out-of-state physician practice by approximately 3 percent. My null result on total employment is consistent with this: if physicians obtain additional state licenses but continue practicing from their home state (via telehealth), state-level employment counts would not change.

\citet{ohKleiner2025} find that universal licensing recognition increases patient utilization without increasing physician migration. This is precisely the mechanism consistent with my null finding---patients gain access, but physicians do not relocate, so establishment-based employment data show no effect.

More broadly, the occupational licensing literature has documented substantial effects of licensing on wages and entry \citep{kleinerKrueger2013, kleiner2000occupational}. My results suggest that \textit{reducing} licensing barriers through interstate compacts does not generate symmetric effects on aggregate supply, at least in the short to medium run. This asymmetry could reflect the difference between removing barriers to initial entry (which creates new workers) and reducing barriers to multi-state practice (which enables existing workers to serve more markets).

\subsection{Limitations}

Several limitations should be acknowledged. First, the QCEW measures employment at the establishment level based on physical location. It cannot capture cross-border telehealth activity, which is likely the primary channel through which the IMLC operates. This is a fundamental measurement limitation, not a flaw in the research design, but it means the null result should not be interpreted as evidence that the IMLC had no effects on healthcare delivery.

Second, the analysis uses annual data at the state level, which may be too coarse to detect effects that operate at finer geographic or temporal scales. County-level or quarterly data could reveal heterogeneous effects within states---for example, effects concentrated in border regions or in health professional shortage areas.

Third, the sample period of 2014--2023 provides only 3 years of pre-treatment data for the first adoption cohort (2017) and even less for later cohorts. The limited pre-treatment period makes it harder to assess parallel trends and reduces power for detecting gradual effects.

Fourth, 11 never-treated states is a relatively small comparison group. While \citet{callaway2021difference} allows for finite-sample inference, the estimates are necessarily less precise than they would be with a larger control group.

Fifth, the never-treated states (Alaska, California, Massachusetts, New York, Oregon, Virginia, and others) differ systematically from treated states in size, urbanization, and political orientation. These differences do not invalidate the research design---identification requires parallel trends, not identical levels---but they raise questions about external validity.


\section{Conclusion}
\label{sec:conclusion}

The Interstate Medical Licensure Compact represents one of the most ambitious reforms of occupational licensing in recent US history, with 40 states adopting an expedited pathway for multi-state physician licensure within seven years. Using a staggered difference-in-differences design with modern heterogeneity-robust estimators, I find that IMLC adoption had no detectable effect on aggregate healthcare employment, establishment counts, or wages.

This null result is informative, not empty. The IMLC was designed to reduce barriers to cross-border practice, and existing evidence suggests it succeeded in enabling more physicians to hold licenses in multiple states. But this increased licensing did not translate into measurable changes in where physicians are employed or how many healthcare establishments exist. The most likely explanation is that the compact facilitates virtual cross-border practice---physicians serve patients in other states via telehealth while remaining physically located at their existing establishments. This mechanism is invisible to employment statistics but potentially significant for patient access.

The policy implications are nuanced. Interstate licensing compacts may be effective at expanding healthcare access through telehealth without creating the kinds of aggregate labor market effects that traditional supply-side interventions generate. Policymakers evaluating the IMLC's impact should focus on patient-level access measures---telehealth utilization, appointment wait times, healthcare spending---rather than employment statistics, which are poorly suited to detecting the virtual channels through which the compact operates.

For the broader occupational licensing debate, my results suggest an important asymmetry: while licensing requirements can create substantial barriers to entry and reduce supply \citep{kleiner2006licensing}, reducing those barriers through interstate compacts does not necessarily generate symmetric increases in aggregate supply. The margin on which these reforms operate---enabling existing practitioners to serve patients across state lines---is fundamentally different from the margin of new entry into the profession.

Future research should examine the IMLC's effects using data that can track cross-border healthcare delivery. Insurance claims data, which record both provider and patient location, would allow researchers to directly measure whether compact adoption increased the volume of cross-state telehealth encounters. Patient-level measures of healthcare access and utilization in underserved areas would also help determine whether the IMLC is fulfilling its promise of expanding care to those who need it most. Interstate compacts may well expand the reach of medicine, but they do not appear to grow the ranks of those who practice it.


\section*{Acknowledgements}

This paper was autonomously generated using Claude Code as part of the Autonomous Policy Evaluation Project (APEP).

\noindent\textbf{Project Repository:} \url{https://github.com/SocialCatalystLab/ape-papers}

\noindent\textbf{Contributors:} @ai1scl

\noindent\textbf{First Contributor:} \url{https://github.com/ai1scl}

\label{apep_main_text_end}
\newpage
\bibliography{references}

\newpage
\appendix

\section{Data Appendix}
\label{app:data}

\subsection{QCEW Data Access}

All QCEW data were obtained from the BLS public API at \url{https://data.bls.gov/cew/data/api/}. The API endpoint returns CSV files for a given year and NAICS industry code. For each year from 2014 to 2023, I requested data for NAICS sectors 62 (Healthcare and Social Assistance), 621 (Ambulatory Health Care Services), 622 (Hospitals), and 72 (Accommodation and Food Services).

State-level observations were identified by filtering for area codes matching the pattern ``SS000'' (where SS is a two-digit state FIPS code). The raw data include separate rows for different ownership types (federal, state, local, and private). I aggregated across all ownership types to obtain total employment, establishment counts, and wages for each state-industry-year cell.

\subsection{IMLC Treatment Coding}

Treatment dates were coded from the following sources:
\begin{itemize}
\item Interstate Medical Licensure Compact Commission official website \citep{imlcc2024}
\item Federation of State Medical Boards resource guides \citep{fsmb2024}
\item State legislative records and FSMB press releases
\end{itemize}

The treatment year represents the year in which a state's medical board became operational within the compact and began processing applications---not the year of legislative enactment, which may precede operationalization. All states that enacted enabling legislation before April 2017 are coded as operational in 2017, when the compact first became active.

States that adopted the IMLC in 2024 or later (Florida, Arkansas, North Carolina) are coded as never-treated because their adoption falls outside the QCEW data window (2014--2023).

\subsection{ACS Work-from-Home Data}

Work-from-home rates were obtained from the American Community Survey 1-year estimates, Table B08301 (Means of Transportation to Work). The work-from-home share is computed as $\text{WFH Workers} / \text{Total Workers}$. ACS 1-year estimates are available for all years in the sample except 2020, when the Census Bureau suspended 1-year releases.

\subsection{Variable Definitions}

\begin{table}[H]
\centering
\caption{Variable Definitions}
\label{tab:vars}
\small
\begin{tabular}{lp{10cm}}
\toprule
Variable & Definition \\
\midrule
\texttt{log\_hc\_emp} & Natural log of annual average healthcare employment (NAICS 62) \\
\texttt{log\_amb\_emp} & Natural log of annual average ambulatory care employment (NAICS 621) \\
\texttt{log\_hosp\_emp} & Natural log of annual average hospital employment (NAICS 622) \\
\texttt{log\_hc\_estabs} & Natural log of annual average healthcare establishment count (NAICS 62) \\
\texttt{log\_hc\_avgpay} & Natural log of average annual pay in healthcare, computed as total wages / total employment \\
\texttt{log\_plc\_emp} & Natural log of annual average accommodation employment (NAICS 72), used as placebo \\
\texttt{treated} & $= 1$ if state $s$ has adopted the IMLC by year $t$; $= 0$ otherwise \\
\texttt{first\_treat} & Year of IMLC adoption; 0 for never-treated states (the \texttt{did} package treats $G=0$ as $G=\infty$) \\
\texttt{wfh\_share} & Share of workers working primarily from home (ACS 1-year) \\
\bottomrule
\end{tabular}
\end{table}


\section{Identification Appendix}
\label{app:identification}

\subsection{Pre-Trends Analysis}

\Cref{fig:pretrends} presents a focused view of the pre-treatment parallel trends for healthcare employment. The figure plots average log healthcare employment over time separately for eventually-treated and never-treated states, allowing visual inspection of the parallel trends assumption.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/fig2_pretrends.pdf}
\caption{Pre-Treatment Trends in Healthcare Employment}
\label{fig:pretrends}
\floatfoot{\textit{Notes:} Average log healthcare employment (NAICS 62) for eventually-treated states (those that adopt the IMLC by 2023) and never-treated states. The vertical dashed line marks the first IMLC adoption year (2017).}
\end{figure}

\subsection{Sun-Abraham Event Study}

As an alternative to the Callaway-Sant'Anna estimator, I estimate the event study using the \citet{sunAbraham2021} interaction-weighted estimator. This estimator uses cohort-specific relative-time indicators and produces coefficient estimates that are robust to heterogeneous treatment effects. The Sun-Abraham estimates are qualitatively similar to the Callaway-Sant'Anna results, confirming the null finding across estimators.

\subsection{State-Level Treatment Assignment}

\begin{table}[H]
\centering
\caption{IMLC Adoption by State}
\label{tab:treatment}
\small
\begin{tabular}{llc|llc}
\toprule
State & Abbr & Year & State & Abbr & Year \\
\midrule
Alabama & AL & 2017 & Nebraska & NE & 2017 \\
Arizona & AZ & 2017 & Nevada & NV & 2017 \\
Colorado & CO & 2017 & New Hampshire & NH & 2017 \\
Idaho & ID & 2017 & Pennsylvania & PA & 2017 \\
Illinois & IL & 2017 & South Dakota & SD & 2017 \\
Iowa & IA & 2017 & Tennessee & TN & 2017 \\
Kansas & KS & 2017 & Utah & UT & 2017 \\
Maine & ME & 2017 & Washington & WA & 2017 \\
Minnesota & MN & 2017 & West Virginia & WV & 2017 \\
Mississippi & MS & 2017 & Wisconsin & WI & 2017 \\
Montana & MT & 2017 & Wyoming & WY & 2017 \\
\midrule
DC & DC & 2018 & Michigan & MI & 2018 \\
Maryland & MD & 2018 & Vermont & VT & 2018 \\
\midrule
Georgia & GA & 2019 & Oklahoma & OK & 2019 \\
Kentucky & KY & 2019 & North Dakota & ND & 2019 \\
\midrule
Louisiana & LA & 2020 & & & \\
\midrule
Delaware & DE & 2021 & Texas & TX & 2021 \\
Ohio & OH & 2021 & & & \\
\midrule
Connecticut & CT & 2022 & New Jersey & NJ & 2022 \\
Indiana & IN & 2022 & Rhode Island & RI & 2022 \\
\midrule
Hawaii & HI & 2023 & Missouri & MO & 2023 \\
\midrule
\multicolumn{6}{l}{\textit{Never Treated (in sample period):}} \\
\multicolumn{6}{l}{AK, AR, CA, FL, MA, NM, NY, NC, OR, SC, VA} \\
\bottomrule
\end{tabular}
\end{table}


\section{Robustness Appendix}
\label{app:robustness}

\subsection{Goodman-Bacon Decomposition}

The \citet{goodmanBacon2021} decomposition reveals the composition of the TWFE estimator by separating the overall estimate into weighted averages of all possible $2 \times 2$ DiD comparisons. In this setting, the decomposition shows that the majority of identifying variation comes from treated-vs-never-treated comparisons (the cleanest source of variation), with smaller contributions from early-vs-late timing comparisons.

The concordance between the TWFE and Callaway-Sant'Anna estimates suggests that heterogeneity bias---the primary concern motivating modern staggered DiD estimators---is minimal in this setting. This is consistent with the finding that treatment effects are near zero for all cohorts and time periods.

\subsection{COVID Sensitivity}

The COVID-19 pandemic caused unprecedented disruptions to healthcare employment beginning in March 2020. To assess whether pandemic dynamics confound my estimates, I re-estimate the main specification excluding 2020 and 2021. The resulting ATT of $-0.008$ log points (SE $= 0.011$) is negligibly different from the baseline estimate of $-0.005$, indicating that COVID-era observations are not driving the null result.

I also restrict the sample to pre-2020 adoption cohorts only (2017, 2018, 2019), whose treatment effects can be estimated entirely from pre-pandemic variation. The ATT is $-0.005$ log points (SE $= 0.012$), virtually identical to the full-sample estimate.

\subsection{Alternative Control Group}

Replacing never-treated states with not-yet-treated states as the comparison group yields an ATT of $0.001$ log points (SE $= 0.009$). The not-yet-treated specification is more demanding because it requires that states not yet in the IMLC serve as valid counterfactuals, even though they may differ in unobservable ways from already-treated states. The near-zero estimate under this alternative control group provides further evidence of a genuine null effect.


\section{Heterogeneity Appendix}
\label{app:heterogeneity}

\subsection{By Adoption Cohort}

The absence of cohort heterogeneity (\Cref{fig:cohort}) is noteworthy. The 2017 cohort (22 states) has the longest post-treatment horizon and the most precise estimates, yet shows no significant effect. Later cohorts with shorter post-treatment periods show similar null effects, though with wider confidence intervals.

One might expect first-mover effects: early adopters could benefit from reduced competition for multi-state physicians before the compact becomes widespread. Alternatively, later adopters might benefit more from network effects as the number of member states grows. Neither pattern appears in the data.

\subsection{By State Size}

While I do not present formal heterogeneity estimates by state size, the summary statistics (\Cref{tab:summary}) show that never-treated states are substantially larger than treated states. The null result is therefore not driven by comparing large states to small states---within the treated group, states of all sizes show similar lack of response to IMLC adoption.


\section{Additional Figures and Tables}
\label{app:additional}

This section collects supplementary exhibits referenced throughout the paper.

\input{tables/tab4_event_study.tex}


\end{document}
