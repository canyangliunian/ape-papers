{
  "paper_id": "apep_0066",
  "scan_date": "2026-02-06T12:39:38.152002+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "CLEAN",
  "files_scanned": 6,
  "flags": [
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "paper.tex",
      "lines": [
        33,
        34,
        241,
        242,
        335,
        336
      ],
      "evidence": "Key numerical results (point estimates, SEs, CIs) appear as literals in the manuscript tables/abstract, but the provided code does not generate LaTeX tables or otherwise programmatically write these numbers into paper.tex. The analysis scripts do compute TWFE and Callaway-Sant'Anna estimates and print them to console / save RDS objects, but the manuscript values could have been manually transcribed. This is not proof of wrongdoing, but it is a reproducibility/integrity risk because the paper cannot be mechanically verified against the computed objects without an explicit table-generation step or a documented copy/paste protocol.: I find a precisely estimated null effect: PFL adoption does not significantly change the female self-employment rate, with a point estimate of $-0.19$ percentage points (SE = 0.14).\n...\nA placebo test on male self-employment shows no significant effect ($-0.28$ pp, SE = 0.21).\n...\nATT (percentage points) & $-0.163$ & $-0.187$ \\\\\n& $(0.150)$ & $(0.140)$ \\\\\n...\n95\\% CI & $[-0.46, 0.14]$ & $[-0.46, 0.09]$",
      "confidence": 0.72
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        31
      ],
      "evidence": "A random seed is set globally, but no random-number-based simulation/bootstrapping/permutation is present in the provided scripts. This is likely harmless boilerplate, but in an audit context it is worth confirming that no omitted scripts use RNG to generate or alter core outcomes. No evidence of fabricated/simulated outcome data is present in the files provided.: set.seed(20260127)",
      "confidence": 0.55
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "LOW",
      "file": "02_clean_data.R",
      "lines": [
        20,
        21,
        22,
        23,
        24
      ],
      "evidence": "The manuscript states ACS 2020 1-year estimates are not released (so DC-2020 is not in the data). The code still drops DC-2020 explicitly. This does not change results if the row truly does not exist, but it signals a minor mismatch between stated sample construction and code logic (the filter suggests the author anticipated DC-2020 could be present).: # Exclude year 2020 (COVID disruption, already missing from ACS 1-year)\nfilter(!is.na(year)) %>%\n# Drop partial-year treatment observations ...\nfilter(!(state_abbr == \"NJ\" & year == 2009)) %>%\nfilter(!(state_abbr == \"DC\" & year == 2020)) %>%\nfilter(!(state_abbr == \"OR\" & year == 2023))",
      "confidence": 0.6
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "CLEAN",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 0,
      "MEDIUM": 1,
      "LOW": 2
    },
    "one_liner": "Minor issues only",
    "executive_summary": "Minor code quality issues detected, but no evidence of data fabrication or manipulation.",
    "top_issues": [],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0066_scan.json"
  },
  "error": null
}