{
  "paper_id": "apep_0018",
  "scan_date": "2026-02-06T12:34:43.019545+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 2,
  "flags": [
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "HIGH",
      "file": "fetch_data.py",
      "lines": [
        63,
        88,
        96,
        101,
        110,
        125
      ],
      "evidence": "The fetch script can fall back to parsing a text file into a 2-column dataset (poverty_centered, mort_related), but the analysis script later expects multiple specific Ludwig\u2013Miller variables (e.g., mort_age59_related_postHS, mort_age59_injury_postHS, mort_age59_related_preHS). If the TXT fallback is used (e.g., because the DTA fetch fails), the saved data/headstart_treatment.csv will not contain the required outcome columns, causing the analysis to error or silently not run the intended specifications. This creates a serious reproducibility/provenance risk because the same codebase can produce materially different datasets depending on which URL succeeds.: urls = [\n    (\"https://raw.githubusercontent.com/rdpackages/rdrobust/master/R/headst.rda\", \"rda\"),\n    (\"https://raw.githubusercontent.com/rdpackages/rdrobust/master/stata/rdrobust_headst.dta\", \"dta\"),\n    (\"https://users.ssc.wisc.edu/~behansen/econometrics/headstart.txt\", \"txt\"),\n]\n...\nif fmt == 'txt':\n    ...\n    df = pd.DataFrame(data, columns=['poverty_centered', 'mort_related'])\n...\nelif fmt == 'dta':\n    ...\n    df = pd.read_stata(DATA_DIR / \"headst_temp.dta\")",
      "confidence": 0.85
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "analyze_rdd.py",
      "lines": [
        222,
        223,
        231,
        232,
        310,
        311,
        352
      ],
      "evidence": "The analysis implements the paper's RDD on specific post-Head-Start and placebo mortality variables, but the data-fetch pipeline does not guarantee these columns exist (see TXT fallback). As written, the analysis can (i) crash with KeyError when trying to access a missing outcome column (in rd_plot), or (ii) skip additional outcomes because of `if outcome_var in df.columns`, leading to partial/inconsistent outputs relative to the manuscript's reported figures/tables.: primary_outcome = 'mort_age59_related_postHS'\n...\nrd_plot_result = rd_plot(df, primary_outcome, primary_label, bandwidth=18, filename=\"figure2_rd_mortality.png\")\n...\nother_outcomes = [\n    ('mort_age59_injury_postHS', 'Injury Mortality (Ages 5-9)'),\n    ('mort_age59_all_postHS', 'All-Cause Mortality (Ages 5-9)'),\n    ('mort_age59_related_preHS', 'HS-Related Mortality (Pre-Treatment Placebo)'),\n]\n...\nplacebo_var = 'mort_age59_related_preHS'",
      "confidence": 0.9
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "paper.tex",
      "lines": [
        1
      ],
      "evidence": "The manuscript contains many fully-specified numerical results (point estimates, SEs, p-values, CIs, Ns, and a McCrary SE) directly embedded in LaTeX tables and text. In the provided code, results are saved to JSON (data/analysis_results.json) but there is no script that programmatically writes these LaTeX tables or inserts these numbers into paper.tex. This creates a high risk that manuscript numbers were manually transcribed (or could be altered) rather than automatically generated from the computed model objects. This is especially salient for the McCrary SE reported in the text (SE = 0.15), which is not computed anywhere in the code.: Our replication yields estimates consistent with the original finding... (\\tau = -1.20, SE = 0.66, p = 0.07; 95\\% CI: [-2.49, 0.10]). Validity tests support the design's credibility: the McCrary density test shows no evidence of manipulation (log discontinuity = -0.002, SE = 0.15), and placebo tests ... ($\\tau$ = -0.64, p = 0.72).\n...\n\\begin{table}[H]\n... RD Estimate ($\\tau$) & -1.632 & -1.830* & -1.212 & -1.198 & -1.304* \\\\\n& (0.933) & (0.846) & (0.802) & (0.662) & (0.614) \\\\\n95\\% CI & [-3.46, 0.20] & [-3.49, -0.17] & [-2.78, 0.36] & [-2.49, 0.10] & [-2.51, -0.10] \\\\\np-value & 0.08 & 0.03 & 0.13 & 0.07 & 0.03 \\\\\nN & 571 & 645 & 807 & 954 & 1,059 \\\\\n\\end{table}",
      "confidence": 0.8
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "analyze_rdd.py",
      "lines": [
        33,
        41,
        70,
        71,
        103,
        104
      ],
      "evidence": "The manuscript describes a McCrary density test with an estimated log discontinuity and standard error. The implemented function is not a McCrary (2008) estimator: it bins the running variable, runs simple global linear regressions of bin counts on bin centers on each side, and computes a log difference with an ad hoc `+ 1` inside the log. It does not compute (or return) a standard error or a formal test statistic. Therefore, the paper's McCrary SE and inference are not supported by the code as provided, and even the point estimate definition differs from the standard McCrary log density discontinuity.: def mccrary_density_test(df, bandwidth=18):\n    ...\n    slope_left, intercept_left, r_left, p_left, se_left = stats.linregress(X_left, y_left)\n    ...\n    slope_right, intercept_right, r_right, p_right, se_right = stats.linregress(X_right, y_right)\n    ...\n    log_disc = np.log(pred_at_cutoff_right + 1) - np.log(pred_at_cutoff_left + 1)\n    ...\n    return {\n        'log_discontinuity': log_disc,\n        'pred_left': pred_at_cutoff_left,\n        'pred_right': pred_at_cutoff_right\n    }",
      "confidence": 0.85
    }
  ],
  "file_verdicts": [
    {
      "file": "fetch_data.py",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "analyze_rdd.py",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "paper.tex",
      "verdict": "SUSPICIOUS"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 3,
      "MEDIUM": 1,
      "LOW": 0
    },
    "one_liner": "unclear provenance; method mismatch; hard-coded results",
    "executive_summary": "The data pipeline does not reliably establish provenance or required structure: `fetch_data.py` can fall back to parsing a simple two-column text file (`poverty_centered`, `mort_related`), even though the RDD implementation in `analyze_rdd.py` expects a much richer set of specific Ludwig\u2013Miller outcome and placebo variables (e.g., post\u2013Head Start and placebo mortality measures). As written, the analysis can therefore run on incomplete/incorrect inputs or fail silently, making the empirical methodology in the code inconsistent with what the paper describes. Meanwhile, `paper.tex` hard-codes extensive numerical results (estimates, SEs, p-values, CIs, sample sizes, and a McCrary SE) directly into the manuscript rather than generating them from the provided code, preventing end-to-end reproducibility.",
    "top_issues": [
      {
        "category": "DATA_PROVENANCE_MISSING",
        "severity": "HIGH",
        "short": "The fetch script can fall back to parsing a text file int...",
        "file": "fetch_data.py",
        "lines": [
          63,
          88
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0018/code/fetch_data.py#L63-L125"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The analysis implements the paper's RDD on specific post-...",
        "file": "analyze_rdd.py",
        "lines": [
          222,
          223
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0018/code/analyze_rdd.py#L222-L352"
      },
      {
        "category": "HARD_CODED_RESULTS",
        "severity": "HIGH",
        "short": "The manuscript contains many fully-specified numerical re...",
        "file": "paper.tex",
        "lines": [
          1
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0018/code/paper.tex#L1"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0018_scan.json"
  },
  "error": null
}