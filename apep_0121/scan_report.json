{
  "paper_id": "apep_0121",
  "scan_date": "2026-02-06T12:46:58.904252+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "CLEAN",
  "files_scanned": 8,
  "flags": [
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        96,
        132,
        150
      ],
      "evidence": "The fetch loop attempts to download ACS 1-year estimates for 2010\u20132022, including 2020 (which the manuscript states was not released) and earlier years (2010\u20132014) that are outside the manuscript\u2019s stated analysis window (2015\u20132022 excluding 2020). The script silently proceeds when a year fails and later binds all years that succeeded. Without an explicit filter to 2015\u20132019, 2021\u20132022 at the end of the pipeline (and since the main analysis script is not provided here), it is unclear whether the estimation sample truly matches the manuscript\u2019s stated panel.: acs_years <- 2010:2022\n...\nurl <- sprintf(\n  \"https://api.census.gov/data/%d/acs/acs1?get=%s&for=state:*\",\n  yr, var_string\n)\n...\nraw <- fetch_json(url)\ndf  <- census_to_df(raw)\n\nif (!is.null(df)) {\n  df$year <- yr\n  acs_list[[as.character(yr)]] <- df\n} else {\n  cat(\"    -> FAILED for year\", yr, \"\\n\")\n}",
      "confidence": 0.74
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        205,
        223,
        240
      ],
      "evidence": "The entire state minimum wage panel is manually hard-coded via literals by state-year. This is not inherently an integrity problem because both the manuscript and code comments explicitly describe minimum wage history as coming from DOL/NCSL/EPI sources, but it creates a provenance/auditability risk: there is no machine-readable source file, citation-per-entry, or reproducible scrape to verify the exact values. A small transcription error could materially change treatment timing (first_treat) and therefore the DiD cohorts.: cat(\"\\n--- Part B: Building state minimum wage panel ---\\n\")\n...\n# Create base panel: 51 entities x 13 years\nmw_base <- expand.grid(\n  state_fips = all_fips,\n  year       = 2010:2022,\n  stringsAsFactors = FALSE\n) %>%\n  mutate(state_mw = federal_mw)  # default: all states start at federal\n...\n# Alaska (02): CPI-indexed since 2003; $8.75 ballot measure 2015\nmw_base <- set_mw(mw_base, \"02\", list(\n  \"2010\" = 7.75, \"2011\" = 7.75, ... \"2022\" = 10.34\n))\n...\n# ... many states hard-coded similarly ...",
      "confidence": 0.83
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "MEDIUM",
      "file": "06_tables.R",
      "lines": [
        90,
        146,
        160
      ],
      "evidence": "The Sun\u2013Abraham \u2018treatment effect\u2019 reported in the main results table is computed as a simple mean of multiple post-treatment event-study coefficients, and its SE is computed as sqrt(mean(se^2)). This is not the correct standard error for an average of correlated estimates (it ignores the covariance structure and weights). This can misstate precision and, depending on covariance signs, can under- or over-estimate the SE. While not \u201cfabrication,\u201d it is a hard-coded aggregation rule that may not match the manuscript\u2019s stated Sun\u2013Abraham \u2018mean post-treatment effect\u2019 unless the paper explicitly defines it this way and uses correct inference.: if (!is.null(sa)) {\n  sa_coefs <- coef(sa)\n  sa_names <- names(sa_coefs)\n  post_coefs <- sa_coefs[grepl(\"::\", sa_names)]\n  if (length(post_coefs) > 0) {\n    sa_est <- mean(post_coefs)\n    # Approximate SE from individual SEs\n    sa_ses <- se(sa)[grepl(\"::\", names(se(sa)))]\n    sa_se  <- sqrt(mean(sa_ses^2))\n  }\n}",
      "confidence": 0.86
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "04_robustness.R",
      "lines": [
        112,
        119,
        126
      ],
      "evidence": "The manuscript describes the main identification strategy as CS-DiD with treatment defined by a $1 gap threshold; these TWFE dose-response models are labeled robustness checks, which is fine. However, the variable naming and construction of mw_gap_continuous depend on prior cleaning (02_clean_data.R). If the panel includes years outside the manuscript sample (see earlier finding) and is not filtered before these regressions, these robustness regressions may be estimated on a different time window than reported in the paper (unless filtering occurs elsewhere).: dose_linear <- feols(\n  pct_with_parents ~ mw_gap_continuous | state_fips + year,\n  data    = panel,\n  cluster = ~state_fips\n)\n...\npanel$mw_gap_sq <- panel$mw_gap_continuous^2\n\ndose_quad <- feols(\n  pct_with_parents ~ mw_gap_continuous + mw_gap_sq | state_fips + year,\n  data    = panel,\n  cluster = ~state_fips\n)",
      "confidence": 0.62
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "MEDIUM",
      "file": "04_robustness.R",
      "lines": [
        201,
        207
      ],
      "evidence": "The covariate-adjusted CS-DiD restricts the sample by dropping observations with missing unemployment_rate or log_rent. This can change the identifying sample and potentially induce selection if missingness is non-random (e.g., if particular years failed to fetch from the BLS API). The manuscript frames unemployment as a balance test and rent as a mediator; using them as controls is labeled robustness, but any sample attrition should be reported (N, which states/years dropped) to avoid post-hoc selection concerns.: panel_cov <- panel %>%\n  filter(!is.na(unemployment_rate) & !is.na(log_rent))\n\ncs_controls <- tryCatch({\n  att_gt(\n    ...,\n    data        = panel_cov,\n    xformla     = ~ unemployment_rate + log_rent,\n    ...\n  )\n}, ...)",
      "confidence": 0.68
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        49
      ],
      "evidence": "A global random seed is set. No simulated data generation is present in the provided scripts; however, setting a seed at the project level is sometimes associated with bootstrapping/clustered bootstrap inference (which the manuscript uses) and is generally appropriate. This is flagged only because the audit checklist includes seed setting as a potential marker; in context, it appears consistent with reproducible bootstrap SEs.: set.seed(20240101)                             # Reproducibility",
      "confidence": 0.55
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "LOW",
      "file": "00_packages.R",
      "lines": [
        66
      ],
      "evidence": "The script uses here::here() as a fallback for path detection, but the 'here' package is not included in required_packages nor loaded. This is not an integrity issue per se, but it can cause non-reproducible behavior depending on the working directory and may silently change where data/tables are written/read if the earlier directory checks fail.: PAPER_DIR <- here::here()",
      "confidence": 0.7
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "07_revision_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "CLEAN",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 0,
      "MEDIUM": 4,
      "LOW": 3
    },
    "one_liner": "Minor issues only",
    "executive_summary": "Minor code quality issues detected, but no evidence of data fabrication or manipulation.",
    "top_issues": [],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0121_scan.json"
  },
  "error": null
}