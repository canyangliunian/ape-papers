{
  "paper_id": "apep_0074",
  "scan_date": "2026-02-06T12:41:08.681548+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 8,
  "flags": [
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "07_tables.R",
      "lines": [
        24,
        56
      ],
      "evidence": "Table 1 (adoption timeline) is manually hard-coded with pre/post-year counts and even a different implied sample window (\"2005-2017\"). These quantities should be derived from the analysis panel (1999\u20132017 per manuscript and policy panel code). Several entries appear inconsistent with the manuscript (e.g., CT pre-years shown as \"0 (pre-sample)\" vs manuscript explicitly uses 1999 as a (contaminated) nominal pre-year; CA pre-years shown as \"11\" despite manuscript having 17 pre-years in 1999\u20132015). This creates a high risk that the paper\u2019s tables do not reflect the computed dataset and could be edited to present desired values.: erpo_table <- tribble(\n  ~State, ~`Effective Date`, ~`Treatment Year`, ~`Pre-Treatment Years`, ~`Post-Treatment Years`,\n  \"Connecticut\", \"Oct 1999\", \"2000\", \"0 (pre-sample)\", \"13 (2005-2017)\",\n  \"Indiana\", \"Jul 2005\", \"2006\", \"1\", \"12\",\n  \"California\", \"Jan 2016\", \"2016\", \"11\", \"2\",\n  \"Washington\", \"Dec 2016\", \"2017\", \"12\", \"1\",\n  \"Oregon\", \"Jan 2018\", \"2018\", \"13\", \"0 (post-sample)\",\n  \"Florida\", \"Mar 2018\", \"2019\", \"13\", \"0 (post-sample)\",\n  ...\n)",
      "confidence": 0.9
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "07_tables.R",
      "lines": [
        167,
        183
      ],
      "evidence": "Table 3 hard-codes sample sizes (states=51, years=2005\u20132017, N=663, treated states=4). This contradicts both the manuscript (main results exclude CT, 50 jurisdictions, 1999\u20132017, N=950) and the analysis code (04_main_analysis.R sets main sample as CT-excluded, 1999\u20132017). Even if an alternate 2005\u20132017 sample existed, it is not constructed anywhere in the provided code. Hard-coding these values can misrepresent the actual estimation sample and threatens integrity of reported results.: cat(\"\\\\midrule\nStates & 51 & 51 & 51 & 51 \\\\\\\\\nYears & 2005-2017 & 2005-2017 & 2005-2017 & 2005-2017 \\\\\\\\\nObservations & 663 & 663 & 663 & 663 \\\\\\\\\nTreated States & 4 & 4 & 4 & 4 \\\\\\\\\n\\\\bottomrule\n\\\\end{tabular}\")",
      "confidence": 0.95
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "HIGH",
      "file": "07_tables.R",
      "lines": [
        74,
        132
      ],
      "evidence": "The tables script labels the sample as 2005\u20132017 (and uses pre/post-year logic consistent with a 2005 start), but the data pipeline (01_policy_data.R, 02_fetch_mortality_data.R, 03_merge_data.R) constructs and saves a 1999\u20132017 panel, and the manuscript\u2019s identification discussion relies on 1999\u20132017. If the paper is built from these tables, the manuscript and computed analysis are misaligned on the core sample window and thus on the identifying variation (especially CT/IN pre-periods).: summ_stats <- df %>%\n  mutate(\n    Group = case_when(\n      first_treat == 0 ~ \"Never Treated\",\n      first_treat <= 2006 ~ \"Early Adopters\",\n      TRUE ~ \"Later Adopters\"\n    )\n  )\n...\ncat(\"...\\nSample: 2005-2017.\\n\")",
      "confidence": 0.85
    },
    {
      "category": "SELECTIVE_REPORTING",
      "severity": "MEDIUM",
      "file": "07_tables.R",
      "lines": [
        140,
        158
      ],
      "evidence": "The manuscript repeatedly emphasizes that conventional inference is unreliable with only ~3 treated clusters, yet the table-generation code adds significance stars using a normal critical value and prints \u201c* p < 0.05.\u201d This encourages selective \u2018significance\u2019 interpretation contrary to the manuscript\u2019s stated inference caveat. While not fabrication, it is inconsistent reporting that could mislead readers.: format_est <- function(est, se, alpha = 0.05) {\n  t_stat <- abs(est / se)\n  if (t_stat > qnorm(1 - alpha/2)) {\n    return(sprintf(\"%.3f*\", est))\n  } else {\n    return(sprintf(\"%.3f\", est))\n  }\n}\n...\n\\item Notes: ... * p < 0.05.",
      "confidence": 0.8
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "05_figures.R",
      "lines": [
        118,
        148
      ],
      "evidence": "Figure 3 constructs cohort-specific confidence intervals using the *overall* SE as a proxy when cohort SEs are NA. This is not a valid inferential procedure and can materially distort uncertainty by understating or overstating cohort-specific variability. The plot subtitle says \u201cPoint estimates only; CIs unavailable,\u201d but the code still computes CI bounds (even if not drawn explicitly in the final ggplot, the data frame contains them). This is methodologically misleading unless the figure is strictly point-estimates-only and no CI geometry is used (currently no ribbon/errorbar is plotted, but the proxy-CI computation is still a red flag for potential future plotting/reporting).: # Note: SEs are NA for group-specific effects with few units\n# Use overall SE as a rough approximation for CIs (conservative)\noverall_se <- grp$overall.se  # Use overall SE as proxy\n...\nmutate(\n  # Use overall SE as a conservative approximation for CIs\n  se = overall_se,\n  ci_lower = estimate - 1.96 * se,\n  ci_upper = estimate + 1.96 * se,\n...",
      "confidence": 0.75
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_policy_data.R",
      "lines": [
        9,
        33
      ],
      "evidence": "ERPO effective dates/treatment years are manually encoded with no machine-readable provenance (no PDAPS/Giffords/Everytown data import, no stored citation file, no checks). Given treatment timing is the key explanatory variable, lack of auditable provenance increases the risk of coding errors or discretionary choices affecting results. The manuscript cites sources generally, which helps, but the code does not link to exact source records or document verification.: erpo_adoption <- tribble(\n  ~state_abbr, ~state_name, ~effective_date, ~treatment_year,\n  \"CT\", \"Connecticut\",   \"1999-10-01\", 2000,\n  \"IN\", \"Indiana\",       \"2005-07-01\", 2006,\n  \"CA\", \"California\",    \"2016-01-01\", 2016,\n  \"WA\", \"Washington\",    \"2016-12-08\", 2017,\n  ...\n)",
      "confidence": 0.8
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "03_merge_data.R",
      "lines": [
        50,
        56
      ],
      "evidence": "Takes log of suicide_rate without guarding against zero/negative values. If any state-year had suicide_rate = 0 or missing, this would create -Inf/NA and could silently affect downstream models (e.g., robustness log spec). Suicide rates are typically >0, so this is likely benign, but it should be explicitly handled (e.g., filter/small offset) and documented.: analysis_data <- analysis_data %>%\n  mutate(\n    # State numeric ID\n    state_id = as.numeric(factor(state_abbr)),\n    # Log suicide rate for semi-elasticity interpretation\n    log_suicide_rate = log(suicide_rate)\n  )",
      "confidence": 0.55
    }
  ],
  "file_verdicts": [
    {
      "file": "02_fetch_mortality_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "07_tables.R",
      "verdict": "SUSPICIOUS"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_merge_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "01_policy_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "CLEAN"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 3,
      "MEDIUM": 3,
      "LOW": 1
    },
    "one_liner": "hard-coded results; method mismatch",
    "executive_summary": "The tables script (`07_tables.R`) manually hard-codes key results instead of deriving them from the analysis panel, including Table 1\u2019s pre/post-year counts and an implied sample window of 2005\u20132017. It also hard-codes Table 3\u2019s sample description and sizes (51 states, 2005\u20132017, N=663, 4 treated states), which contradict both the manuscript (50 jurisdictions excluding CT, 1999\u20132017, N=950) and the data pipeline that constructs a 1999\u20132017 panel. Overall, the reporting logic in the tables is inconsistent with the actual data preparation and could produce tables that don\u2019t correspond to the analyzed dataset.",
    "top_issues": [
      {
        "category": "HARD_CODED_RESULTS",
        "severity": "HIGH",
        "short": "Table 1 (adoption timeline) is manually hard-coded with p...",
        "file": "07_tables.R",
        "lines": [
          24,
          56
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0074/code/07_tables.R#L24-L56"
      },
      {
        "category": "HARD_CODED_RESULTS",
        "severity": "HIGH",
        "short": "Table 3 hard-codes sample sizes (states=51, years=2005\u201320...",
        "file": "07_tables.R",
        "lines": [
          167,
          183
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0074/code/07_tables.R#L167-L183"
      },
      {
        "category": "METHODOLOGY_MISMATCH",
        "severity": "HIGH",
        "short": "The tables script labels the sample as 2005\u20132017 (and use...",
        "file": "07_tables.R",
        "lines": [
          74,
          132
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0074/code/07_tables.R#L74-L132"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0074_scan.json"
  },
  "error": null
}