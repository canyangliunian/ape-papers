{
  "paper_id": "apep_0134",
  "scan_date": "2026-02-06T12:49:08.973850+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 4,
  "flags": [
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "05_figures.R",
      "lines": [
        150,
        176
      ],
      "evidence": "Figure 3 (Synthetic Control: East Harlem) is generated from a hard-coded, 'illustrative' dataset rather than being computed from the actual panel data and synthetic control model output. This can materially misrepresent the estimated effect path, pre-treatment fit, and 2024 gap shown in the manuscript (and the saved PDF figure is what readers see). The header comment explicitly allows this fallback: 'If results file not found, plots use representative values', which creates a high risk that the manuscript figure is not a direct product of the analysis code/data.: synth_data <- tibble(\n  year = rep(2015:2024, 2),\n  type = rep(c(\"Actual (East Harlem)\", \"Synthetic Control\"), each = 10),\n  od_rate = c(\n    # Actual East Harlem\n    42.5, 54.2, 68.3, 72.1, 78.5, 92.4, 88.1, 88.9, 82.3, 75.2,\n    # Synthetic (matched pre, diverges post)\n    43.2, 55.1, 67.8, 73.5, 79.2, 91.8, 87.5, 94.5, 98.2, 102.5\n  )\n)",
      "confidence": 0.95
    },
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        118,
        135
      ],
      "evidence": "The key outcome data are loaded from a local CSV ('data/raw/dohmh_overdose_rates.csv') described as manually compiled from PDFs/EpiQuery, but the repository code shown does not include an extraction script or any automated provenance/audit trail linking each cell to an underlying DOHMH table. The manuscript claims replication data are available, but from the code alone, the integrity of the manually-entered dataset cannot be verified (risk of transcription/curation errors). This is not fabrication per se, but provenance is incomplete without (i) the raw PDFs, (ii) a structured extraction log, or (iii) checksums/citations at the row level.: overdose_rates <- read_csv(\n  file.path(PAPER_DIR, \"data\", \"raw\", \"dohmh_overdose_rates.csv\"),\n  show_col_types = FALSE\n) %>%\n  select(uhf_id, year, od_rate, od_count)\n",
      "confidence": 0.75
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "03_main_analysis.R",
      "lines": [
        167,
        195
      ],
      "evidence": "Randomization inference (RI) samples placebo treated units from 'all_uhfs', which includes the actually treated UHFs (201, 203). Under the manuscript\u2019s described placebo-in-space logic ('iteratively reassigning treatment to each control unit' / donor pool permutations), permutations should draw treated assignments from donor/control units only. Including the true treated units in the permutation set changes the null distribution and can distort the RI p-value. The code even defines 'control_uhfs' but does not use it.: # Permutation test: randomly assign treatment to 2 UHFs\nset.seed(20211130)\nn_perms <- 1000\ncontrol_uhfs <- unique(did_data$uhf_id[did_data$treatment_status == \"control\"])\nall_uhfs <- unique(did_data$uhf_id)\n\npermuted_effects <- numeric(n_perms)\nfor (i in 1:n_perms) {\n  # Randomly select 2 UHFs as \"treated\"\n  fake_treated <- sample(all_uhfs, 2)\n  permuted_effects[i] <- compute_did(did_data, fake_treated)\n}\n\n# P-value: proportion of permuted effects more extreme than observed\nri_pvalue <- mean(abs(permuted_effects) >= abs(observed_effect))",
      "confidence": 0.85
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "LOW",
      "file": "03_main_analysis.R",
      "lines": [
        109,
        122
      ],
      "evidence": "The event-study specification includes year fixed effects on the right-hand side while also using i(year, treat, ...) and absorbing year fixed effects via '| uhf_id + year'. In fixest, the i(year, treat, ...) interaction is still identified, but this structure can be confusing and is easy to mis-specify (e.g., if year FE were included twice differently). Not necessarily wrong, but it merits checking that the plotted coefficients correspond to the intended estimand in the manuscript.: did_event <- feols(\n  od_rate ~ i(year, treat, ref = 2020) | uhf_id + year,\n  data = did_data,\n  cluster = ~uhf_id\n)",
      "confidence": 0.6
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "03_main_analysis.R",
      "lines": [
        205,
        247
      ],
      "evidence": "The manuscript describes placebo-in-time tests as 'run the synthetic control analysis with placebo treatment dates', but the code implements placebo-in-time using a simple before/after difference-in-differences on means (not SCM/augsynth). This mismatch could change the magnitude/variance of placebo effects and how they should be interpreted as an SCM falsification test.: # Run synthetic control with fake treatment dates\nplacebo_years <- c(2016, 2017, 2018, 2019, 2020)\n\nplacebo_effects <- sapply(placebo_years, function(fake_year) {\n  synth_data_placebo <- panel_data %>%\n    filter(uhf_id == 203 | treatment_status == \"control\") %>%\n    filter(year <= 2021) %>%  # Use only pre-treatment data\n    mutate(\n      treat = ifelse(uhf_id == 203, 1, 0),\n      post = ifelse(year >= fake_year, 1, 0)\n    )\n\n  # Simple DiD for placebo\n  pre_treat <- synth_data_placebo %>%\n    filter(treat == 1, post == 0) %>%\n    pull(od_rate) %>%\n    mean(na.rm = TRUE)\n  post_treat <- synth_data_placebo %>%\n    filter(treat == 1, post == 1) %>%\n    pull(od_rate) %>%\n    mean(na.rm = TRUE)\n  pre_control <- synth_data_placebo %>%\n    filter(treat == 0, post == 0) %>%\n    pull(od_rate) %>%\n    mean(na.rm = TRUE)\n  post_control <- synth_data_placebo %>%\n    filter(treat == 0, post == 1) %>%\n    pull(od_rate) %>%\n    mean(na.rm = TRUE)\n\n  (post_treat - pre_treat) - (post_control - pre_control)\n})",
      "confidence": 0.85
    },
    {
      "category": "DATA_FABRICATION",
      "severity": "MEDIUM",
      "file": "05_figures.R",
      "lines": [
        1,
        21
      ],
      "evidence": "The figures script explicitly permits generating publishable outputs from 'representative values' if analysis outputs are missing. Combined with the hard-coded synthetic control series below, this creates a pathway to produce figures without running estimation on real data. This is best categorized as a fabrication risk in the workflow (not necessarily that the authors fabricated, but that the codebase enables it).: # NOTE: Figure data comes from analysis_results.rds when available.\n# If results file not found, plots use representative values based on\n# main analysis specification. For full replication, run 03_main_analysis.R\n# first to generate actual estimates.",
      "confidence": 0.8
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "01_fetch_data.R",
      "lines": [
        73,
        85
      ],
      "evidence": "Donor pool exclusions (spillover neighborhoods) are hard-coded. The manuscript provides a justification (spillover risk), so this is not inherently problematic. However, because exclusions can materially affect SCM fit and RI p-values (especially with small donor pools), it would strengthen integrity to (i) generate adjacency programmatically or (ii) log donor-pool construction choices and show sensitivity to alternative spillover definitions. Given the paper\u2019s robustness section, this is a minor concern.: spillover_uhfs <- c(\n  202,  # Central Harlem (adjacent to both)\n  204,  # Upper West Side (adjacent to Washington Heights)\n  205,  # Upper East Side (adjacent to East Harlem)\n  105,  # Crotona-Tremont (Bronx, adjacent to East Harlem)\n  106,  # Highbridge-Morrisania (Bronx, adjacent to Washington Heights)\n  107   # Hunts Point-Mott Haven (Bronx, near East Harlem)\n)\n\nuhf_neighborhoods <- uhf_neighborhoods %>%\n  mutate(\n    treatment_status = case_when(\n      uhf_id %in% treated_uhfs ~ \"treated\",\n      uhf_id %in% spillover_uhfs ~ \"spillover\",\n      TRUE ~ \"control\"\n    )\n  )",
      "confidence": 0.65
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_figures.R",
      "verdict": "SUSPICIOUS"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 1,
      "MEDIUM": 4,
      "LOW": 2
    },
    "one_liner": "hard-coded results",
    "executive_summary": "In `05_figures.R`, Figure 3 (\u201cSynthetic Control: East Harlem\u201d) is produced from a hard-coded, explicitly \u201cillustrative\u201d dataset instead of being computed from the paper\u2019s panel data and the synthetic control model output. This breaks the end-to-end reproducibility of the figure and raises the risk that the displayed treatment and counterfactual paths do not reflect the actual estimation results, undermining the credibility of the reported effect.",
    "top_issues": [
      {
        "category": "HARD_CODED_RESULTS",
        "severity": "HIGH",
        "short": "Figure 3 (Synthetic Control",
        "file": "05_figures.R",
        "lines": [
          150,
          176
        ],
        "github_url": "/apep_0134/code/05_figures.R#L150-L176"
      }
    ],
    "full_report_url": "/tournament/corpus_scanner/scans/apep_0134_scan.json"
  },
  "error": null
}