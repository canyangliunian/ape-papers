{
  "paper_id": "apep_0148",
  "scan_date": "2026-02-06T12:52:14.511041+00:00",
  "scan_version": "2.0.0",
  "model": "openai/gpt-5.2",
  "overall_verdict": "SUSPICIOUS",
  "files_scanned": 9,
  "flags": [
    {
      "category": "DATA_PROVENANCE_MISSING",
      "severity": "MEDIUM",
      "file": "01_fetch_data.R",
      "lines": [
        123,
        124,
        125,
        126,
        127,
        128
      ],
      "evidence": "The manuscript states: \"I also incorporate state minimum wage data from the Department of Labor to control for concurrent policy changes.\" However, the codebase shown does not actually fetch minimum wage data from DOL (or another documented source), nor does it construct or merge a state-year minimum wage series later (in the provided scripts). Instead, it creates only a placeholder state list and explicitly defers construction. This creates a provenance gap (no source file/API call) and a risk that the control is absent or later manually assembled off-script.: # ---- State Minimum Wages ----\n# Source: DOL / UC Berkeley Labor Center\n# We'll use a simplified approach with known minimum wages\n\nstate_min_wage <- tibble(\n  statefip = 1:56,\n  state_abbr = c(\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"DC\", \"FL\",\n                 \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\",\n                 \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\",\n                 \"NJ\", \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\",\n                 \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\",\n                 \"WY\", NA, NA, NA, NA, NA)\n) %>%\n  filter(!is.na(state_abbr))\n\n# For now, we'll construct minimum wage data in the cleaning script\n# using known values for the analysis period",
      "confidence": 0.78
    },
    {
      "category": "HARD_CODED_RESULTS",
      "severity": "HIGH",
      "file": "paper.tex",
      "lines": [
        221,
        222,
        223,
        224,
        225,
        226,
        227,
        228,
        229,
        230,
        231,
        232,
        233,
        234,
        235,
        236,
        237,
        238,
        239,
        240,
        241,
        242,
        243,
        244,
        245,
        246,
        247,
        248,
        249,
        250,
        251,
        252,
        253
      ],
      "evidence": "The manuscript embeds final numeric regression results directly in LaTeX tables (e.g., -0.012, 0.004, R-squared values, observation counts). Meanwhile, the codebase generates LaTeX tables into separate files (e.g., tables/table2_main_results.tex, tables/table3_gender_gap.tex) using fixest::etable() and manual string construction. However, paper.tex does not appear to \\input{} those generated table files and instead hard-codes table contents. This breaks the reproducibility link between code outputs and manuscript-reported results and increases integrity risk (transcription or post-hoc editing). In an academic audit, this is a major red flag unless the workflow explicitly documents manual table transcription/verification.: \\begin{table}[H]\n\\centering\n\\caption{Effect of Salary Transparency Laws on Log Wages}\n\\label{tab:main}\n\\begin{threeparttable}\n\\begin{tabular}{lcccc}\n\\toprule\n& (1) & (2) & (3) & (4) \\\\\n& State-Year & Individual & + Occ/Ind FE & + Demographics \\\\\n\\midrule\nTreated $\\times$ Post & -0.012** & -0.014** & -0.016*** & -0.018*** \\\\\n& (0.004) & (0.005) & (0.005) & (0.005) \\\\\n...",
      "confidence": 0.74
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "07_tables.R",
      "lines": [
        70,
        71,
        72,
        73,
        74,
        75,
        76,
        77,
        78,
        79,
        80,
        81,
        82,
        83,
        84,
        85,
        86,
        87,
        88,
        89
      ],
      "evidence": "The script extracts Callaway-Sant'Anna overall ATT and SE (cs_att/cs_se) but does not actually place them into the exported LaTeX table: etable() is called only on m1, m3, m4, m5 (TWFE state-year and individual-level regressions). The manuscript's Table \\\"Effect of Salary Transparency Laws on Log Wages\\\" explicitly includes a 'Callaway-Sant'Anna estimate' column. Unless another (unprovided) script injects cs_att/cs_se into the table, the code does not implement the table structure described in the paper. This also amplifies the 'hard-coded results' concern because the manuscript table may not be produced by the scripts.: # (2) C-S Simple ATT\n# Extract from results\ncs_att <- results$att_simple$overall.att\ncs_se <- results$att_simple$overall.se\n\n# Export table\netable(m1, m3, m4, m5,\n       tex = TRUE,\n       ...\n       file = \"tables/table2_main_results.tex\")",
      "confidence": 0.8
    },
    {
      "category": "SUSPICIOUS_TRANSFORMS",
      "severity": "LOW",
      "file": "07_tables.R",
      "lines": [
        33,
        34,
        35,
        36,
        37,
        38,
        39,
        40,
        41,
        42
      ],
      "evidence": "Table 1 construction assumes that group_by(ever_treated) produces rows in an order where index [2] corresponds to treated states and [1] to controls. dplyr typically sorts groups, but relying on positional indexing for treated/control labels is fragile and can silently flip treated/control columns if factor/ordering changes (e.g., if ever_treated becomes a labeled factor). This is not fabrication, but it is a integrity/reproducibility risk for reported descriptive differences and state counts\u2014especially given the manuscript footnote stating prior \"code integrity issues (Table 1 state counts...)\".: sum_stats <- df %>%\n  filter(income_year < 2021) %>%\n  group_by(ever_treated) %>%\n  summarize(\n    `Hourly Wage` = sprintf(\"%.2f\", mean(hourly_wage)),\n    ...\n  )\n\n... \n\"Hourly Wage (\\\\$) & \", sum_stats$`Hourly Wage`[2], \" & \", sum_stats$`Hourly Wage`[1]",
      "confidence": 0.67
    },
    {
      "category": "METHODOLOGY_MISMATCH",
      "severity": "MEDIUM",
      "file": "05_robustness.R",
      "lines": [
        196,
        197,
        198,
        199,
        200,
        201,
        202
      ],
      "evidence": "The placebo test is intended to assign a fake treatment 2 years early and estimate an effect that should be ~0. However, the code then restricts the dataset to years strictly before the earliest real treatment year (income_year < 2021). For some cohorts, this eliminates the 'post' periods relative to the placebo timing (e.g., Colorado placebo g=2019 would require post years >=2019, which exist in 2019-2020, but later cohorts' placebo post periods may be truncated). This restriction makes the placebo design non-standard and potentially invalid/underpowered compared to what the paper describes (a clean placebo two years prior). If the manuscript reports a specific placebo ATT, it may not correspond to this implementation.: # Create placebo treatment (2 years before actual)\nstate_year_placebo <- state_year %>%\n  mutate(\n    g_placebo = ifelse(g > 0, g - 2, 0),\n    # Exclude actual post-treatment periods\n    income_year_restricted = income_year\n  ) %>%\n  filter(income_year < min(g[g > 0]))  # Only pre-treatment data",
      "confidence": 0.7
    }
  ],
  "file_verdicts": [
    {
      "file": "01_fetch_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "07_tables.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_packages.R",
      "verdict": "CLEAN"
    },
    {
      "file": "06_figures.R",
      "verdict": "CLEAN"
    },
    {
      "file": "00_policy_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "02_clean_data.R",
      "verdict": "CLEAN"
    },
    {
      "file": "05_robustness.R",
      "verdict": "CLEAN"
    },
    {
      "file": "04_main_analysis.R",
      "verdict": "CLEAN"
    },
    {
      "file": "03_descriptives.R",
      "verdict": "CLEAN"
    },
    {
      "file": "paper.tex",
      "verdict": "SUSPICIOUS"
    }
  ],
  "summary": {
    "verdict": "SUSPICIOUS",
    "counts": {
      "CRITICAL": 0,
      "HIGH": 1,
      "MEDIUM": 3,
      "LOW": 1
    },
    "one_liner": "hard-coded results",
    "executive_summary": "Paper **apep_0148** hard-codes final regression outputs directly into `paper.tex` (coefficients, standard errors, R-squared values, and observation counts) instead of pulling them from the analysis pipeline, even though the codebase is set up to generate LaTeX tables separately. This breaks the linkage between code and reported results, making the manuscript\u2019s key numbers non-reproducible from the repository and allowing the paper to present values that may not match the underlying computations.",
    "top_issues": [
      {
        "category": "HARD_CODED_RESULTS",
        "severity": "HIGH",
        "short": "The manuscript embeds final numeric regression results di...",
        "file": "paper.tex",
        "lines": [
          221,
          222
        ],
        "github_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/apep_0148/code/paper.tex#L221-L253"
      }
    ],
    "full_report_url": "https://github.com/SocialCatalystLab/ape-papers/blob/main/tournament/corpus_scanner/scans/apep_0148_scan.json"
  },
  "error": null
}